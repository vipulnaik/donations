/* Donee donation cases: MIRI */

insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('http://lesswrong.com/r/discussion/lw/mj0/miri_fundraiser_why_now_matters/','MIRI Fundraiser: Why now matters','2015-07-24',NULL,'Nate Soares','Machine Intelligence Research Institute',NULL,'Machine Intelligence Research Institute','Donee donation case','AI safety','Cross-posted at LessWrong and on the MIRI blog at https://intelligence.org/2015/07/20/why-now-matters/ -- this post occurs just two months after Soares takes over as MIRI Executive Director. It is a followup to https://intelligence.org/2015/07/17/miris-2015-summer-fundraiser/'),
  ('https://intelligence.org/2015/07/17/miris-2015-summer-fundraiser/','MIRI’s 2015 Summer Fundraiser!','2015-07-17',NULL,'Nate Soares','Machine Intelligence Research Institute',NULL,'Machine Intelligence Research Institute','Donee donation case','AI safety','MIRI announces its summer fundraiser and links to a number of documents to help donors evaluate it. This is the first fundraiser under new Executive Director Nate Soares, just a couple months after he assumed office'),
  ('https://intelligence.org/2016/09/16/miris-2016-fundraiser/','MIRI’s 2016 Fundraiser','2016-09-16',NULL,'Nate Soares','Machine Intelligence Research Institute',NULL,'Machine Intelligence Research Institute','Donee donation case','AI safety','MIRI announces its single 2016 fundraiser (as opposed to previous years when it conducted two fundraisers, it is conducting just one this time, in the Fall)'),
  ('https://www.facebook.com/robbensinger/posts/10157530911060447','Crunch time!! The 2016 fundraiser for the AI safety group I work at, MIRI, is going a lot slower than expected','2016-10-25','2016-10-25','Rob Bensinger','Facebook',NULL,'Machine Intelligence Research Institute','Donee donation case','AI safety','Rob Bensinger, Research Communications Director at MIRI, takes to his personal Facebook to ask people to chip in for the MIRI fundraiser, which is going slower than he and MIRI expected, and may not meet its target. The final comment by Bensinger notes that $582,316 out of the target of $750,000 was raised, and that about $260k of that was raised after his post, so he credits the final push for helping MIRI move closer to its fundraising goals'),
  ('https://intelligence.org/2017/12/01/miris-2017-fundraiser/','MIRI’s 2017 Fundraiser','2017-12-01',NULL,'Malo Bourgon','Machine Intelligence Research Institute',NULL,'Machine Intelligence Research Institute','Donee donation case','AI safety','Document provides cumulative target amounts for 2017 fundraiser ($625,000 Target 1, $850,000 Target 2, $1,250,000 Target 3) along with what MIRI expects to accomplish at each target level. Funds raised from the Open Philanthropy Project and an anonymous cryptocurrency donor (see https://intelligence.org/2017/07/04/updates-to-the-research-team-and-a-major-donation/ for more) are identified as reasons for the greater financial security and more long-term and ambitious planning'),
  ('https://intelligence.org/2017/12/14/end-of-the-year-matching/','End-of-the-year matching challenge!','2017-12-14',NULL,'Rob Bensinger','Machine Intelligence Research Institute','Christian Calderon|Marius van Voorden','Machine Intelligence Research Institute','Donee donation case','AI safety','MIRI gives an update on how its fundraising efforts are going, noting that it has met its first fundraising target, listing two major donations (Christian Calderon: $367,574 and Marius van Voorden: $59K), and highlighting the 2017 charity drive where donations up to $1 million to a list of charities including MIRI will be matched'),
  ('http://effective-altruism.com/ea/1io/miri_2017_fundraiser_and_strategy_update/','MIRI 2017 Fundraiser and Strategy Update','2017-12-15',NULL,'Malo Bourgon','Machine Intelligence Research Institute',NULL,'Machine Intelligence Research Institute','Donee donation case','AI safety','MIRI provides an update on its fundraiser and its strategy in a general-interest forum for people interested in effective altruism. They say the fundraiser is already going quite well, but believe they can still use marginal funds well to expand more'),
  ('https://intelligence.org/2018/11/26/miris-2018-fundraiser/','MIRI’s 2018 Fundraiser','2018-11-26','2018-11-28','Malo Bourgon','Machine Intelligence Research Institute','Dan Smith|Aaron Merchak|Matt Ashton|Stephen Chidwick','Machine Intelligence Research Institute','Donee donation case','AI safety','MIRI announces its 2018 end-of-year fundraising, with Target 1 of $500,000 and Target 2 of $1,200,000. It provides an overview of its 2019 budget and plans to explain the values it has worked out for Target 1 and Target 2. The post also mentions a matching opportunity sponsored by professional poker players Dan Smith, Aaron Merchak, Matt Ashton, and Stephen Chidwick, in partnership with Raising for Effective Giving (REG), which provides matching for donations to MIRI and REG up to $20,000. The post is referenced by Effective Altruism Funds in their grant write-up for a $40,000 grant to MIRI, at https://app.effectivealtruism.org/funds/far-future/payouts/3JnNTzhJQsu4yQAYcKceSi'),
  ('https://intelligence.org/2019/12/02/miris-2019-fundraiser/','MIRI’s 2019 Fundraiser','2019-12-02',NULL,'Malo Bourgon','Machine Intelligence Research Institute',NULL,'Machine Intelligence Research Institute','Donee donation case','AI safety','MIRI announces its 2019 fundraiser, with a target of $1 million for fundraising. The blog post describes MIRI''s projected budget, and provides more details on MIRI''s activities in 2019, including (1) workshops and scaling up, and (2) research and write-ups. Regarding research, the blog post reaffirms continuation of the nondisclosure-by-default policy announced in 2018 at https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/ The post is link-posted to the Effective Altruism Forum at https://forum.effectivealtruism.org/posts/LCApwxbdX4njYzgdr/miri-s-2019-fundraiser');

/* Other donee updates */

insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('https://forum.effectivealtruism.org/posts/GA7ytcMeRQe5b27ge/ask-miri-anything-ama','Ask MIRI Anything (AMA)','2016-10-11',NULL,'Rob Bensinger','Machine Intelligence Research Institute',NULL,'Machine Intelligence Research Institute','Donee AMA','AI safety','Rob Bensinger, the Research Communications Manager at MIRI, hosts an Ask Me Anything (AMA) on the Effective Altruism Forum during the October 2016 Fundraiser'),
  ('https://intelligence.org/2017/04/30/2017-updates-and-strategy/','2017 Updates and Strategy','2017-04-30',NULL,'Rob Bensinger','Machine Intelligence Research Institute',NULL,'Machine Intelligence Research Institute','Donee periodic update','AI safety','MIRI provides updates on its progress as an organization and outlines its strategy and budget for the coming year. Key update is that recent developments in AI have made them increase the probability of AGI before 2035 by a little bit. MIRI has also been in touch with researchers at FAIR, DeepMind, and OpenAI'),
  ('https://intelligence.org/2017/07/04/updates-to-the-research-team-and-a-major-donation/','Updates to the research team, and a major donation','2017-07-04',NULL,'Malo Bourgon','Machine Intelligence Research Institute',NULL,'Machine Intelligence Research Institute','Donee periodic update','AI safety','MIRI announces a surprise $1.01 million donation from an Ethereum cryptocurrency investor (2017-05-30) as well as updates related to team and fundraising'),
  ('https://intelligence.org/2017/11/08/major-grant-open-phil/','A major grant from the Open Philanthropy Project','2017-09-08',NULL,'Malo Bourgon','Machine Intelligence Research Institute','Open Philanthropy Project','Machine Intelligence Research Institute','Donee periodic update','AI safety','MIRI announces that it has received a three-year grant at $1.25 million per year from the Open Philanthropy Project, and links to the announcement from Open Phil at https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support-2017 and notes "The Open Philanthropy Project has expressed openness to potentially increasing their support if MIRI is in a position to usefully spend more than our conservative estimate, if they believe that this increase in spending is sufficiently high-value, and if we are able to secure additional outside support to ensure that the Open Philanthropy Project isn’t providing more than half of our total funding."'),
  ('https://intelligence.org/2018/01/10/fundraising-success/','Fundraising success!','2018-01-10',NULL,'Malo Bourgon','Machine Intelligence Research Institute',NULL,'Machine Intelligence Research Institute','Donee periodic update','AI safety','MIRI announces the success of its fundraiser, providing information on its top doonors, and thanking everybody who contributed'),
  ('https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/','2018 Update: Our New Research Directions','2018-11-22',NULL,'Nate Soares','Machine Intelligence Research Institute',NULL,'Machine Intelligence Research Institute','Donee periodic update','AI safety','MIRI executive director Nate Soares explains the new research directions being followed by MIRI, and how they differ from the original Agent Foundations agenda. The post also talks about how MIRI is being cautious in terms of sharing technical details of its research, until there is greater internal clarity on what findings need to be developed further, and what findings should be shared with what group. The post ends with guidance for people interested in joining the MIRI team to further the technical agenda. The post is referenced by Effective Altruism Funds in their grant write-up for a $40,000 grant to MIRI, at https://app.effectivealtruism.org/funds/far-future/payouts/3JnNTzhJQsu4yQAYcKceSi The nondisclosure-by-default section of the post is also referenced by Ben Hoskin in https://forum.effectivealtruism.org/posts/BznrRBgiDdcTwWWsB/2018-ai-alignment-literature-review-and-charity-comparison#MIRI__The_Machine_Intelligence_Research_Institute and also cited by him as one of the reasons he is not donating to MIRI this year (general considerations related to this are described at https://forum.effectivealtruism.org/posts/BznrRBgiDdcTwWWsB/2018-ai-alignment-literature-review-and-charity-comparison#Openness in the same post). Issa Rice also references these concerns in his donation decision write-up for 2018 at https://issarice.com/donation-history#section-3 but nonetheless decides to allocate $500 to MIRI'),
  ('https://intelligence.org/2019/04/01/new-grants-open-phil-beri/','New grants from the Open Philanthropy Project and BERI','2019-04-01',NULL,'Rob Bensinger','Machine Intelligence Research Institute','Open Philanthropy Project|Berkeley Existential Risk Initiative','Machine Intelligence Research Institute','Donee periodic update','AI safety','MIRI announces two grants to it: a two-year grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support-2019 totaling $2,112,500 from the Open Philanthropy Project, with half of it disbursed in 2019 and the other half disbursed in 2020. The amount disbursed in 2019 (of a little over $1.06 million) is on top of the $1.25 million already committed by the Open Philanthropy Project as part of the 3-year $3.75 million grant https://intelligence.org/2017/11/08/major-grant-open-phil/ The $1.06 million in 2020 may be supplemented by further grants from the Open Philanthropy Project. The grant size from the Open Philanthropy Project was determined by the Committee for Effective Altruism Support. The post also notes that the Open Philanthropy Project plans to determine future grant sizes using the Committee. MIRI expects the grant money to play an important role in decision-making as it executes on growing its research team as described in its 2018 strategy update post https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/ and fundraiser post https://intelligence.org/2018/11/26/miris-2018-fundraiser/'),
  ('https://intelligence.org/2019/02/11/our-2018-fundraiser-review/','Our 2018 Fundraiser Review','2019-02-11','2019-02-11','Colm Ó Riain','Machine Intelligence Research Institute',NULL,'Machine Intelligence Research Institute','Donee periodic update','AI safety','MIRI gives an update on its 2018 fundraiser. Key topics discussed include four types of donation matching programs that MIRI benefited from: (1) WeTrust Spring''s ETH-matching event, (2) Facebook''s Giving Tuesday event with https://donations.fb.com/giving-tuesday/ linked to, (3) Double Up Drive challenge, (4) Corporate matching');

/* External evaluations */

insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('http://www.vetta.org/2009/08/funding-safe-agi/','Funding safe AGI','2019-08-03',NULL,'Shane Legg',NULL,NULL,'Machine Intelligence Research Institute','Evaluator review of donee','AI safety','Shane Legg, who had previously received a $10,000 grant from the Singularity Institute for Artificial Intelligence (SIAI) and would go on to co-found DeepMind, talks about SIAI and AI safety. He says that, probably, nobody knows how to deal with the problem of constructing a safe AGI, but SIAI is, in relative terms, the best. However, he provides some suggestions on how it could encourage and monitor AI development more closely rather than trying to build everything on its own. SIAI would later change its name to the Machine Intelligence Research Institute (MIRI)'),
  ('https://multiverseaccordingtoben.blogspot.com/2010/10/singularity-institutes-scary-idea-and.html','The Singularity Institute''s Scary Idea (and Why I Don''t Buy It)','2010-10-29',NULL,'Ben Goertzel',NULL,NULL,'Machine Intelligence Research Institute','Evaluator review of donee','AI safety','Ben Goertzel, who previously worked as Director of Research at MIRI (then called the Singularity Institute for Artificial Intelligence (SIAI)) articulates its "Scary Idea" and explains why he does not believe in it. His articulation of the Scary Idea: "If I or anybody else actively trying to build advanced AGI succeeds, we''re highly likely to cause an involuntary end to the human race."'),
  ('https://groups.yahoo.com/neo/groups/givewell/conversations/topics/270','Singularity Institute for Artificial Intelligence','2011-04-30','2011-07-18','Holden Karnofsky','GiveWell','Open Philanthropy Project','Machine Intelligence Research Institute','Evaluator review of donee','AI safety','In this email thread on the GiveWell mailing list, Holden Karnofsky gives his views on the Singularity Institute for Artificial Intelligence (SIAI), the former name for the Machine Intelligence Research Institute (MIRI). The reply emails include a discussion of how much weight to give to, and what to learn from, the support for MIRI by Peter Thiel, a wealthy early MIRI backer. In the final email in the thread, Holden Karnofsky includes an audio recording with Jaan Tallinn, another wealthy early MIRI backer. This analysis likely influences the review https://www.lesswrong.com/posts/6SGqkCgHuNr7d4yJm/thoughts-on-the-singularity-institute-si published by Karnofsky next year, as well as the initial position of the Open Philanthropy Project (a GveWell spin-off grantmaker) toward MIRI'),
  ('https://www.lesswrong.com/posts/qqhdj3W3vSfB5E9ss/siai-an-examination','SIAI - An Examination','2011-05-02',NULL,'Brandon Reinhart','LessWrong','Brandon Reinhart','Machine Intelligence Research Institute','Evaluator review of donee','AI safety','Post discussing initial investigation into the Singularity Institute for Artificial Intelligence (SIAI), the former name of Machine Intelligence Research Institute (MIRI), with the intent of deciding whether to donate. Final takeaway is that it was a worthy donation target, though no specific donation is announced in the post. See http://lesswrong.com/r/discussion/lw/5fo/siai_fundraising/ for an earlier draft of the post (along with a number of comments that were incorporated into the official version)'),
  ('https://www.lesswrong.com/posts/6SGqkCgHuNr7d4yJm/thoughts-on-the-singularity-institute-si','Thoughts on the Singularity Institute (SI)','2012-05-11',NULL,'Holden Karnofsky','LessWrong','Open Philanthropy Project','Machine Intelligence Research Institute','Evaluator review of donee','AI safety','Post discussing reasons Holden Karnofsky, co-executive director of GiveWell, does not recommend the Singularity Institute (SI), the historical name for the Machine Intelligence Research Institute. This evaluation would be the starting point for the initial position of the Open Philanthropy Project (a GiveWell spin-off grantmaker) toward MIRI, but Karnofsky and the Open Philanthropy Project would later update in favor of AI safety in general and MIRI in particular; this evolution is described in https://docs.google.com/document/d/1hKZNRSLm7zubKZmfA7vsXvkIofprQLGUoW43CYXPRrk/edit'),
  ('https://slatestarcodex.com/2014/10/07/tumblr-on-miri/','Tumblr on MIRI','2014-10-07',NULL,'Scott Alexander','Slate Star Codex',NULL,'Machine Intelligence Research Institute','Evaluator review of donee','AI safety','The blog post is structured as a response to recent criticism of MIRI on Tumblr, but is mainly a guardedly positive assessment of MIRI. In particular, it highlights the important role played by MIRI in elevating the profile of AI risk, citing attention from Stephen Hawking, Elon Musk, Gary Drescher, Max Tegmark, Stuart Russell, and Peter Thiel.'),
  ('https://thingofthings.wordpress.com/2016/02/17/concerning-miris-place-in-the-ea-movement/','Concerning MIRI’s Place in the EA Movement','2016-02-17',NULL,'Ozy Brennan','Thing of Things',NULL,'Machine Intelligence Research Institute','Miscellaneous commentary','AI safety','The post does not directly evaluate MIRI, but highlights the importance of object-level evaluation of the quality and value of the work done by MIRI. Also thanks MIRI, LessWrong, and Yudkowsky for contributions to the growth of the effective altruist movement'),
  ('http://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support','Machine Intelligence Research Institute — General Support','2016-09-06',NULL,'Open Philanthropy Project','Open Philanthropy Project','Open Philanthropy Project','Machine Intelligence Research Institute','Evaluator review of donee','AI safety','Open Phil writes about the grant at considerable length, more than it usually does. This is because it says that it has found the investigation difficult and believes that others may benefit from its process. The writeup also links to reviews of MIRI research by AI researchers, commissioned by Open Phil: http://files.openphilanthropy.org/files/Grants/MIRI/consolidated_public_reviews.pdf (the reviews are anonymized). The date is based on the announcement date of the grant, see https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/XkSl27jBDZ8 for the email'),  
  ('http://files.openphilanthropy.org/files/Grants/MIRI/consolidated_public_reviews.pdf','Anonymized Reviews of Three Recent Papers from MIRI’s Agent Foundations Research Agenda (PDF)','2016-09-06',NULL,NULL,'Open Philanthropy Project','Open Philanthropy Project','Machine Intelligence Research Institute','Evaluator review of donee','AI safety','Reviews of the technical work done by MIRI, solicited and compiled by the Open Philanthropy Project as part of its decision process behind a grant for general support to MIRI documented at http://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support (grant made 2016-08, announced 2016-09-06)'),
  ('https://www.facebook.com/vipulnaik.r/posts/10210792236177912','Belief status: off-the-cuff thoughts!','2017-01-19','2017-01-19','Vipul Naik','Facebook',NULL,'Machine Intelligence Research Institute','Reasoning supplement','AI safety','The post argues that (lack of) academic endorsement of the work done by MIRI should not be an important factor in evaluating MIRI, offering three reasons. Commenters include Rob Bensinger, Research Communications Manager at MIRI'),
  ('https://forum.effectivealtruism.org/posts/SEL9PW8jozrvLnkb4/my-current-thoughts-on-miri-s-highly-reliable-agent-design','My current thoughts on MIRI’s highly reliable agent design work','2017-07-07',NULL,'Daniel Dewey','Effective Altruism Forum','Open Philanthropy Project','Machine Intelligence Research Institute','Evaluator review of donee','AI safety','Post discusses thoughts on the MIRI work on highly reliable agent design. Dewey is looking into the subject to inform Open Philanthropy Project grantmaking to MIRI specifically and for AI risk in general; the post reflects his own opinions that could affect Open Phil decisions. See https://groups.google.com/forum/#!topic/long-term-world-improvement/FeZ_h2HXJr0 for critical discussion, in particular the comments by Sarah Constantin'),
  ('https://www.facebook.com/robbensinger/posts/10159100119210447','I’ve noticed that this misconception is still floating around','2017-08-30',NULL,'Rob Bensinger','Facebook',NULL,'Machine Intelligence Research Institute','Reasoning supplement','AI safety','Post notes an alleged popular misconception that the reason to focus on AI risk is that it is low-probability but high-impact, but MIRI researchers assign a medium-to-high probability of AI risk in the medium-term future'),
  ('https://www.lesswrong.com/posts/5evRqMmGxTKf98pvT/evaluating-the-feasibility-of-si-s-plan','Evaluating the feasibility of SI''s plan','2013-01-10',NULL,'Joshua Fox','LessWrong',NULL,'Machine Intelligence Research Institute','Evaluator review of donee','AI safety','This blog post, co-authored with Kaj Sotala, gives a simplified description of the plan being followed by the Singularity Institute (SI), the former name of the Machine Intelligence Research Institute (MIRI). It is critical of SI for focusing on its "perfect" friendly AI, and suggests that more focus be given to improving the safety of existing systems in development, such as OpenCog. In a reply comment, Eliezer notes that the "heuristic safety" that the blog post suggests focusing on is difficult, that people overestimate the feasibility of heuristic safety ideas, and that trying for a safety approach that seems highly likely to succeed is the best way to guard against safety approaches that are doomed to fail. There is further discussion in the comments from Wei Dai, Gwern, and a cryptography researcher'),
  ('https://www.greaterwrong.com/posts/iELiEEZERCXJS9DT4/how-does-miri-know-it-has-a-medium-probability-of-success','How does MIRI Know it Has a Medium Probability of Success?','2013-08-01',NULL,'Peter Hurford','LessWrong',NULL,'Machine Intelligence Research Institute','Miscellaneous commentary','AI safety','In this bleg, Peter Hurford asks why MIRI thinks it has a medium probability of success at achieving the goal of friendly AI (and avoiding unfriendly AI). The post attracts multiple comments from Eliezer Yudkowsky, Carl Shulman, Wei Dai, and others');

/* Donors describe why they donate */

insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('https://reason.com/2008/05/01/technology-is-at-the-center/','''Technology Is at the Center'' Entrepreneur and philanthropist Peter Thiel on liberty and scientific progress','2008-05-01',NULL,'Ronald Bailey','Reason Magazine','Peter Thiel','Machine Intelligence Research Institute|Methuselah Foundation','Broad donor strategy','AI safety|Scientific research/longevity research','In an interview with Ronald Bailey, the science correspondent of Reason Magazine, Peter Thiel talks about his political ideology of libertarianism as well as his philanthropic activities. He talks about two areas that he is donating heavily in: accelerating a safe technological singularity (through donations to the Singulariy Institute) and anti-aging research (through donations to the Methuselah Foundation'),
  ('https://forum.effectivealtruism.org/posts/WAxSTJbdXMvFScPSa/why-i-m-donating-to-miri-this-year','Why I''m donating to MIRI this year','2016-11-30',NULL,'Owen Cotton-Barratt',NULL,'Owen Cotton-Barratt','Machine Intelligence Research Institute','Single donation documentation','AI safety','Primary interest is in existential risk. Cited CoI and other reasons for not donating to own employer, Centre for Effective Altruism. Notes disagreements with MIRI, citing http://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support#research but highlights need for epistemic humility'),
   ('https://thezvi.wordpress.com/2017/12/17/i-vouch-for-miri/','I Vouch For MIRI','2017-12-17',NULL,'Zvi Mowshowitz',NULL,'Zvi Mowshowitz','Machine Intelligence Research Institute','Single donation documentation','AI safety','Mowshowitz explains why he made his $10,000 donation to MIRI, and makes the case for others to support MIRI. He believes that MIRI understands the hardness of the AI safety problem, is focused on building solutions for the long term, and has done humanity a great service through its work on functional decision theory'),
   ('https://putanumonit.com/2017/12/10/worried-about-ai/','AI: a Reason to Worry, and to Donate','2017-12-10',NULL,'Jacob Falkovich',NULL,'Jacob Falkovich','Machine Intelligence Research Institute|Future of Life Institute|Center for Human-Compatible AI|Berkeley Existential Risk Initiative|Future of Humanity Institute|Effective Altruism Funds','Single donation documentation','AI safety','Falkovich explains why he thinks AI safety is a much more important and relatively neglected existential risk than climate change, and why he is donating to it. He says he is donating to MIRI because he is reasonably certain of the importance of their work on AI aligment. However, he lists a few other organizations for which he is willing to match donations up to 0.3 bitcoins, and encourages other donors to use their own judgment to decide among them: Future of Life Institute, Center for Human-Compatible AI, Berkeley Existential Risk Initiative, Future of Humanity Institute, and Effective Altruism Funds (the Long-Term Future Fund)');
   

