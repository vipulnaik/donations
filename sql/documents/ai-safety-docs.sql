/* AI safety general */

insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,affected_influencers,document_scope,cause_area,notes) values
  ('https://forum.effectivealtruism.org/posts/nSot23sAjoZRgaEwa/2016-ai-risk-literature-review-and-charity-comparison','2016 AI Risk Literature Review and Charity Comparison','2016-12-13',NULL,'Larks','Effective Altruism Forum','Larks','Machine Intelligence Research Institute|Future of Humanity Institute|OpenAI|Center for Human-Compatible AI|Future of Life Institute|Centre for the Study of Existential Risk|Leverhulme Centre for the Future of Intelligence|Global Catastrophic Risk Institute|Global Priorities Project|AI Impacts|Xrisks Institute|X-Risks Net|Center for Applied Rationality|80,000 Hours|Raising for Effective Giving',NULL,'Review of current state of cause area','AI safety','The lengthy blog post covers all the published work of prominent organizations focused on AI risk. References https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support#sources1007 for the MIRI part of it but notes the absence of information on the many other orgs. The conclusion: "The conclusion: "Donate to both the Machine Intelligence Research Institute and the Future of Humanity Institute, but somewhat biased towards the former. I will also make a smaller donation to the Global Catastrophic Risks Institute."'),
  ('https://www.centreforeffectivealtruism.org/blog/changes-in-funding-in-the-ai-safety-field','Changes in funding in the AI safety field','2017-02-01','2017-02-02','Sebastian Farquhar','Centre for Effective Altruism',NULL,'Machine Intelligence Research Institute|Center for Human-Compatible AI|Leverhulme Centre for the Future of Intelligence|Future of Life Institute|Future of Humanity Institute|OpenAI|MIT Media Lab',NULL,'Review of current state of cause area','AI safety','The post reviews AI safety funding from 2014 to 2017 (projections for 2017). Cross-posted on EA Forum at http://effective-altruism.com/ea/16s/changes_in_funding_in_the_ai_safety_field/'),
  ('https://forum.effectivealtruism.org/posts/XKwiEpWRdfWo7jy7f/2017-ai-safety-literature-review-and-charity-comparison','2017 AI Safety Literature Review and Charity Comparison','2017-12-20',NULL,'Larks','Effective Altruism Forum','Larks','Machine Intelligence Research Institute|Future of Humanity Institute|Global Catastrophic Risk Institute|Centre for the Study of Existential Risk|AI Impacts|Center for Human-Compatible AI|Center for Applied Rationality|Future of Life Institute|80,000 Hours',NULL,'Review of current state of cause area','AI safety','The lengthy blog post covers all the published work of prominent organizations focused on AI risk. It is an annual refresh of https://forum.effectivealtruism.org/posts/nSot23sAjoZRgaEwa/2016-ai-risk-literature-review-and-charity-comparison -- a similar post published a year before it. The conclusion: "Significant donations to the Machine Intelligence Research Institute and the Global Catastrophic Risks Institute. A much smaller one to AI Impacts."'),
  ('https://www.lesswrong.com/posts/XFpDTCHZZ4wpMT8PZ/a-model-i-use-when-making-plans-to-reduce-ai-x-risk','A model I use when making plans to reduce AI x-risk','2018-01-18',NULL,'Ben Pace','LessWrong','Berkeley Existential Risk Initiative',NULL,NULL,'Review of current state of cause area','AI safety','The author describes his implicit model of AI risk, with four parts: (1) Alignment is hard, (2) Getting alignment right accounts for most of the variance in whether an AGI system will be positive for humanity, (3) Our current epistemic state regarding AGI timelines will continue until we are close (<2 years from) to having AGI, and (4) Given timeline uncertainty, it is best to spend marginal effort on plans that assume / work in shorter timelines. There is a lot of discussion in the comments'),
  ('https://forum.effectivealtruism.org/posts/HqatEhdEb42vhSo7B/opportunities-for-individual-donors-in-ai-safety','Opportunities for individual donors in AI safety','2018-03-12',NULL,'Alex Flint','Effective Altruism Forum',NULL,'Machine Intelligence Research Institute|Future of Humanity Institute',NULL,'Review of current state of cause area','AI safety','Alex Flint discusses the history of AI safety funding, and suggests some heuristics for individual donors based on what he has seen to be successful in the past.'),
  ('https://forum.effectivealtruism.org/posts/BznrRBgiDdcTwWWsB/2018-ai-alignment-literature-review-and-charity-comparison','2018 AI Alignment Literature Review and Charity Comparison','2018-12-17',NULL,'Larks','Effective Altruism Forum','Larks','Machine Intelligence Research Institute|Future of Humanity Institute|Center for Human-Compatible AI|Centre for the Study of Existential Risk|Global Catastrophic Risk Institute|Global Priorities Institute|Australian National University|Berkeley Existential Risk Initiative|Ought|AI Impacts|OpenAI|Effective Altruism Foundation|Foundational Research Institute|Median Group|Convergence Analysis',NULL,'Review of current state of cause area','AI safety','Cross-posted to LessWrong at https://www.lesswrong.com/posts/a72owS5hz3acBK5xc/2018-ai-alignment-literature-review-and-charity-comparison This is the third post in a tradition of annual blog posts on the state of AI safety and the work of various organizations in the space over the course of the year; the previous two blog posts are at https://forum.effectivealtruism.org/posts/nSot23sAjoZRgaEwa/2016-ai-risk-literature-review-and-charity-comparison and https://forum.effectivealtruism.org/posts/XKwiEpWRdfWo7jy7f/2017-ai-safety-literature-review-and-charity-comparison The post has a "methodological considerations" section that discusses how the author views track records, politics, openness, the research flywheel, near vs far safety research, other existential risks, financial reserves, donation matching, poor quality research, and the Bay Area. The number of organizations reviewed is also larger than in previous years. Excerpts from the conclusion: "Despite having donated to MIRI consistently for many years as a result of their highly non-replaceable and groundbreaking work in the field, I cannot in good faith do so this year given their lack of disclosure. [...] This is the first year I have attempted to review CHAI in detail and I have been impressed with the quality and volume of their work. I also think they have more room for funding than FHI. As such I will be donating some money to CHAI this year. [...] As such I will be donating some money to GCRI again this year. [...] As such I do not plan to donate to AI Impacts this year, but if they are able to scale effectively I might well do so in 2019. [...] I also plan to start making donations to individual researchers, on a retrospective basis, for doing useful work. [...] This would be somewhat similar to Impact Certificates, while hopefully avoiding some of their issues.'),
  ('https://forum.effectivealtruism.org/posts/dpBB24QsnsRnkq5JT/2019-ai-alignment-literature-review-and-charity-comparison','2019 AI Alignment Literature Review and Charity Comparison','2019-12-19',NULL,'Larks','Effective Altruism Forum','Larks|Effective Altruism Funds: Long-Term Future Fund|Open Philanthropy|Survival and Flourishing Fund','Future of Humanity Institute|Center for Human-Compatible AI|Machine Intelligence Research Institute|Global Catastrophic Risk Institute|Centre for the Study of Existential Risk|Ought|OpenAI|AI Safety Camp|Future of Life Institute|AI Impacts|Global Priorities Institute|Foundational Research Institute|Median Group|Center for Security and Emerging Technology|Leverhulme Centre for the Future of Intelligence|Berkeley Existential Risk Initiative|AI Pulse','Survival and Flourishing Fund','Review of current state of cause area','AI safety','Cross-posted to LessWrong at https://www.lesswrong.com/posts/SmDziGM9hBjW9DKmf/2019-ai-alignment-literature-review-and-charity-comparison This is the fourth post in a tradition of annual blog posts on the state of AI safety and the work of various organizations in the space over the course of the year; the previous year''s post is at https://forum.effectivealtruism.org/posts/BznrRBgiDdcTwWWsB/2018-ai-alignment-literature-review-and-charity-comparison The post has sections on "Research" and "Finance" for a number of organizations working in the AI safety space, many of whom accept donations. A "Capital Allocators" section discusses major players who allocate funds in the space. A lengthy "Methodological Thoughts" section explains how the author approaches some underlying questions that influence his thoughts on all the organizations. To make selective reading of the document easier, the author ends each paragraph with a hashtag, and lists the hashtags at the beginning of the document.'),
  ('https://forum.effectivealtruism.org/posts/K7Z87me338BQT3Mcv/2020-ai-alignment-literature-review-and-charity-comparison','2020 AI Alignment Literature Review and Charity Comparison','2020-12-21',NULL,'Larks','Effective Altruism Forum','Larks|Effective Altruism Funds: Long-Term Future Fund|Open Philanthropy|Survival and Flourishing Fund','Future of Humanity Institute|Center for Human-Compatible AI|Machine Intelligence Research Institute|Global Catastrophic Risk Institute|Centre for the Study of Existential Risk|OpenAI|Berkeley Existential Risk Initiative|Ought|Global Priorities Institute|Center on Long-Term Risk|Center for Security and Emerging Technology|AI Impacts|Leverhulme Centre for the Future of Intelligence|AI Safety Camp|Future of Life Institute|Convergence Analysis|Median Group|AI Pulse|80,000 Hours','Survival and Flourishing Fund','Review of current state of cause area','AI safety','Cross-posted to LessWrong at https://www.lesswrong.com/posts/pTYDdcag9pTzFQ7vw/2020-ai-alignment-literature-review-and-charity-comparison This is the fifth post in a tradition of annual blog posts on the state of AI safety and the work of various organizations in the space over the course of the year; the previous year''s post is at https://forum.effectivealtruism.org/posts/dpBB24QsnsRnkq5JT/2019-ai-alignment-literature-review-and-charity-comparison The post is structured very similar to the previous year''s post. It has sections on "Research" and "Finance" for a number of organizations working in the AI safety space, many of whom accept donations. A "Capital Allocators" section discusses major players who allocate funds in the space. A lengthy "Methodological Thoughts" section explains how the author approaches some underlying questions that influence his thoughts on all the organizations. To make selective reading of the document easier, the author ends each paragraph with a hashtag, and lists the hashtags at the beginning of the document. See https://www.lesswrong.com/posts/uEo4Xhp7ziTKhR6jq/reflections-on-larks-2020-ai-alignment-literature-review for discussion of some aspects of the post by Alex Flint.'),
  ('https://www.lesswrong.com/posts/uEo4Xhp7ziTKhR6jq/reflections-on-larks-2020-ai-alignment-literature-review','Reflections on Larks’ 2020 AI alignment literature review','2021-01-01',NULL,'Alex Flint','LessWrong',NULL,NULL,NULL,'Miscellaneous commentary','AI safety','In this post, Alex Flint offers thoughts on the 2020 AI alignment review https://forum.effectivealtruism.org/posts/K7Z87me338BQT3Mcv/2020-ai-alignment-literature-review-and-charity-comparison covering the following aspects: depth of research (Flint thinks research with depth is important and research lacking depth may crowd it out), flywheel (Flint is "impressed and more than a little disturbed" by the flywheel described in the original post), skepticism about strategy research, and concern about the lack of scalable uses of money. Rob Bensinger, MIRI''s research communications manager, endorses the post, and Linda Linsefors has a comment exchange with Flint.'),
  ('https://forum.effectivealtruism.org/posts/BNQMyWGCNWDdP2WyG/2021-ai-alignment-literature-review-and-charity-comparison','2021 AI Alignment Literature Review and Charity Comparison','2021-12-23',NULL,'Larks','Effective Altruism Forum','Larks|Effective Altruism Funds: Long-Term Future Fund|Survival and Flourishing Fund|FTX Future Fund','Future of Humanity Institute|Future of Humanity Institute|Centre for the Governance of AI|Center for Human-Compatible AI|Machine Intelligence Research Institute|Global Catastrophic Risk Institute|Centre for the Study of Existential Risk|OpenAI|Google Deepmind|Anthropic|Alignment Research Center|Redwood Research|Ought|AI Impacts|Global Priorities Institute|Center on Long-Term Risk|Centre for Long-Term Resilience|Rethink Priorities|Convergence Analysis|Stanford Existential Risk Initiative|Effective Altruism Funds: Long-Term Future Fund|Berkeley Existential Risk Initiative|80,000 Hours','Survival and Flourishing Fund','Review of current state of cause area','AI safety','Cross-posted to LessWrong at https://www.lesswrong.com/posts/C4tR3BEpuWviT7Sje/2021-ai-alignment-literature-review-and-charity-comparison This is the sixth post in a tradition of annual blog posts on the state of AI safety and the work of various organizations in the space over the course of the year; the post is structured similarly to the previous year''s post https://forum.effectivealtruism.org/posts/K7Z87me338BQT3Mcv/2020-ai-alignment-literature-review-and-charity-comparison but has a few new features. The author mentions that he has several conflicts of interest that he cannot individually disclose. He also starts collecting "second preferences" data this year for all the organizations he talks to, which is where the organization would like to see funds go, other than itself. The Long-Term Future Fund is the clear winner here. He also announces that he''s looking for a research assistant to help with next year''s post given the increasing time demands and his reduced time availability. His final rot13''ed donation decision is to donate to the Long-Term Future Fund so that sufficiently skilled AI safety researchers can make a career with LTFF funding; his second preference for donations is BERI. Many other organizations that he considers to be likely to be doing excellent work are either already well-funded or do not provide sufficient disclosure.'),
  ('https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is','(My understanding of) What Everyone in Technical Alignment is Doing and Why','2022-08-28',NULL,'Thomas Larsen|Eli','LessWrong','Fund for Alignment Resesarch','Aligned AI|Alignment Research Center|Anthropic|Center for AI Safety|Center for Human-Compatible AI|Center on Long-Term Risk|Conjecture|DeepMind|Encultured|Future of Humanity Institute|Machine Intelligence Research Institute|OpenAI|Ought|Redwood Research',NULL,'Review of current state of cause area','AI safety','This post, cross-posted between LessWrong and the Alignment Forum, goes into detail on the authors'' understanding of various research agendas and the organizations pursuing them.');

/* Random stuff */
insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('https://www.facebook.com/danielfilan/posts/10210393063045457','Claim: if you work in an AI alignment org funded by donations, you should not own much cryptocurrency, since much of your salary comes from people who do','2017-11-18',NULL,'Daniel Filan',NULL,NULL,'Machine Intelligence Research Institute','Miscellaneous commentary','AI safety','The post by Daniel Filan claims that organizations working in AI risk get a large share of their donations from cryptocurrency investors, so their fundraising success is tied to the success of cryptocurrency. For better diversification, therefore, people working at such organizations should not own cryptocurrency. The post has a number of comments from Malo Bourgon of the Machine Intelligence Research Institute, which is receiving a lot of money from cryptocurrency investors in the months surrounding the post date'),
  ('https://forum.effectivealtruism.org/posts/obDjqJGGobT6zggER/what-should-founders-pledge-research#6m8iQ3tNGWuK6sbxB','Thanks for putting up with my follow-up questions. Out of the areas you mention, I''d be very interested in ...','2019-09-10',NULL,'Ryan Carey','Effective Altruism Forum','Founders Pledge|Open Philanthropy','OpenAI|Machine Intelligence Research Institute','Broad donor strategy','AI safety|Global catastrophic risks|Scientific research|Politics','Ryan Carey replies to John Halstead''s question on what Founders Pledge shoud research. He first gives the areas within Halstead''s list that he is most excited about. He also discusses three areas not explicitly listed by Halstead: (a) promotion of effective altruism, (b) scholarships for people working on high-impact research, (c) more on AI safety -- specifically, funding low-mid prestige figures with strong AI safety interest (what he calls "highly-aligned figures"), a segment that he claims the Open Philanthropy Project is neglecting, with the exception of MIRI and a couple of individuals.'),
  ('https://www.lesswrong.com/posts/P3Yt66Wh5g7SbkKuT/how-to-get-into-independent-research-on-alignment-agency','How To Get Into Independent Research On Alignment/Agency','2021-11-18',NULL,'John Wentworth','LessWrong','Effective Altruism Funds: Long-Term Future Fund','Effective Altruism Funds: Long-Term Future Fund','Miscellaneous commentary','AI safety','John Wentworth, an independent AI safety researcher who makes a full-time-equivalent of $90,000 a year and is funded partly by the Long-Term Future Fund (LTFF), explains more about how he does independent research and how he''s able to get paid for it. Wentworth would be cited as a success story of the LTFF by Zvi Mowshowitz in https://www.lesswrong.com/posts/kuDKtwwbsksAW4BG2/zvi-s-thoughts-on-the-survival-and-flourishing-fund-sff#Access_to_Power_and_Money and his post would be cited by the 2021 AI Alignment Review https://forum.effectivealtruism.org/posts/BNQMyWGCNWDdP2WyG/2021-ai-alignment-literature-review-and-charity-comparison#LTFF__Long_term_future_fund "[the post] suggests that LTFF has been crucial to enabling the emergence of independent safety researcher as a viable occupation; this seems like a very major positive for the LTFF."');

/* Jeff Kaufman */
insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('https://www.jefftk.com/p/superintelligence-risk-project-conclusion','Superintelligence Risk Project: Conclusion','2017-09-15',NULL,'Jeff Kaufman',NULL,NULL,'Machine Intelligence Research Institute','Review of current state of cause area','AI safety','This is the concluding post (with links to all earlier posts) of a month-long investigation by Jeff Kaufman into AI risk. Kaufman investigates by reading the work of, and talking with, both people who work in AI risk reduction and people who work on machine learning and AI in industry and academia, but are not directly involved with safety. His conclusion is that there likely should continue to be some work on AI risk reduction, and this should be respected by people working on AI. He is not confident about how the current level and type of work on AI risk compares with the optimal level and type of such work');
