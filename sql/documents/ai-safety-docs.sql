/* AI safety general */

insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('https://forum.effectivealtruism.org/posts/nSot23sAjoZRgaEwa/2016-ai-risk-literature-review-and-charity-comparison','2016 AI Risk Literature Review and Charity Comparison','2016-12-13',NULL,'Ben Hoskin','Effective Altruism Forum','Ben Hoskin','Machine Intelligence Research Institute|Future of Humanity Institute|OpenAI|Center for Human-Compatible AI|Future of Life Institute|Centre for the Study of Existential Risk|Leverhulme Centre for the Future of Intelligence|Global Catastrophic Risk Institute|Global Priorities Project|AI Impacts|Xrisks Institute|X-Risks Net|Center for Applied Rationality|80,000 Hours|Raising for Effective Giving','Review of current state of cause area','AI safety','The lengthy blog post covers all the published work of prominent organizations focused on AI risk. References https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support#sources1007 for the MIRI part of it but notes the absence of information on the many other orgs. The conclusion: "The conclusion: "Donate to both the Machine Intelligence Research Institute and the Future of Humanity Institute, but somewhat biased towards the former. I will also make a smaller donation to the Global Catastrophic Risks Institute."'),
  ('https://www.centreforeffectivealtruism.org/blog/changes-in-funding-in-the-ai-safety-field','Changes in funding in the AI safety field','2017-02-01','2017-02-02','Sebastian Farquhar','Centre for Effective Altruism',NULL,'Machine Intelligence Research Institute|Center for Human-Compatible AI|Leverhulme Centre for the Future of Intelligence|Future of Life Institute|Future of Humanity Institute|OpenAI|MIT Media Lab','Review of current state of cause area','AI safety','The post reviews AI safety funding from 2014 to 2017 (projections for 2017). Cross-posted on EA Forum at http://effective-altruism.com/ea/16s/changes_in_funding_in_the_ai_safety_field/'),
  ('https://forum.effectivealtruism.org/posts/XKwiEpWRdfWo7jy7f/2017-ai-safety-literature-review-and-charity-comparison','2017 AI Safety Literature Review and Charity Comparison','2017-12-20',NULL,'Ben Hoskin','Effective Altruism Forum','Ben Hoskin','Machine Intelligence Research Institute|Future of Humanity Institute|Global Catastrophic Risk Institute|Centre for the Study of Existential Risk|AI Impacts|Center for Human-Compatible AI|Center for Applied Rationality|Future of Life Institute|80,000 Hours','Review of current state of cause area','AI safety','The lengthy blog post covers all the published work of prominent organizations focused on AI risk. It is an annual refresh of https://forum.effectivealtruism.org/posts/nSot23sAjoZRgaEwa/2016-ai-risk-literature-review-and-charity-comparison -- a similar post published a year before it. The conclusion: "Significant donations to the Machine Intelligence Research Institute and the Global Catastrophic Risks Institute. A much smaller one to AI Impacts."'),
  ('https://www.lesswrong.com/posts/XFpDTCHZZ4wpMT8PZ/a-model-i-use-when-making-plans-to-reduce-ai-x-risk','A model I use when making plans to reduce AI x-risk','2018-01-18',NULL,'Ben Pace','LessWrong','Berkeley Existential Risk Initiative',NULL,'Review of current state of cause area','AI safety','The author describes his implicit model of AI risk, with four parts: (1) Alignment is hard, (2) Getting alignment right accounts for most of the variance in whether an AGI system will be positive for humanity, (3) Our current epistemic state regarding AGI timelines will continue until we are close (<2 years from) to having AGI, and (4) Given timeline uncertainty, it is best to spend marginal effort on plans that assume / work in shorter timelines. There is a lot of discussion in the comments'),
  ('https://forum.effectivealtruism.org/posts/HqatEhdEb42vhSo7B/opportunities-for-individual-donors-in-ai-safety','Opportunities for individual donors in AI safety','2018-03-12',NULL,'Alex Flint','Effective Altruism Forum',NULL,'Machine Intelligence Research Institute|Future of Humanity Institute','Review of current state of cause area','AI safety','Alex Flint discusses the history of AI safety funding, and suggests some heuristics for individual donors based on what he has seen to be successful in the past.'),
  ('https://forum.effectivealtruism.org/posts/BznrRBgiDdcTwWWsB/2018-ai-alignment-literature-review-and-charity-comparison','2018 AI Alignment Literature Review and Charity Comparison','2018-12-17',NULL,'Ben Hoskin','Effective Altruism Forum','Ben Hoskin','Machine Intelligence Research Institute|Future of Humanity Institute|Center for Human-Compatible AI|Centre for the Study of Existential Risk|Global Catastrophic Risk Institute|Global Priorities Institute|Australian National University|Berkeley Existential Risk Initiative|Ought|AI Impacts|OpenAI|Effective Altruism Foundation|Foundational Research Institute|Median Group|Convergence Analysis','Review of current state of cause area','AI safety','Cross-posted to LessWrong at https://www.lesswrong.com/posts/a72owS5hz3acBK5xc/2018-ai-alignment-literature-review-and-charity-comparison This is the third post in a tradition of annual blog posts on the state of AI safety and the work of various organizations in the space over the course of the year; the previous two blog posts are at https://forum.effectivealtruism.org/posts/nSot23sAjoZRgaEwa/2016-ai-risk-literature-review-and-charity-comparison and https://forum.effectivealtruism.org/posts/XKwiEpWRdfWo7jy7f/2017-ai-safety-literature-review-and-charity-comparison The post has a "methodological considerations" section that discusses how the author views track records, politics, openness, the research flywheel, near vs far safety research, other existential risks, financial reserves, donation matching, poor quality research, and the Bay Area. The number of organizations reviewed is also larger than in previous years. Excerpts from the conclusion: "Despite having donated to MIRI consistently for many years as a result of their highly non-replaceable and groundbreaking work in the field, I cannot in good faith do so this year given their lack of disclosure. [...] This is the first year I have attempted to review CHAI in detail and I have been impressed with the quality and volume of their work. I also think they have more room for funding than FHI. As such I will be donating some money to CHAI this year. [...] As such I will be donating some money to GCRI again this year. [...] As such I do not plan to donate to AI Impacts this year, but if they are able to scale effectively I might well do so in 2019. [...] I also plan to start making donations to individual researchers, on a retrospective basis, for doing useful work. [...] This would be somewhat similar to Impact Certificates, while hopefully avoiding some of their issues.'),
  ('https://forum.effectivealtruism.org/posts/dpBB24QsnsRnkq5JT/2019-ai-alignment-literature-review-and-charity-comparison','2019 AI Alignment Literature Review and Charity Comparison','2019-12-19',NULL,'Ben Hoskin','Effective Altruism Forum','Ben Hoskin|Effective Altruism Funds: Long-Term Future Fund|Open Philanthropy Project|Survival and Flourising Fund','Future of Humanity Institute|Center for Human-Compatible AI|Machine Intelligence Research Institute|Global Catastrophic Risk Institute|Centre for the Study of Existential Risk|Ought|OpenAI|AI Safety Camp|Future of Life Institute|AI Impacts|Global Priorities Institute|Foundational Research Institute|Median Group|Center for Security and Emerging Technology|Leverhulme Centre for the Future of Intelligence|Berkeley Existential Risk Initiative|AI Pulse','Review of current state of cause area','AI safety','Cross-posted to LessWrong at https://www.lesswrong.com/posts/SmDziGM9hBjW9DKmf/2019-ai-alignment-literature-review-and-charity-comparison This is the fourth post in a tradition of annual blog posts on the state of AI safety and the work of various organizations in the space over the course of the year; the previous year''s post is at https://forum.effectivealtruism.org/posts/BznrRBgiDdcTwWWsB/2018-ai-alignment-literature-review-and-charity-comparison The post has sections on "Research" and "Finance" for a number of organizations working in the AI safety space, many of whom accept donations. A "Capital Allocators" section discusses major players who allocate funds in the space. A lengthy "Methodological Thoughts" section explains how the author approaches some underlying questions that influence his thoughts on all the organizations');

/* Random stuff */
insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('https://www.facebook.com/danielfilan/posts/10210393063045457','Claim: if you work in an AI alignment org funded by donations, you should not own much cryptocurrency, since much of your salary comes from people who do','2017-11-18',NULL,'Daniel Filan',NULL,NULL,'Machine Intelligence Research Institute','Miscellaneous commentary','AI safety','The post by Daniel Filan claims that organizations working in AI risk get a large share of their donations from cryptocurrency investors, so their fundraising success is tied to the success of cryptocurrency. For better diversification, therefore, people working at such organizations should not own cryptocurrency. The post has a number of comments from Malo Bourgon of the Machine Intelligence Research Institute, which is receiving a lot of money from cryptocurrency investors in the months surrounding the post date'),
  ('https://forum.effectivealtruism.org/posts/obDjqJGGobT6zggER/what-should-founders-pledge-research#6m8iQ3tNGWuK6sbxB','Thanks for putting up with my follow-up questions. Out of the areas you mention, I''d be very interested in ...','2019-09-10',NULL,'Ryan Carey','Effective Altruism Forum','Founders Pledge|Open Philanthropy Project','OpenAI|Machine Intelligence Research Institute','Broad donor strategy','AI safety|Global catastrophic risks|Scientific research|Politics','Ryan Carey replies to John Halstead''s question on what Founders Pledge shoud research. He first gives the areas within Halstead''s list that he is most excited about. He also discusses three areas not explicitly listed by Halstead: (a) promotion of effective altruism, (b) scholarships for people working on high-impact research, (c) more on AI safety -- specifically, funding low-mid prestige figures with strong AI safety interest (what he calls "highly-aligned figures"), a segment that he claims the Open Philanthropy Project is neglecting, with the exception of MIRI and a couple of individuals.');

/* Jeff Kaufman */
insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('https://www.jefftk.com/p/superintelligence-risk-project-conclusion','Superintelligence Risk Project: Conclusion','2017-09-15',NULL,'Jeff Kaufman',NULL,NULL,'Machine Intelligence Research Institute','Review of current state of cause area','AI safety','This is the concluding post (with links to all earlier posts) of a month-long investigation by Jeff Kaufman into AI risk. Kaufman investigates by reading the work of, and talking with, both people who work in AI risk reduction and people who work on machine learning and AI in industry and academia, but are not directly involved with safety. His conclusion is that there likely should continue to be some work on AI risk reduction, and this should be respected by people working on AI. He is not confident about how the current level and type of work on AI risk compares with the optimal level and type of such work');
