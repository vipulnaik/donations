# -- TLYCS donation case
insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('http://effective-altruism.com/ea/19k/the_life_you_can_saves_2016_annual_report/',"The Life You Can Save's 2016 Annual Report",# '
   '2017-04-26',NULL,'Jon Behar','The Life You Can Save',NULL,'The Life You Can Save','Donee donation case','Effective altruism/movement growth/fundraising','TLYCS provides an annual report of money moved and influenced');

# -- Donation documentations
insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('http://slatestarcodex.com/2017/02/02/guest-post-the-international-refugee-assistance-program/','[GUEST POST] The International Refugee Assistance Project','2017-02-02',NULL,'Elizabeth Van Nostrand','Slate Star Codex','Elizabeth Van Nostrand','International Refugee Assistance Project','Single donation documentation','Migration policy/refugee assistance','Discusses 100 dollar donation to International Refugee Assistance Project (IRAP) in light of the executive order by Donald Trump on 2017-01-27 banning entry of people from 7 countries. Highlights value of strong existing capacity in the American Civil Liberties Union (ACLU) that allowed for a quick response, and help from the IRAP in doing so. ACLU is well-funded, but IRAP has just gotten off the ground with a grant from the Open Philanthropy Project, so donating to that'),
  ('https://acesounderglass.com/2016/12/01/5167/','Seeing Like A State, Flashlights, and Giving This Year','2016-12-01',NULL,'Elizabeth Van Nostrand','Personal blog','Elizabeth Van Nostrand,Ozy Brennan','Tostan','Third-party case for donation','Education','Argues that donations to things that build capacity could be more valuable than donations to things with directly measured impact. Recommends Tostan for its capacity-building. The post influenced others to donate to Tostan: see https://thingofthings.wordpress.com/2016/12/22/donations-post-2016/ and https://acesounderglass.com/2017/01/05/tracking-donations/'),
  ('http://effective-altruism.com/ea/16b/why_i_donated_to_the_environmental_data/','Why I donated to the Environmental Data & Governance Initiative','2017-01-16',NULL,'Jacob Trefethen','Effective Altruism Forum','Jacob Trefethen','Environmental Data and Governance Initiative','Single donation documentation','Environmentalism/watchdog','Part 1 of the post focused on the case for donating to young, promising organizations. Part 2 explained why the author considered the specific donee a good fit'),
  ('http://effective-altruism.com/ea/15y/where_i_gave_and_why_in_2016/','Where I gave and why in 2016 ','2017-01-06',NULL,'Ben Kuhn','Effective Altruism Forum','Ben Kuhn','EA Giving Group,GiveWell,GiveDirectly','Periodic donation list documentation','Global health and global poverty','Originally posted at http://www.benkuhn.net/giving-2016 and cross-posted to Effective Altruism Forum due to evidence that people like donation write-ups'),
  ('http://effective-altruism.com/ea/14u/eas_write_about_where_they_give/','EAs write about where they give','2016-12-09',NULL,'Julia Wise','Effective Altruism Forum','Blake Borgeson,Eva Vivalt,Ben Kuhn,Alexander Gordon-Brown and Denise Melchin,Elizabeth Van Nostrand','Machine Intelligence Research Institute,Center for Applied Rationality,AidGrade,Charity Science: Health,80000 Hours,Centre for Effective Altruism,Tostan','Periodic donation list documentation','Global health and global poverty, AI risk','Julia Wise got submissions from multiple donors about their donation plans and put them together in a single post. The goal was to cover people outside of organizations that publish such posts for their employees');
  
insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('https://jsteinhardt.wordpress.com/2016/12/28/donations-for-2016','Donations for 2016','2016-12-28',NULL,'Jacob Steinhardt',NULL,'Jacob Steinhardt','Donor lottery,GiveWell top charities,GiveDirectly,Blue Ribbon Study Panel,Carnegie Endowment for International Peace','Periodic donation list documentation','Biosecurity and pandemic preparedness,International relations,Global health and global poverty','Explanation for donation choices, also mention experimental funding of smaller projects');

# -- AI risk general

insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('http://effective-altruism.com/ea/14w/2017_ai_risk_literature_review_and_charity/','2017 AI Risk Literature Review and Charity Comparison','2016-12-13',NULL,'Ben Hoskin','Effective Altruism Forum','Ben Hoskin','Machine Intelligence Research Institute,Future of Humanity Institute,OpenAI,Center for Human-Compatible AI,Future of Life Institute,Centre for the Study of Existential Risk,Leverhulme Centre for the Future of Intelligence,Global Catastrophic Risk Institute,Global Priorities Project,AI Impacts,Xrisks Institute,X-Risks Net,Center for Applied Rationality,80000 Hours,Raising for Effective Giving','Review of current state of cause area','AI risk','The lengthy blog post covers all the published work of prominent organizations focused on AI risk. References http://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support#sources1007 for the MIRI part of it but notes the absence of information on the many other orgs'),
  ('https://www.centreforeffectivealtruism.org/blog/changes-in-funding-in-the-ai-safety-field','Changes in funding in the AI safety field','2017-02-01','2017-02-02','Sebastian Farquhar','Centre for Effective Altruism',NULL,'Machine Intelligence Research Institute,Center for Human-Compatible AI,Leverhulme Centre for the Future of Intelligence,Future of Life Institute,Future of Humanity Institute,OpenAI,MIT Media Lab','Review of current state of cause area','AI risk','The post reviews AI safety funding from 2014 to 2017 (projections for 2017). Cross-posted on EA Forum at http://effective-altruism.com/ea/16s/changes_in_funding_in_the_ai_safety_field/'),
  ('http://effective-altruism.com/ea/1iu/2018_ai_safety_literature_review_and_charity/','2018 AI Safety Literature Review and Charity Comparison','2017-12-20',NULL,'Ben Hoskin','Effective Altruism Forum','Ben Hoskin','Machine Intelligence Research Institute,Future of Humanity Institute,Global Catastrophic Risk Institute,Centre for the Study of Existential Risk,AI Impacts,Center for Human-Compatible AI,Center for Applied Rationality,Future of Life Institute,80000 Hours','Review of current state of cause area','AI risk','The lengthy blog post covers all the published work of prominent organizations focused on AI risk. It is an annual refresh of http://effective-altruism.com/ea/14w/2017_ai_risk_literature_review_and_charity/ -- a similar post published a year before it'),
  ('http://existence.org/reports/2017/09/12/semi-annual-report.html','BERI''s semi-annual report, August','2017-09-12','2017-09-12','Rebecca Raible','Berkeley Existential Risk Initiative',NULL,'Berkeley Existential Risk Initiative','Donee periodic update','AI risk/other global catastrophic risks','A blog post announcing BERI''s semi-annual report.'),
  ('http://existence.org/growth','What we’re thinking about as we grow - ethics, oversight, and getting things done','2017-10-19','2017-10-19','Andrew Critch','Berkeley Existential Risk Initiative',NULL,'Berkeley Existential Risk Initiative','Donee periodic update','AI risk/other global catastrophic risks','Outlines BERI''s approach to growth and "ethics" (transparency, oversight, trust, etc.).'),
  ('http://existence.org/engineering','Forming an engineering team','2017-10-25','2017-10-25','Andrew Critch','Berkeley Existential Risk Initiative',NULL,'Berkeley Existential Risk Initiative','Donee periodic update','AI risk/other global catastrophic risks',NULL),
  ('http://existence.org/computing','Announcing BERI Computing Grants','2017-12-01','2017-12-01','Andrew Critch','Berkeley Existential Risk Initiative',NULL,'Berkeley Existential Risk Initiative','Donee periodic update','AI risk/other global catastrophic risks',NULL);

# -- OpenAI
insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('https://slatestarcodex.com/2015/12/17/should-ai-be-open/','Should AI Be Open?','2015-12-17',NULL,'Scott Alexander','Slate Star Codex',NULL,'OpenAI','Evaluator review of donee','AI risk','Scott Alexander draws a parallel between "open" AI and open nuclear weapons, to highlight that openness is not necessarily good here. Plus a lot of general rambling'),
  ('http://lesswrong.com/r/discussion/lw/oul/openai_makes_humanity_less_safe/','OpenAI makes humanity less safe','2017-04-03',NULL,'Benjamin Hoffman','LessWrong','Open Philanthropy Project','OpenAI','Evaluator review of donee','AI risk','Hoffman argues that OpenAI is good intentions gone awry, and is critical of the Open Philanthropy Project donation to OpenAI');

# -- Older stuff on Deworm the World Initiative
insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('http://endtheneglect.org/2011/01/spotlight-on-deworm-the-world/','Spotlight on Deworm the World','2011-01-26',NULL,'Alanna Shaikh','End the Neglect',NULL,'Deworm the World Initiative','Evaluator review of donee','Global health/deworming','Alanna Shaikh praises Deworm the World for working with existing groups rather than creating redundant, duplicative effort. This is still in the early days of the organization, before it goes under Innovations for Poverty Action and then Evidence Action');

# -- Other

insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('https://issarice.com/against-malaria-foundation','Against Malaria Foundation','2017-04-27','2017-04-28','Issa Rice',NULL,NULL,'Against Malaria Foundation','Evaluator review of donee','Global health/malaria','The document records off-the-cuff thoughts that the author had while creating https://timelines.issarice.com/wiki/Timeline_of_Against_Malaria_Foundation as part of contract work. It is a collection of interrelated but distinct observations. See https://www.facebook.com/vipulnaik.r/posts/10211866218306794 for a Facebook discussion of the post'),
  ('https://issarice.com/open-philanthropy-project-non-grant-funding','Open Philanthropy Project non-grant funding','2017-04-02','2017-05-17','Issa Rice',NULL,'Open Philanthropy Project',NULL,'Miscellaneous commentary',NULL,'The document lists some funding by the Open Philanthropy Project that is publicly disclosed (either by Open Philanthropy Project or by the donee or another reliable source) but is not part of the Open Philanthropy Project grants database, and is not included in employee salaries and benefits.');

# -- Kevin Watkinson critique
insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('http://effective-altruism.com/ea/1e8/should_eas_think_twice_before_donating_to_gfi/','Should EAs think twice before donating to GFI?','2017-08-31',NULL,'Kevin Watkinson','Effective Altruism Forum','Open Philanthropy Project','The Good Food Institute','Third-party case against donation','Animal welfare','The post argues against donations to The Good Food Institute, noting its limited track record as well as the huge amount of funding it is already receiving from the Open Philanthropy Project. This post is made shortly after an exchange between the post author (Kevin Watkinson) and Holden Karnofsky of the Open Philanthropy Project in http://www.openphilanthropy.org/blog/march-2017-open-thread?page=1#comment-305 (the open thread of the Open Philanthropy Project). The post also critiques Animal Charity Evaluators (ACE) for a positive assessment of GFI, and comments include a response from an ACE employee and an ACE board member (neither in an official capacity)');

# -- Sentience Institute docs

insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('http://effective-altruism.com/ea/1ik/sentience_institute_2017_accomplishments_2018/','Sentience Institute 2017 Accomplishments, 2018 Plans, and Room for Funding','2017-12-14',NULL,'Kelly Witwicki','Sentience Institute',NULL,'Sentience Institute','Donee donation case','Animal welfare','The blog post provides an update on the activities of the Sentience Institute since its launch June 2017, its plans for 2018, and its estimate of its own room for more funding ($185,000)');

# -- Effective Altruism Foundation docs

insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('http://effective-altruism.com/ea/1in/effective_altruism_foundation_update_plans_for/','Effective Altruism Foundation update: Plans for 2018 and room for more funding','2017-12-15',NULL,'Jonas Vollmer','Effective Altruism Foundation',NULL,'Effective Altruism Foundation,Raising for Effective Giving,Foundational Research Institute,Wild-Animal Suffering Research','Donee donation case','Effective altruism/movement growth/s-risk reduction','The document describes the 2018 plan and room for more funding of the Effective Altruism Foundation. Subsidiaries include Raising for Effective Giving, Foundational Research Institute, and Wild-Animal Suffering Research, Also cross-posted at https://ea-foundation.org/blog/our-plans-for-2018/ (own blog)');

# -- Global Priorities Institute docs

insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('http://effective-altruism.com/ea/1ij/new_releases_global_priorities_institute_research/','New releases: Global Priorities Institute research agenda and posts we’re hiring for','2017-12-14',NULL,'Michelle Hutchinson','Global Priorities Institute',NULL,'Global Priorities Institute','Donee periodic update','Cause prioritization','Hutchinson reports on the progress and plans for the Global Priorities Institute, housed at Oxford University, and also describes the posts it is hiring for');

# -- Random stuff
insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('https://www.facebook.com/danielfilan/posts/10210393063045457','Claim: if you work in an AI alignment org funded by donations, you should not own much cryptocurrency, since much of your salary comes from people who do','2017-11-18',NULL,'Daniel Filan',NULL,NULL,'Machine Intelligence Research Institute','Miscellaneous commentary','AI risk','The post by Daniel Filan claims that organizations working in AI risk get a large share of their donations from cryptocurrency investors, so their fundraising success is tied to the success of cryptocurrency. For better diversification, therefore, people working at such organizations should not own cryptocurrency. The post has a number of comments from Malo Bourgon of the Machine Intelligence Research Institute, which is receiving a lot of money from cryptocurrency investors in the months surrounding the post date');

# -- Jeff Kaufman
insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('https://www.jefftk.com/p/superintelligence-risk-project-conclusion','Superintelligence Risk Project: Conclusion','2017-09-15',NULL,'Jeff Kaufman',NULL,NULL,'Machine Intelligence Research Institute','Review of current state of cause area','AI risk','This is the concluding post (with links to all earlier posts) of a month-long investigation by Jeff Kaufman into AI risk. Kaufman investigates by reading the work of, and talking with, both people who work in AI risk reduction and people who work on machine learning and AI in industry and academia, but are not directly involved with safety. His conclusion is that there likely should continue to be some work on AI risk reduction, and this should be respected by people working on AI. He is not confident about how the current level and type of work on AI risk compares with the optimal level and type of such work');

