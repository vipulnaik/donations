insert into documents(url,title,publication_date,modified_date,author,publisher,affected_donors,affected_donees,document_scope,cause_area,notes) values
  ('http://effective-altruism.com/ea/1fn/oxford_prioritisation_project_review/','Oxford Prioritisation Project Review','2017-10-12',NULL,'Tom Sittler|Jacob L','Effective Altruism Forum','Oxford Prioritisation Project',NULL,'Evaluator retrospective',NULL,'The two main people who coordinated the Oxford Prioritisation Project look back on the experience, and highlight the major lessons for themselves and others'),
  ('https://oxpr.io/blog/2017/5/20/four-quantiative-models-aggregation-and-final-decision','Four quantiative models, aggregation, and final decision','2017-05-20',NULL,'Tom Sittler','Oxford Prioritisation Project','Oxford Prioritisation Project','80,000 Hours|Animal Charity Evaluators|Machine Intelligence Research Institute|StrongMinds','Single donation documentation','Effective altruism/career advice','The post describes how the Oxford Prioritisation Project compared its four finalists (80000 Hours, Animal Charity Evaluators, Machine Intelligence Research Institute, and StrongMinds) by building quantitative models for each, including modeling of uncertainties. Based on these quantitative models, 80000 Hours was chosen as the winner. Also posted to http://effective-altruism.com/ea/1ah/four_quantiative_models_aggregation_and_final/ for comments'),
  ('https://oxpr.io/blog/2017/5/20/a-model-of-the-machine-intelligence-research-institute','A model of the Machine Intelligence Research Institute','2017-05-20',NULL,'Sindy Li','Oxford Prioritisation Project','Oxford Prioritisation Project','Machine Intelligence Research Institute','Evaluator review of donee','AI safety','The post describes a quantitative model of the Machine Intelligence Research Institute, available at https://www.getguesstimate.com/models/8789 on Guesstimate. Also posted to http://effective-altruism.com/ea/1ae/a_model_of_the_machine_intelligence_research/ for comments'),
  ('https://oxpr.io/blog/2017/5/20/a-model-of-animal-charity-evaluators','A model of Animal Charity Evaluators','2017-05-20',NULL,'Dominik Peters|Tom Sittler','Oxford Prioritisation Project','Oxford Prioritisation Project','Animal Charity Evaluators','Evaluator review of donee','Animal welfare/charity evaluator','This post describes a quantitative model of Animal Charity Evaluators, available at https://repl.it/HWJZ/23 on repl.it. It continues a previous post http://effective-altruism.com/ea/19g/charity_evaluators_first_model_and_open_questions/ that includes some open questions. Also posted to http://effective-altruism.com/ea/1af/a_model_of_animal_charity_evaluators_oxford/ for comments'),
  ('https://oxpr.io/blog/2017/5/13/a-model-of-strongminds','A model of StrongMinds','2017-05-13',NULL,'Lovisa Tenberg|Konstantin Sietzy','Oxford Prioritisation Project','Oxford Prioritisation Project','StrongMinds','Evaluator review of donee','Mental health','The post describes a detailed, annotated translation in Guesstimate at https://www.getguesstimate.com/models/8753 of a model of StrongMinds built in 2016 by James Snowden of the Centre for Effective Altruism. The blog post serves as an appendix to the Guesstimate. Also posted at http://effective-altruism.com/ea/1a0/a_model_of_strongminds_oxford_prioritisation/ for comments'),
  ('https://oxpr.io/blog/2017/5/13/a-model-of-80000-hours','A model of 80,000 Hours','2017-05-14',NULL,'Sindy Li','Oxford Prioritisation Project','Oxford Prioritisation Project','80,000 Hours','Evaluator review of donee','Effective altruism/movement growth','The post supplements a model built in Guesstimate to estimate the impact of 80000 Hours. The model is a Monte Carlo model. Also posted to http://effective-altruism.com/ea/1a1/a_model_of_80000_hours_oxford_prioritisation/ for comments'),
  ('https://oxpr.io/blog/2017/4/27/how-much-does-work-in-ai-safety-help-the-world-probability-distribution-version','How much does work in AI safety help the world? Probability distribution version','2017-04-26',NULL,'Tom Sittler','Oxford Prioritisation Project','Oxford Prioritisation Project',NULL,'Review of current state of cause area','AI safety','Tom Sittler discusses a model created by the Global Priorities Project (GPP) to assess the value of work in AI safety. He has converted the model to a Guesstimate model availabe at https://www.getguesstimate.com/models/8697 and wants comments. Also cross-posted to http://effective-altruism.com/ea/19r/how_much_does_work_in_ai_safety_help_the_world/ looking for comments'),
  ('https://oxpr.io/blog/2017/3/11/qays-langan-dathi-ai-safety','AI Safety: Is it worthwhile for us to look further into donating into AI research?','2017-03-11',NULL,'Qays Langan-Dathi','Oxford Prioritisation Project','Oxford Prioritisation Project','Machine Intelligence Research Institute','Review of current state of cause area','AI safety','The post concludes: "In conclusion my answer to my main point is, yes. There is a good chance that AI risk prevention is the most cost effective focus area for saving the most amount of lives with or without regarding future human lives."'),
  ('https://oxpr.io/blog/2017/3/11/daniel-may-should-we-make-a-grant-to-a-meta-charity','Should we make a grant to a meta-charity?','2017-03-11',NULL,'Daniel May','Oxford Prioritisation Project','Oxford Prioritisation Project','Giving What We Can|80,000 Hours|Raising for Effective Giving','Review of current state of cause area','Effective altruism/movement growth/fundraising','The summary says: "I introduce the concept of meta-charity, discuss some considerations for OxPrio, and look into how meta-charities evaluate their impact, and the reliability of these figures for our purposes (finding the most cost-effective organisation to donate £10,000 today). I then look into the room for more funding for a few meta-charities, and finally conclude that these are worth seriously pursuing further." See http://effective-altruism.com/ea/189/daniel_may_should_we_make_a_grant_to_a/ for a cross-post that has comments'),
  ('https://oxpr.io/blog/2017/4/19/modelling-the-good-food-institute','Modelling the Good Food Institute','2017-04-18',NULL,'Dominik Peters','Oxford Prioritisation Project','Oxford Prioritisation Project','The Good Food Institute','Evaluator review of donee','Animal welfare/meat alternatives','The summary says: "We have attempted to build a quantitative model to estimate the impact of the Good Food Institute (GFI). We have found this exceptionally difficult due to the diversity of GFI’s activities and the particularly unclear counterfactuals. In this post, I explain some of the modelling approaches we tried, and why we are not satisfied with them."'),
  ('https://oxpr.io/blog/2017/4/25/charity-evaluators-a-first-model-and-open-questions','Charity evaluators: a first model and open questions','2017-04-25',NULL,'Dominik Peters|Tom Sittler','Oxford Prioritisation Project','Oxford Prioritisation Project','GiveWell|Animal Charity Evaluators','Review of current state of cause area','Charity evaluator','The abstract says: "We describe a simple simulation model for the recommendations of a charity evaluator like GiveWell or ACE. The model captures some real-world phenomena, such as initial overconfidence in impact estimates. We are unsure how to choose the parameters of the underlying distributions, and are happy to receive feedback on this." See http://effective-altruism.com/ea/19g/charity_evaluators_first_model_and_open_questions/ for a cross-post with comments'),
  ('https://oxpr.io/blog/2017/2/28/final-donation-decision-version-0','Final decision: Version 0','2017-03-01',NULL,'Tom Sittler','Oxford Prioritisation Project','Oxford Prioritisation Project','Against Malaria Foundation|Machine Intelligence Research Institute|The Good Food Institute|StrongMinds','Reasoning supplement',NULL,'Version 0 of a decision process for what charity to grant 10,000 UK pouds to. Result was a tie between Machine Intelligence Research Institute and StrongMinds. See http://effective-altruism.com/ea/187/oxford_prioritisation_project_version_0/ for a cross-post with comments'),
  ('https://oxpr.io/blog/2017/2/28/daniel-may-current-view-machine-intelligence-research-institute','Daniel May: current view, Machine Intelligence Research Institute','2017-02-15',NULL,'Daniel May','Oxford Prioritisation Project','Oxford Prioritisation Project','Machine Intelligence Research Institute','Evaluator review of donee','AI safety','Daniel May evaluates the Machine Intelligence Research Institute and describes his reasons for considering it the best donation opportunity'),
  ('https://oxpr.io/blog/2017/2/28/lovisa-tengberg-current-view-strongminds','Lovisa Tengberg: current view, StrongMinds','2017-02-14',NULL,'Lovisa Tengberg','Oxford Prioritisation Project','Oxford Prioritisation Project','StrongMinds|Against Malaria Foundation','Evaluator review of donee','Mental health','Lovisa Tengberg evaluates StrongMinds and argues that it could be the best donation opportunities. Other candidates mentioned, all in the area of mental health, are Alderman Foundation, AEGIS Foundation, and Network for Empowerment and Progressive Initiative'),
  ('https://oxpr.io/blog/2017/2/28/konstantin-sietzy-current-view-strongminds','Konstantin Sietzy: current view, StrongMinds','2017-02-21',NULL,'Konstantin Sietzy','Oxford Prioritisation Project','Oxford Prioritisation Project','StrongMinds|Machine Intelligence Research Institute','Evaluator review of donee','Mental health','Konstantin Sietzy explains why StrongMinds is the best charity in his view. Also lists Machine Intelligence Research Institute as the runner-up'),
  ('https://oxpr.io/blog/2017/2/11/tom-sittler-assumptions-of-arguments-for-existential-risk-reduction','Tom Sittler: Assumptions of arguments for existential risk reduction','2017-01-27','2017-02-10','Tom Sittler','Oxford Prioritisation Project','Oxford Prioritisation Project',NULL,'Review of current state of cause area','AI safety','The abstract reads: "I review an informal argument for existential risk reduction as the top priority. I argue the informal argument, or at least some renditions of it, are vulnerable to two objections: (i) The far future may not be good, and we are making predictions based on very weak evidence when we estimate whether it will be good (ii) reductions in existential risk over the next century are much less valuable than equivalent increases in the probability that humanity will have a very long future."'),
  ('https://oxpr.io/blog/2017/2/13/crispr-biorisk-as-an-oxford-prioritisation-project-topic','Laurie Pycroft: "CRISPR biorisk as an Oxford Prioritisation Project topic"','2017-02-13',NULL,'Laurie Pycroft','Oxford Prioritisation Project','Oxford Prioritisation Project',NULL,'Review of current state of cause area','Biosecurity and pandemic preparedness','The article explores CRISPR biorisk as a potential funding opportunity for the Oxford Prioritisation Project'),
  ('https://oxpr.io/blog/2017/2/13/another-brick-in-the-wall','Another brick in the wall?','2017-02-13',NULL,'Tom Sittler|Konstantin Sietzy|Jacob L','Oxford Prioritisation Project','Oxford Prioritisation Project',NULL,'Broad donor strategy',NULL,'The summary begins: "Should the Oxford Prioritisation Project focus on donation opportunities that are ‘the right size’? Is it important to find a £10,000 funding gap for a specific purchase, (or by way of analogy, £10,000-shaped lego bricks)?" The conclusion is that Lego bricks are unlikely to be relevant'),
  ('https://oxpr.io/blog/2017/2/15/daniel-may-open-science-little-room-for-more-funding','Daniel May: "Open Science: little room for more funding."','2017-02-15',NULL,'Daniel May','Oxford Prioritisation Project','Oxford Prioritisation Project|Laura and John Arnold Foundation|Open Philanthropy',NULL,'Review of current state of cause area','Scientific research','The summary states: "I consider open science as a cause area, by reviewing Open Phil’s published work, as well as some popular articles and research, and assessing the field for scale, neglectedness, and tractability. I conclude that the best giving opportunities will likely be filled by foundations such as LJAF and Open Phil, and recommend that the Oxford Prioritisation Project focusses elsewhere." Also available as a Google Doc at https://docs.google.com/document/d/13wsMAugRacu52EPZo6-7NJh4QuYayKyIbjChwU0KsVU/edit?usp=sharing and at the Effective Altruism Forum at http://effective-altruism.com/ea/17g/daniel_may_open_science_little_room_for_more/ (10 comments)'),
  ('https://oxpr.io/blog/2017/2/19/sindy-li-current-view-against-malaria-foundation','Sindy li: current view, Against Malaria Foundation','2017-02-19',NULL,'Sindy Li','Oxford Prioritisation Project','Oxford Prioritisation Project','Against Malaria Foundation|Schistosomiasis Control Initiative|Drugs for Neglected Diseases Initiative','Evaluator review of donee','Global health','Sindy Li provides her best guess as to the best opportunity for the Oxford Prioritisation Project, saying it is the Against Malaria Foundation. Her analysis relies on the GiveWell cost-effectiveness estimates. She identifies mental health as another area (citing https://oxpr.io/blog/2017/2/28/lovisa-tengberg-current-view-strongminds by Lovisa Tengberg and https://oxpr.io/blog/2017/2/28/konstantin-sietzy-current-view-strongminds by Konstantin Sietzy) that she might look into more'),
  ('https://oxpr.io/blog/2017/2/28/tom-sittler-current-view-machine-intelligence-research-institute','Tom Sittler: current view, Machine Intelligence Research Institute','2017-02-08',NULL,'Tom Sittler','Oxford Prioritisation Project','Oxford Prioritisation Project','Machine Intelligence Research Institute|Future of Humanity Institute','Evaluator review of donee','AI safety','Tom Sittler explains why he considers the Machine Intelligence Research Institute the best donation opportunity. Cites http://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support http://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity http://effective-altruism.com/ea/14c/why_im_donating_to_miri_this_year/ http://effective-altruism.com/ea/14w/2017_ai_risk_literature_review_and_charity/ and mentions Michael Dickens model as a potential reason to update');
  # -- ('https://oxpr.io/blog/2017/2/10/sindy-li-does-research-into-neglected-tropical-diseases-dominate-givewell-top-interventions',
