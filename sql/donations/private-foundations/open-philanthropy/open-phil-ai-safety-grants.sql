/* This document covers donations by the Open Philanthropy Project that are (almost) completely for AI safety projects or AI safety organizations.
 There are a few other grants related loosely to AI safety at open-phil-other-gcr-and-security-grants.sql */

/* Grants to the Machine Intelligence Research Institute (MIRI) */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision,donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','Machine Intelligence Research Institute',500000,'2016-08-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/', NULL, NULL, '2016-09-06', 'day', 'https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/XkSl27jBDZ8',NULL,
  /* donation_process */ 'The grant page describes the process in Section 1. Background and Process. "Open Philanthropy Project staff have been engaging in informal conversations with MIRI for a number of years. These conversations contributed to our decision to investigate potential risks from advanced AI and eventually make it one of our focus areas. [...] We attempted to assess MIRI’s research primarily through detailed reviews of individual technical papers. MIRI sent us five papers/results which it considered particularly noteworthy from the last 18 months: [...] This selection was somewhat biased in favor of newer staff, at our request; we felt this would allow us to better assess whether a marginal new staff member would make valuable contributions. [...] All of the papers/results fell under a category MIRI calls “highly reliable agent design”.[...] Papers 1-4 were each reviewed in detail by two of four technical advisors (Paul Christiano, Jacob Steinhardt, Christopher Olah, and Dario Amodei). We also commissioned seven computer science professors and one graduate student with relevant expertise as external reviewers. Papers 2, 3, and 4 were reviewed by two external reviewers, while Paper 1 was reviewed by one external reviewer, as it was particularly difficult to find someone with the right background to evaluate it. [...] A consolidated document containing all public reviews can be found here." The link is to https://www.openphilanthropy.org/files/Grants/MIRI/consolidated_public_reviews.pdf "In addition to these technical reviews, Daniel Dewey independently spent approximately 100 hours attempting to understand MIRI’s research agenda, in particular its relevance to the goals of creating safer and more reliable advanced AI. He had many conversations with MIRI staff members as a part of this process. Once all the reviews were conducted, Nick, Daniel, Holden, and our technical advisors held a day-long meeting to discuss their impressions of the quality and relevance of MIRI’s research. In addition to this review of MIRI’s research, Nick Beckstead spoke with MIRI staff about MIRI’s management practices, staffing, and budget needs.',
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'The grant page, Section 3.1 Budget and room for more funding, says: "MIRI operates on a budget of approximately $2 million per year. At the time of our investigation, it had between $2.4 and $2.6 million in reserve. In 2015, MIRI’s expenses were $1.65 million, while its income was slightly lower, at $1.6 million. Its projected expenses for 2016 were $1.8-2 million. MIRI expected to receive $1.6-2 million in revenue for 2016, excluding our support. Nate Soares, the Executive Director of MIRI, said that if MIRI were able to operate on a budget of $3-4 million per year and had two years of reserves, he would not spend additional time on fundraising. A budget of that size would pay for 9 core researchers, 4-8 supporting researchers, and staff for operations, fundraising, and security. Any additional money MIRI receives beyond that level of funding would be put into prizes for open technical questions in AI safety. MIRI has told us it would like to put $5 million into such prizes."',
  /* intended_funding_timeframe_in_months */ '12',
  /* donor_donee_reason */ 'The grant page, Section 3.2 Case for the grant, gives five reasons: (1) Uncertainty about technical assessment (i.e., despite negative technical assessment, there is a chance that MIRI''s work is high-potential), (2) Increasing research supply and diversity in the important-but-neglected AI safety space, (3) Potential for improvement of MIRI''s research program, (4) Recognition of MIRI''s early articulation of the value alignment problem, (5) Other considerations: (a) role in starting CFAR and running SPARC, (b) alignment with effective altruist values, (c) shovel-readiness, (d) "participation grant" for time spent in evaluation process, (e) grant in advance of potential need for significant help from MIRI for consulting on AI safety',
  /* donor_amount_reason */ 'The maximal funding that Open Phil would give MIRI would be $1.5 million per year. However, Open Phil recommended a partial amount, due to some reservations, described on the grant page, Section 2 Our impression of MIRI’s Agent Foundations research: (1) Assessment that it is not likely relevant to reducing risks from advanced AI, especially to the risks from transformative AI in the next 20 years, (2) MIRI has not made much progress toward its agenda, with internal and external reviewers describing their work as technically nontrivial, but unimpressive, and compared with what an unsupervised graduate student could do in 1 to 3 years. Section 3.4 says: "We ultimately settled on a figure that we feel will most accurately signal our attitude toward MIRI. We feel $500,000 per year is consistent with seeing substantial value in MIRI while not endorsing it to the point of meeting its full funding needs."',
  /* donor_timing_reason */ 'No specific timing-related considerations are discussed',
  /* donor_next_donation_thoughts */ 'Section 4 Plans for follow-up says: "As of now, there is a strong chance that we will renew this grant next year. We believe that most of our important open questions and concerns are best assessed on a longer time frame, and we believe that recurring support will help MIRI plan for the future. Two years from now, we are likely to do a more in-depth reassessment. In order to renew the grant at that point, we will likely need to see a stronger and easier-to-evaluate case for the relevance of the research we discuss above, and/or impressive results from the newer, machine learning-focused agenda, and/or new positive impact along some other dimension."',
  /* donor_retrospective */ 'Although there is no explicit retrospective of this grant, the two most relevant followups are Daniel Dewey''s blog post https://forum.effectivealtruism.org/posts/SEL9PW8jozrvLnkb4/my-current-thoughts-on-miri-s-highly-reliable-agent-design (not an official MIRI statement, but Dewey works on AI safety grants for Open Phil) and the three-year $1.25 million/year grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support-2017 made in October 2017 (about a year after this grant). The more-than-doubling of the grant amount and the three-year commitment are both more positive for MIRI than the expectations at the time of the original grant',
  /* notes */ 'The grant page links to commissioned reviews at http://files.openphilanthropy.org/files/Grants/MIRI/consolidated_public_reviews.pdf The grant is also announced on the MIRI website at https://intelligence.org/2016/08/05/miri-strategy-update-2016/'),

  ('Open Philanthropy','Machine Intelligence Research Institute',3750000,'2017-10-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support-2017','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Nick Beckstead','2017-11-08','day','https://groups.google.com/a/openphilanthropy.org/forum/#!msg/newly.published/sym4vNvqFbw/d3CCxBZ1BgAJ',NULL,
  /* donation_process */ 'The donor, Open Philanthropy Project, appears to have reviewed the progress made by MIRI one year after the one-year timeframe for the previous grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support ended. The full process is not described, but the July 2017 post https://forum.effectivealtruism.org/posts/SEL9PW8jozrvLnkb4/my-current-thoughts-on-miri-s-highly-reliable-agent-design suggests that work on the review had been going on well before the grant renewal date',
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'According to the grant page: "MIRI expects to use these funds mostly toward salaries of MIRI researchers, research engineers, and support staff." ',
  /* intended_funding_timeframe_in_months */ '36',
  /* donor_donee_reason */ 'The reasons for donating to MIRI remain the same as the reasons for the previous grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support made in August 2016, but with two new developments: (1) a very positive review of MIRI’s work on “logical induction” by a machine learning researcher who (i) is interested in AI safety, (ii) is rated as an outstanding researcher by at least one of Open Phil''s close advisors, and (iii) is generally regarded as outstanding by the ML. (2) An increase in AI safety spending by Open Phil, so that Open Phil is "therefore less concerned that a larger grant will signal an outsized endorsement of MIRI’s approach." The skeptical post https://forum.effectivealtruism.org/posts/SEL9PW8jozrvLnkb4/my-current-thoughts-on-miri-s-highly-reliable-agent-design by Daniel Dewey of Open Phil, from July 2017, is not discussed on the grant page',
  /* donor_amount_reason */ 'The grant page explains "We are now aiming to support about half of MIRI’s annual budget." In the previous grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support of $500,000 made in August 2016, Open Phil had expected to grant about the same amount ($500,000) after one year. The increase to $3.75 million over three years (or $1.25 million/year) is due to the two new developments: (1) a very positive review of MIRI’s work on “logical induction” by a machine learning researcher who (i) is interested in AI safety, (ii) is rated as an outstanding researcher by at least one of Open Phil''s close advisors, and (iii) is generally regarded as outstanding by the ML. (2) An increase in AI safety spending by Open Phil, so that Open Phil is "therefore less concerned that a larger grant will signal an outsized endorsement of MIRI’s approach."',
  /* donor_timing_reason */ 'The timing is mostly determined by the end of the one-year funding timeframe of the previous grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support made in August 2016 (a little over a year before this grant)',
  /* donor_next_donation_thoughts */ 'The MIRI blog post https://intelligence.org/2017/11/08/major-grant-open-phil/ says: "The Open Philanthropy Project has expressed openness to potentially increasing their support if MIRI is in a position to usefully spend more than our conservative estimate, if they believe that this increase in spending is sufficiently high-value, and if we are able to secure additional outside support to ensure that the Open Philanthropy Project isn’t providing more than half of our total funding."',
  /* donor_retrospective */ NULL,
  /* notes */ 'MIRI, the grantee, blogs about the grant at https://intelligence.org/2017/11/08/major-grant-open-phil/ Open Phil''s statement that due to its other large grants in the AI safety space, it is "therefore less concerned that a larger grant will signal an outsized endorsement of MIRI’s approach." is discussed in the comments on the Facebook post https://www.facebook.com/vipulnaik.r/posts/10213581410585529 by Vipul Naik'),
  ('Open Philanthropy','Machine Intelligence Research Institute',150000,'2018-06-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-ai-safety-retraining-program','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Claire Zabel','2018-06-27','day',NULL,NULL, /* date adjusted */
  /* donation_process */ 'The grant is a discretionary grant, so the approval process is short-circuited; see https://www.openphilanthropy.org/giving/grants/discretionary-grants for more',
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to suppport the artificial intelligence safety retraining project. MIRI intends to use these funds to provide stipends, structure, and guidance to promising computer programmers and other technically proficient individuals who are considering transitioning their careers to focus on potential risks from advanced artificial intelligence. MIRI believes the stipends will make it easier for aligned individuals to leave their jobs and focus full-time on safety. MIRI expects the transition periods to range from three to six months per individual. The MIRI blog post https://intelligence.org/2018/09/01/summer-miri-updates/ says: "Buck [Shlegeris] is currently selecting candidates for the program; to date, we’ve made two grants to individuals."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'The grant is mentioned by MIRI in https://intelligence.org/2018/09/01/summer-miri-updates/'),

  ('Open Philanthropy','Machine Intelligence Research Institute',2652500,'2019-02-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support-2019','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Claire Zabel|Committee for Effective Altruism Support','2019-04-01','day',NULL,NULL, /* date adjusted */
  /* donation_process */ 'The decision of whether to donate seems to have followed the Open Philanthropy Project''s usual process, but the exact amount to donate was determined by the Committee for Effective Altruism Support using the process described at https://www.openphilanthropy.org/committee-effective-altruism-support',
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'MIRI plans to use these funds for ongoing research and activities related to AI safety. Planned activities include alignment research, a summer fellows program, computer scientist workshops, and internship programs.',
  /* intended_funding_timeframe_in_months */ '24',
  /* donor_donee_reason */ 'The grant page says: "we see the basic pros and cons of this support similarly to what we’ve presented in past writeups on the matter" Past writeups include the grant pages for the October 2017 three-year support https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support-2017 and the August 2016 one-year support https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support',
  /* donor_amount_reason */ 'Amount decided by the Committee for Effective Altruism Support (CEAS) https://www.openphilanthropy.org/committee-effective-altruism-support but individual votes and reasoning are not public. Two other grants with amounts decided by CEAS, made at the same time and therefore likely drawing from the same money pot, are to the Centre for Effective Altruism ($2,756,250) and 80,000 Hours ($4,795,803). The original amount of $2,112,500 is split across two years, and therefore ~$1.06 million per year. https://intelligence.org/2019/04/01/new-grants-open-phil-beri/ clarifies that the amount for 2019 is on top of the third year of three-year $1.25 million/year support announced in October 2017, and the total $2.31 million represents Open Phil''s full intended funding for MIRI for 2019, but the amount for 2020 of ~$1.06 million is a lower bound, and Open Phil may grant more for 2020 later. In November 2019, additional funding would bring the total award amount to $2,652,500.',
  /* donor_timing_reason */ 'Reasons for timing are not discussed, but likely reasons include: (1) The original three-year funding period https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support-2017 is coming to an end, (2) Even though there is time before the funding period ends, MIRI has grown in budget and achievements, so a suitable funding amount could be larger, (3) The Committee for Effective Altruism Support https://www.openphilanthropy.org/committee-effective-altruism-support did its first round of money allocation, so the timing is determined by the timing of that allocation round.',
  /* donor_next_donation_thoughts */ 'According to https://intelligence.org/2019/04/01/new-grants-open-phil-beri/ Open Phil may increase its level of support for 2020 beyond the ~$1.06 million that is part of this grant.',
  /* donor_retrospective */ 'The much larger followup grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support-2020 with a very similar writeup suggests that Open Phil and the Committee for Effective Altruism Support would continue to stand by the reasoning for the grant.',
  /* notes */ 'The grantee, MIRI, discusses the grant on its website at https://intelligence.org/2019/04/01/new-grants-open-phil-beri/ along with a $600,000 grant from the Berkeley Existential Risk Initiative.'),

  ('Open Philanthropy','Machine Intelligence Research Institute',7703750,'2020-02-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support-2020','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Claire Zabel|Committee for Effective Altruism Support','2020-04-10','day',NULL,NULL, /* date adjusted */
  /* donation_process */ 'The decision of whether to donate seems to have followed the Open Philanthropy Project''s usual process, but the exact amount to donate was determined by the Committee for Effective Altruism Support using the process described at https://www.openphilanthropy.org/committee-effective-altruism-support',
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'MIRI plans to use these funds for ongoing research and activities related to AI safety',
  /* intended_funding_timeframe_in_months */ 24,
  /* donor_donee_reason */ 'The grant page says "we see the basic pros and cons of this support similarly to what we’ve presented in past writeups on the matter" with the most similar previous grant being https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support-2019 (February 2019). Past writeups include the grant pages for the October 2017 three-year support https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support-2017 and the August 2016 one-year support https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-general-support',
  /* donor_amount_reason */ 'The amount is decided by the Committee for Effective Altruism Support https://www.openphilanthropy.org/committee-effective-altruism-support but individual votes and reasoning are not public. Three other grants decided by CEAS at around the same time are: Centre for Effective Altruism ($4,146,795), 80,000 Hours ($3,457,284), and Ought ($1,593,333).',
  /* donor_timing_reason */ 'Reasons for timing are not discussed, but this is likely the time when the Committee for Effective Altruism Support does its 2020 allocation.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'The donee describes the grant in the blog post https://intelligence.org/2020/04/27/miris-largest-grant-to-date/ (2020-04-27) along with other funding it has received ($300,000 from the Berkeley Existential Risk Initiative and $100,000 from the Long-Term Future Fund). The fact that the grant is a two-year grant is mentioned here, but not in the grant page on Open Phil''s website. The page also mentions that of the total grant amount of $7.7 million, $6.24 million is coming from Open Phil''s normal funders (Good Ventures) and the remaining $1.46 million is coming from Ben Delo, co-founder of the cryptocurrency trading platform BitMEX, as part of a funding partnership https://www.openphilanthropy.org/blog/co-funding-partnership-ben-delo announced November 11, 2019.');

/* Grants to Berkeley Existential Risk Initiative (BERI) */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision,donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','Berkeley Existential Risk Initiative',403890,'2017-07-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-core-support-and-chai-collaboration/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Center for Human-Compatible AI','Daniel Dewey','2017-09-28','day','https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/5UjOZN6KlWQ',NULL,
  /* donation_process */ 'BERI submitted a grant proposal at https://www.openphilanthropy.org/files/Grants/BERI/BERI_Grant_Proposal_2017.pdf',
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to support work with the Center for Human-Compatible AI (CHAI) at UC Berkeley, to which the Open Philanthropy Project provided a two-year founding grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-center-human-compatible-ai The funding is intended to help BERI hire contractors and part-time employees to help CHAI, such as web development and coordination support, research engineers, software developers, or research illustrators. This funding is also intended to help support BERI’s core staff. More in the grant proposal https://www.openphilanthropy.org/files/Grants/BERI/BERI_Grant_Proposal_2017.pdf',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'The grant page says: "Our impression is that it is often difficult for academic institutions to flexibly spend funds on technical, administrative, and other support services. We currently see BERI as valuable insofar as it can provide CHAI with these types of services, and think it’s plausible that BERI will be able to provide similar help to other academic institutions in the future."',
  /* donor_amount_reason */ 'The grantee submitted a budget for the CHAI collaboration project at https://www.openphilanthropy.org/files/Grants/BERI/BERI_Budget_for_CHAI_Collaboration_2017.xlsx',
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Berkeley Existential Risk Initiative',250000,'2019-01-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-chai-ml-engineers/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Center for Human-Compatible AI','Daniel Dewey','2019-03-04','day',NULL,NULL, /* date adjusted */
  /* donation_process */ 'The grant page describes the donation decision as being based on "conversations with various professors and students"',
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to temporarily or permanently hire machine learning research engineers dedicated to BERI’s collaboration with the Center for Human-compatible Artificial Intelligence (CHAI).',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'The grant page says: "Based on conversations with various professors and students, we believe CHAI could make more progress with more engineering support."',
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/berkeley-existential-risk-initiative-chai-collaboration-2019 suggests that the donor would continue to stand behind the reasoning for the grant.',
  /* notes */ 'Follows previous support https://www.openphilanthropy.org/grants/uc-berkeley-center-for-human-compatible-ai-2016/ for the launch of CHAI and previous grant https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-core-support-and-chai-collaboration/ to collaborate with CHAI.'),

  ('Open Philanthropy','Berkeley Existential Risk Initiative',705000,'2019-11-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-chai-collaboration-2019/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Center for Human-Compatible AI','Daniel Dewey','2019-12-13','day',NULL,NULL, /* date adjusted */
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'The grant page says the grant is "to support continued work with the Center for Human-Compatible AI (CHAI) at UC Berkeley. This includes one year of support for machine learning researchers hired by BERI, and two years of support for CHAI."',
  /* intended_funding_timeframe_in_months */ 24,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-chai-collaboration-2022/ from Open Philanthropy to BERI for the same purpose (CHAI collaboration) suggests satisfaction with the outcome of the grant.',
  /* notes */ 'Open Phil makes a grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-center-human-compatible-ai-2019 to the Center for Human-Compatible AI at the same time (November 2019).'),

  ('Open Philanthropy','Berkeley Existential Risk Initiative',150000,'2020-01-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-general-support/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Claire Zabel',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'The grant page says: "BERI seeks to reduce existential risks to humanity, and collaborates with other long-termist organizations, including the Center for Human-Compatible AI at UC Berkeley. This funding is intended to help BERI establish new collaborations."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-general-support-2/ suggests continued satisfaction with the grantee.',
  /* notes */ NULL),

  ('Open Philanthropy','Berkeley Existential Risk Initiative',210000,'2021-03-01','month','donation log','AI safety/technical research/talent pipeline','https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-seri-summer-fellowships/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Stanford Existential Risks Initiative','Claire Zabel',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to provide stipends for the Stanford Existential Risks Initiative (SERI) summer research fellowship program."',
  /* intended_funding_timeframe_in_months */ 2, /* rough estimate of what a "summer" is like */
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The multiple future grants https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-seri-mats-program-2/ https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-seri-mats-program/ and https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-machine-learning-alignment-theory-scholars/ from Open Philanthropy to BERI for the SERI-MATS program, a successor of sorts to this program, suggests satisfaction with the outcome of this grant.',
  /* notes */ NULL),

  ('Open Philanthropy','Berkeley Existential Risk Initiative',195000,'2021-11-01','month','donation log','AI safety/technical research/talent pipeline','https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-seri-mats-program-2/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','SERI-MATS program',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support its collaboration with the Stanford Existential Risks Initiative (SERI) on the SERI ML Alignment Theory Scholars (MATS) Program. MATS is a two-month program where students will research problems related to AI alignment while supervised by a mentor."',
  /* intended_funding_timeframe_in_months */ 6, /* funding is for one cohort; the whole process from applications to training/research/extension for the cohort is about 6 months; more helpfully, there are two cohorts per year, so the amortized duration of a cohort is 6 months */
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grants https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-seri-mats-program/ and https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-machine-learning-alignment-theory-scholars/ for the second and third cohort of the SERI-MATS program suggests the donor''s continued satisfaction with the SERI-MATS program.',
  /* notes */ 'See https://www.serimats.org/program for details of the program including its timeline. Although the research phase of the timeline is just two months, the application process, training phase, and extension phase together make up about half a year.'),

  ('Open Philanthropy','Berkeley Existential Risk Initiative',1126160,'2022-02-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-chai-collaboration-2022/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Center for Human-Compatible AI',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support continued work with the Center for Human-Compatible AI (CHAI) at UC Berkeley. BERI will use the funding to facilitate the creation of an in-house compute cluster for CHAI’s use, purchase compute resources, and hire a part-time system administrator to help manage the cluster."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Berkeley Existential Risk Initiative',1008127,'2022-04-01','month','donation log','AI safety/technical research/talent pipeline','https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-seri-mats-program/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','SERI-MATS program',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to the Berkeley Existential Risk Initiative to support its collaboration with the Stanford Existential Risks Initiative (SERI) on the second cohort of the SERI Machine Learning Alignment Theory Scholars (MATS) Program. MATS is an educational seminar and independent research program that aims to provide talented scholars with talks, workshops, and research mentorship in the field of AI alignment, and connect them with the Berkeley alignment research community."',
  /* intended_funding_timeframe_in_months */ 6, /* funding is for one cohort; the whole process from applications to training/research/extension for the cohort is about 6 months; more helpfully, there are two cohorts per year, so the amortized duration of a cohort is 6 months */
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ 'The grant is made in time for the second cohort of the SERI-MATS program; this is the cohort being funded by the grant.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-machine-learning-alignment-theory-scholars/ for the third cohort of the SERI-MATS program suggests the donor''s continued satisfaction with the SERI-MATS program. Also, the grant https://www.openphilanthropy.org/grants/conjecture-seri-mats-program-in-london/ for the London-based extension of this cohort (the second cohort) also suggests the donor''s satisfaction with the program.',
  /* notes */ 'See https://www.serimats.org/program for details of the program including its timeline. Although the research phase of the timeline is just two months, the application process, training phase, and extension phase together make up about half a year.'),

  ('Open Philanthropy','Berkeley Existential Risk Initiative',210000,'2022-04-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-ai-standards-2022/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Center for Long-Term Cybersecurity',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support work on the development and implementation of AI safety standards that may reduce potential risks from advanced artificial intelligence."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ 'The grant is made at the same time as the companion grant https://www.openphilanthropy.org/grants/center-for-long-term-cybersecurity-ai-standards-2022/ to the Center for Long-Term Cybersecurity (CLTC), via the University of California, Berkeley.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'There is a companion grant https://www.openphilanthropy.org/grants/center-for-long-term-cybersecurity-ai-standards-2022/ to the Center for Long-Term Cybersecurity (CLTC), via the University of California, Berkeley.'),

  ('Open Philanthropy','Berkeley Existential Risk Initiative',140050,'2022-04-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-david-krueger-collaboration/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','David Krueger',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to the Berkeley Existential Risk Initiative to support its collaboration with Professor David Krueger."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'The grant page says: "The grant amount was updated in August 2023."'),

  ('Open Philanthropy','Berkeley Existential Risk Initiative',30000,'2022-06-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-language-model-alignment-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Samuel Bowman',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support a project led by Professor Samuel Bowman of New York University to develop a dataset and accompanying methods for language model alignment research."',
  /* intended_funding_timeframe_in_months */ 36,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Berkeley Existential Risk Initiative',100000,'2022-11-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-general-support-2/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'Grant "for general support. BERI seeks to reduce existential risks to humanity by providing services and support to university-based research groups, including the Center for Human-Compatible AI at the University of California, Berkeley."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Berkeley Existential Risk Initiative',2047268,'2022-11-01','month','donation log','AI safety/technical research/talent pipeline','https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-machine-learning-alignment-theory-scholars/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','SERI-MATS program',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support their collaboration with the Stanford Existential Risks Initiative (SERI) on SERI’s Machine Learning Alignment Theory Scholars (MATS) program. MATS is an educational seminar and independent research program that aims to provide talented scholars with talks, workshops, and research mentorship in the field of AI alignment, and connect them with the Berkeley alignment research community. This grant will support the MATS program’s third cohort."',
  /* intended_funding_timeframe_in_months */ 6, /* funding is for one cohort; the whole process from applications to training/research/extension for the cohort is about 6 months; more helpfully, there are two cohorts per year, so the amortized duration of a cohort is 6 months */
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ 'The grant is made in time for the third cohort of the SERI-MATS program; this is the cohort being funded by the grant.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/grants/conjecture-seri-mats-2023/ for the London-based extension suggests continued satisfaction with this funded program.',
  /* notes */ 'See https://www.serimats.org/program for details of the program including its timeline. Although the research phase of the timeline is just two months, the application process, training phase, and extension phase together make up about half a year. See also the companion grants: https://www.openphilanthropy.org/grants/ai-safety-support-seri-mats-program/ to AI Safety Support and https://www.openphilanthropy.org/grants/conjecture-seri-mats-2023/ to Conjecture for the London-based extension.'),

  ('Open Philanthropy','Berkeley Existential Risk Initiative',35000,'2023-07-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-lab-retreat/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Anca Dragan',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support a retreat for Anca Dragan’s BAIR lab group, where members will discuss potential risks from advanced artificial intelligence."',
  /* intended_funding_timeframe_in_months */ 1,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Berkeley Existential Risk Initiative',70000,'2023-09-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-scalable-oversight-dataset/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant " to support the creation of a scalable oversight dataset. The purpose of the dataset is to collect questions that non-experts can’t answer even with the internet at their disposal; these kinds of questions can be used to test how well AI systems can lead humans to the right answers without misleading them."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Berkeley Existential Risk Initiative',70000,'2023-10-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-university-collaboration-program/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support [BERI''s] university collaboration program. Selected applicants become eligible for support and services from BERI that would be difficult or impossible to obtain through normal university channels. BERI will use these funds to increase the size of its 2024 cohort." The page https://existence.org/2023/07/27/trial-collaborations-2023.html is linked.',
  /* intended_funding_timeframe_in_months */ 12, /* using the fact that this grant money is funding one year''s cohort */
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL);

/* Grants to AI Impacts */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision,donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','AI Impacts',32000,'2016-12-01','month','donation log','AI safety/strategy','https://www.openphilanthropy.org/grants/ai-impacts-general-support-2016/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,'2017-02-02','day','https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/FU2Is9LMWJg',NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'The grant page says: "AI Impacts plans to use this grant to work on strategic questions related to potential risks from advanced artificial intelligence."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'Renewals in 2018 https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ai-impacts-general-support-2018 and 2020 https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ai-impacts-general-support-2020 suggest continued satisfaction with the grantee.',
  /* notes */ NULL),

  ('Open Philanthropy','AI Impacts',100000,'2018-06-01','month','donation log','AI safety/strategy','https://www.openphilanthropy.org/grants/ai-impacts-general-support-2018/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Daniel Dewey','2018-06-27','day',NULL,NULL, /* date adjusted */
  /* donation_process */ 'Discretionary grant',
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'The grant page says: "AI Impacts plans to use this grant to work on strategic questions related to potential risks from advanced artificial intelligence."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'Renewal in 2020 https://www.openphilanthropy.org/grants/ai-impacts-general-support-2020/ and 2022 https://www.openphilanthropy.org/grants/ai-impacts-general-support/ suggest continued satisfaction with the grantee, though the amount of the 2020 renewal grant is lower (just $50,000).',
  /* notes */ 'The grant is via the Machine Intelligence Research Institute.'),

  ('Open Philanthropy','AI Impacts',50000,'2020-11-01','month','donation log','AI safety/strategy','https://www.openphilanthropy.org/grants/ai-impacts-general-support-2020/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Tom Davidson|Ajeya Cotra',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'The grant page says: "AI Impacts plans to use this grant to work on strategic questions related to potential risks from advanced artificial intelligence."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'Renewal in 2022 https://www.openphilanthropy.org/grants/ai-impacts-general-support/ (for a much larger amount) suggests continued satisfaction with the grantee.',
  /* notes */ NULL),

  ('Open Philanthropy','AI Impacts',364893,'2022-06-01','month','donation log','AI safety/strategy','https://www.openphilanthropy.org/grants/ai-impacts-general-support/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'Grant "for general support. AI Impacts works on strategic questions related to advanced artificial intelligence."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/grants/ai-impacts-expert-survey-on-progress-in-ai/ suggests continued satisfaction with the grantee.',
  /* notes */ NULL),

  ('Open Philanthropy','AI Impacts',150000,'2023-08-01','month','donation log','AI safety/strategy','https://www.openphilanthropy.org/grants/ai-impacts-expert-survey-on-progress-in-ai/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support an expert survey on progress in artificial intelligence. AI Impacts works to answer questions about the future of artificial intelligence."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'AI Impacts previously did expert surveys on the state of AI, including https://aiimpacts.org/2016-expert-survey-on-progress-in-ai/ in 2016 and (a rerun) https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/ in 2022. This survey is likely a followup/rerun of those surveys.');

/* Grants to the University of California, Berkeley (UC Berkeley) (note that grants funding the Center for Human-Compatible AI (CHAI) is entered as a grant to CHAI rather than to UC Berkeley) */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision,donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','University of California, Berkeley',1450016,'2017-10-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-ai-safety-levine-dragan','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Sergey Levine|Anca Dragan','Daniel Dewey','2017-10-20','day','https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/jW3KjxVUOOA',NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'The grant page says: "The work will be led by Professors Sergey Levine and Anca Dragan, who will each devote approximately 20% of their time to the project, with additional assistance from four graduate students. They initially intend to focus their research on how objective misspecification can produce subtle or overt undesirable behavior in robotic systems, though they have the flexibility to adjust their focus during the grant period." The project narrative is at https://www.openphilanthropy.org/files/Grants/UC_Berkeley/Levine_Dragan_Project_Narrative_2017.pdf',
  /* intended_funding_timeframe_in_months */ 48,
  /* donor_donee_reason */ 'The grant page says: "Our broad goals for this funding are to encourage top researchers to work on AI alignment and safety issues in order to build a pipeline for young researchers; to support progress on technical problems; and to generally support the growth of this area of study."',
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'This is the first year that Open Phil makes a grant for AI safety research to the University of California, Berkeley (excluding the founding grant for the Center for Human-Compatible AI). It would begin an annual tradition of multi-year grants to the University of California, Berkeley announced in October/November, though the researchers would be different each year. Note that the grant is to UC Berkeley, but at least one of the researchers (Anca Dragan) is affiliated with the Center for Human-Compatible AI.'),

  ('Open Philanthropy','University of California, Berkeley',1145000,'2018-11-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/university-of-california-berkeley-artificial-intelligence-safety-research-2018','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Pieter Abeel|Aviv Tamar','Daniel Dewey','2018-12-11','day',NULL,NULL, /* date adjusted */
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "for machine learning researchers Pieter Abbeel and Aviv Tamar to study uses of generative models for robustness and interpretability. This funding will allow Mr. Abbeel and Mr. Tamar to fund PhD students and summer undergraduates to work on classifiers, imitation learning systems, and reinforcement learning systems."',
  /* intended_funding_timeframe_in_months */ 36,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'This is the second year that Open Phil makes a grant for AI safety research to the University of California, Berkeley (excluding the founding grant for the Center for Human-Compatible AI). It continues an annual tradition of multi-year grants to the University of California, Berkeley announced in October/November, though the researchers would be different each year. Note that the grant is to UC Berkeley, but at least one of the researchers (Pieter Abbeel) is affiliated with the Center for Human-Compatible AI.'),

  ('Open Philanthropy','University of California, Berkeley',1111000,'2019-11-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-ai-safety-research-2019','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Jacob Steinhardt','Daniel Dewey','2020-02-19','day',NULL,NULL, /* date adjusted */
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'The grant page says: "This funding will allow Professor Steinhardt to fund students to work on robustness, value learning, aggregating preferences, and other areas of machine learning."',
  /* intended_funding_timeframe_in_months */ 36,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'This is the third year that Open Phil makes a grant for AI safety research to the University of California, Berkeley (excluding the founding grant for the Center for Human-Compatible AI). It continues an annual tradition of multi-year grants to the University of California, Berkeley announced in October/November, though the researchers would be different each year. Note that the grant is to UC Berkeley, but at least one of the researchers (Jacob Steinhardt) is affiliated with the Center for Human-Compatible AI.'),

  ('Open Philanthropy','University of California, Berkeley',330000,'2021-02-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-adversarial-robustness-song','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Dawn Song','Catherine Olsson|Daniel Dewey',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support research by Professor Dawn Song on adversarial robustness as a means to improve AI safety."',
  /* intended_funding_timeframe_in_months */ 36,
  /* donor_donee_reason */ 'This is one of five grants made by the donor for "adversarial robustness research" in January and February 2021, all with the same grant investigators (Catherine Olsson and Daniel Dewey) except the Santa Cruz grant that had Olsson and Nick Beckstead. https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-santa-cruz-xie-adversarial-robustness https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/mit-adversarial-robustness-research https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/university-of-tuebingen-adversarial-robustness-hein and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-adversarial-robustness-wagner are the foour other grants.It looks like the donor became interested in funding this research topic at this time.',
  /* donor_amount_reason */ 'No explicit reasons for the amount are given, but the amount is similar to the amounts for other grants from Open Philanthropy to early-stage researchers in adversarial robustness research. This includes three other grants https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-santa-cruz-xie-adversarial-robustness https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-adversarial-robustness-wagner and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/university-of-tuebingen-adversarial-robustness-hein made at the same time as well as grants later in the year to early-stage researchers at Carnegie Mellon University, Stanford University, and University of Southern California.',
  /* donor_timing_reason */ 'This is one of five grants made by the donor for "adversarial robustness research" in Januaay and February 2021, all with the same grant investigators (Catherine Olsson and Daniel Dewey) except the Santa Cruz grant that had Olsson and Nick Beckstead. https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-santa-cruz-xie-adversarial-robustness https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/mit-adversarial-robustness-research https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/university-of-tuebingen-adversarial-robustness-hein and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-adversarial-robustness-wagner are the four other grants. It looks like the donor became interested in funding this research topic at this time.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','University of California, Berkeley',330000,'2021-02-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-adversarial-robustness-wagner','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','David Wagner','Catherine Olsson|Daniel Dewey',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support research by Professor David Wagner on adversarial robustness as a means to improve AI safety."',
  /* intended_funding_timeframe_in_months */ 36,
  /* donor_donee_reason */ 'This is one of five grants made by the donor for "adversarial robustness research" in January and February 2021, all with the same grant investigators (Catherine Olsson and Daniel Dewey) except the Santa Cruz grant that had Olsson and Nick Beckstead. https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-santa-cruz-xie-adversarial-robustness https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/mit-adversarial-robustness-research https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/university-of-tuebingen-adversarial-robustness-hein and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-adversarial-robustness-song are the four other grants. It looks like the donor became interested in funding this research topic at this time.',
  /* donor_amount_reason */ 'No explicit reasons for the amount are given, but the amount is similar to the amounts for other grants from Open Philanthropy to early-stage researchers in adversarial robustness research. This includes three other grants https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-santa-cruz-xie-adversarial-robustness https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/university-of-tuebingen-adversarial-robustness-hein and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-adversarial-robustness-song made at the same time as well as grants later in the year to early-stage researchers at Carnegie Mellon University, Stanford University, and University of Southern California.',
  /* donor_timing_reason */ 'This is one of five grants made by the donor for "adversarial robustness research" in January and February 2021, all with the same grant investigators (Catherine Olsson and Daniel Dewey) except the Santa Cruz grant that had Olsson and Nick Beckstead. https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-santa-cruz-xie-adversarial-robustness https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/mit-adversarial-robustness-research https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/university-of-tuebingen-adversarial-robustness-hein and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-adversarial-robustness-song are the four other grants. It looks like the donor became interested in funding this research topic at this time.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','University of California, Berkeley',87829,'2021-08-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/uc-berkeley-adversarial-robustness-research-aditi-raghunathan/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Aditi Raghunathan',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to "support postdoctoral research by Aditi Raghunathan on adversarial robustness as a means to improve AI safety."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/grants/carnegie-mellon-university-research-on-adversarial-examples/ for the continuation of the grantee''s work at Carnegie Mellon University suggests satisfaction with the grant outcome.',
  /* notes */ 'The grant page says: "The grant amount was updated in July 2023."');

/* Grants to Center for Security and Emerging Technology (CSET); note that the founding grant
   is in open-phil-other-gcr-and-security-grants.sql */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision,donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','Center for Security and Emerging Technology',8000000,'2021-01-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/center-security-and-emerging-technology-general-support','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Luke Muehlhauser',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'The grant page says "This funding is intended to augment our original support for CSET, particularly for its work on the intersection of security and artificial intelligence."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/center-security-and-emerging-technology-general-support-august-2021 for a much larger amount suggests continued satisfaction with the grantee.',
  /* notes */ NULL),

  ('Open Philanthropy','Center for Security and Emerging Technology',38920000,'2021-08-01','month','donation log','AI safety','https://www.openphilanthropy.org/grants/center-for-security-and-emerging-technology-general-support-august-2021/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Luke Muehlhauser',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'The grant page says: "CSET is a think tank, incubated by our January 2019 support, dedicated to policy analysis at the intersection of national and international security and emerging technologies. This funding is intended to augment our original support for CSET, particularly for its work on security and artificial intelligence."',
  /* intended_funding_timeframe_in_months */ 36,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL);

/* Grants to OpenAI */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision,donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','OpenAI',30000000,'2017-03-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/openai-general-support','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/', NULL, NULL, '2017-03-31', 'day', 'https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/PqgSlJy0JT4',NULL,
  /* donation_process */ 'According to the grant page Section 4 Our process: "OpenAI initially approached Open Philanthropy about potential funding for safety research, and we responded with the proposal for this grant. Subsequent discussions included visits to OpenAI’s office, conversations with OpenAI’s leadership, and discussions with a number of other organizations (including safety-focused organizations and AI labs), as well as with our technical advisors."',
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'The funds will be used for general support of OpenAI, with 10 million USD per year for the next three years. The funding is also accompanied with Holden Karnofsky (Open Phil director) joining the OpenAI Board of Directors. Karnofsky and one other board member will oversee OpenAI''s safety and governance work.',
  /* intended_funding_timeframe_in_months */ 36,
  /* donor_donee_reason */ 'Open Phil says that, given its interest in AI safety, it is looking to fund and closely partner with orgs that (a) are working to build transformative AI, (b) are advancing the state of the art in AI research, (c) employ top AI research talent. OpenAI and Deepmind are two such orgs, and OpenAI is particularly appealing due to "our shared values, different starting assumptions and biases, and potential for productive communication." Open Phil is looking to gain the following from a partnership: (i) Improve its understanding of AI research, (ii) Improve its ability to generically achieve goals regarding technical AI safety research, (iii) Better position Open Phil to promote its ideas and goals.',
  /* donor_amount_reason */ 'The grant page Section 2.2 "A note on why this grant is larger than others we’ve recommended in this focus area" explains the reasons for the large grant amount (relative to other grants by Open Phil so far). Reasons listed are: (i) Hits-based giving philosophy, described at https://www.openphilanthropy.org/blog/hits-based-giving in depth, (ii) Disproportionately high importance of the cause if transformative AI is developed in the next 20 years, and likelihood that OpenAI will be very important if that happens, (iii)  Benefits of working closely with OpenAI in informing Open Phil''s understanding of AI safety, (iv) Field-building benefits, including promoting an AI safety culture, (v) Since OpenAI has a lot of other funding, Open Phil can grant a large amount while still not raising the concern of dominating OpenAI''s funding.',
  /* donor_timing_reason */ 'No specific timing considerations are provided. It is likely that the timing of the grant is determined by when OpenAI first approached Open Phil and the time taken for the due diligence.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'External discussions include http://benjaminrosshoffman.com/an-openai-board-seat-is-surprisingly-expensive/ cross-posted to https://www.lesswrong.com/posts/2z5vrsu7BoiWckLby/an-openai-board-seat-is-surprisingly-expensive (post by Ben Hoffman, attracting comments at both places), https://twitter.com/Pinboard/status/848009582492360704 (critical tweet with replies), https://www.facebook.com/vipulnaik.r/posts/10211478311489366 (Facebook post by Vipul Naik, with some comments), https://www.facebook.com/groups/effective.altruists/permalink/1350683924987961/ (Facebook post by Alasdair Pearce in Effective Altruists Facebook group, with some comments), and https://news.ycombinator.com/item?id=14008569 (Hacker News post, with some comments).');

/* Grants to the Center for Human-Compatible AI (CHAI) */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision,donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','Center for Human-Compatible AI',5555550,'2016-08-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-center-human-compatible-ai','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL, NULL, '2016-08-29', 'day', 'https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/3ouYTW5lXBY','https://predictionbook.com/predictions/185224',
  /* donation_process */ 'The grant page section https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-center-human-compatible-ai#Our_process says: "We have discussed the possibility of a grant to support Professor Russell’s work several times with him in the past. Following our decision earlier this year to make this focus area a major priority for 2016, we began to discuss supporting a new academic center at UC Berkeley in more concrete terms."',
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-center-human-compatible-ai#Budget_and_room_for_more_funding says: "Professor Russell estimates that the Center could, if funded fully, spend between $1.5 million and $2 million in its first year and later increase its budget to roughly $7 million per year." The funding from Open Phil will be used toward this budget. An earlier section of the grant page says that the Center''s research topics will include value alignment, value functions defined by partially observable and partially defined terms, the structure of human value systems, and conceptual questions including the properties of ideal value systems.',
  /* intended_funding_timeframe_in_months */ 24,
  /* donor_donee_reason */ 'The grant page gives these reasons: (1) "We expect the existence of the Center to make it much easier for researchers interested in exploring AI safety to discuss and learn about the topic, and potentially consider focusing their careers on it." (2) "The Center may allow researchers already focused on AI safety to dedicate more of their time to the topic and produce higher-quality research." (3) "We hope that the existence of a well-funded academic center at a major university will solidify the place of this work as part of the larger fields of machine learning and artificial intelligence." Also, counterfactual impact: "Professor Russell would not plan to announce a new Center of this kind without substantial additional funding. [...] We are not aware of other potential [substantial] funders, and we believe that having long-term support in place is likely to make it easier for Professor Russell to recruit for the Center."',
  /* donor_amount_reason */ 'The amount is based on budget estimates in https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-center-human-compatible-ai#Budget_and_room_for_more_funding "Professor Russell estimates that the Center could, if funded fully, spend between $1.5 million and $2 million in its first year and later increase its budget to roughly $7 million per year."',
  /* donor_timing_reason */ 'Timing seems to have been determined by the time it took to work out the details of the new center after Open Phil decided to make AI safety a major priority in 2016. According to https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-center-human-compatible-ai#Our_process "We have discussed the possibility of a grant to support Professor Russell’s work several times with him in the past. Following our decision earlier this year to make this focus area a major priority for 2016, we began to discuss supporting a new academic center at UC Berkeley in more concrete terms."',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-center-human-compatible-ai-2019 in November 2019, five-year renewal https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-center-human-compatible-ai-2021 in January 2021, as well as many grants to Berkeley Existential Risk Initiative (BERI) to collaborate with the grantee, suggest that Open Phil would continue to think highly of the grantee, and stand by its reasoning.',
  /* notes */ 'Note that the grant recipient in the Open Phil database has been listed as UC Berkeley, but we have written it as the name of the center for easier cross-referencing.'),

  ('Open Philanthropy','Center for Human-Compatible AI',200000,'2019-11-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-center-human-compatible-ai-2019','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Daniel Dewey','2019-12-20','day',NULL,NULL, /* date adjusted */
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'The grant page says "CHAI plans to use these funds to support graduate student and postdoc research."',
  /* intended_funding_timeframe_in_months */ 24,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'Open Phil makes a $705,000 grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/berkeley-existential-risk-initiative-chai-collaboration-2019 to the Berkeley Existential Risk Initiative (BERI) at the same time (November 2019) to collaborate with CHAI.'),

  ('Open Philanthropy','Center for Human-Compatible AI',11355246,'2021-01-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-center-human-compatible-ai-2021','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Nick Beckstead',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'The grant page says "The multi-year commitment and increased funding will enable CHAI to expand its research and student training related to potential risks from advanced artificial intelligence."',
  /* intended_funding_timeframe_in_months */ 60,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'This is a renewal of the original founding grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-center-human-compatible-ai made August 2016.');

/* Grants to Open Phil AI Fellowship */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision,donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','Open Phil AI Fellowship',1135000,'2018-05-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ai-fellows-program-2018','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Aditi Raghunathan|Chris Maddison|Felix Berkenkamp|Jon Gauthier|Michael Janner|Noam Brown|Ruth Fong','Daniel Dewey','2018-05-31','day',NULL,NULL, /* date adjusted */
  /* donation_process */ 'According to the grant page: "These fellows were selected from more than 180 applicants for their academic excellence, technical knowledge, careful reasoning, and interest in making the long-term, large-scale impacts of AI a central focus of their research"',
  /* intended_use_of_funds_category */ 'Living expenses during project',
  /* intended_use_of_funds */ 'Grant to provide scholarship support to seven machine learning researchers over five years',
  /* intended_funding_timeframe_in_months */ 60,
  /* donor_donee_reason */ 'According to the grant page: "The intent of the Open Phil AI Fellowship is both to support a small group of promising researchers and to foster a community with a culture of trust, debate, excitement, and intellectual excellence. We plan to host gatherings once or twice per year where fellows can get to know one another, learn about each other’s work, and connect with other researchers who share their interests."',
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ 'This is the first of annual sets of grants, decided through an annual application process.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The corresponding grants https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/open-phil-ai-fellowship-2019-class (2019), https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/open-phil-ai-fellowship-2020-class (2020), and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/open-phil-ai-fellowship-2021-class (2021) confirm that these grants will be made annually. Among the grantees, Chris Maddison would continue receiving support from Open Philanthropy in the future in the form of support https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/university-of-toronto-machine-learning-research for his students, indicating continued endorsement of his work.',
  /* notes */ NULL),

  ('Open Philanthropy','Open Phil AI Fellowship',2325000,'2019-05-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/open-phil-ai-fellowship-2019-class','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Aidan Gomez|Andrew Ilyas|Julius Adebayo|Lydia T. Liu|Max Simchowitz|Pratyusha Kullari|Siddharth Karamcheti|Smitha Milli','Daniel Dewey','2019-05-17','day',NULL,NULL, /* date adjusted */
  /* donation_process */ 'According to the grant page: "These fellows were selected from more than 175 applicants for their academic excellence, technical knowledge, careful reasoning, and interest in making the long-term, large-scale impacts of AI a central focus of their research."',
  /* intended_use_of_funds_category */ 'Living expenses during project',
  /* intended_use_of_funds */ 'Grant to provide scholarship support to eight machine learning researchers over five years',
  /* intended_funding_timeframe_in_months */ 60,
  /* donor_donee_reason */ 'According to the grant page: "The intent of the Open Phil AI Fellowship is both to support a small group of promising researchers and to foster a community with a culture of trust, debate, excitement, and intellectual excellence. We plan to host gatherings once or twice per year where fellows can get to know one another, learn about each other’s work, and connect with other researchers who share their interests."',
  /* donor_amount_reason */ 'The amount is about double the amount of the 2018 grant, although the number of people supported is just one more (8 instead of 7). No explicit comparison of grant amounts is done in the grant page.',
  /* donor_timing_reason */ 'This is the second of annual sets of grants, decided through an annual application process, with the announcement made in May/June each year. The timing may have been chosen to sync with the academic year.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grants https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/open-phil-ai-fellowship-2020-class (2020) and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/open-phil-ai-fellowship-2021-class (2021) confirm that the program would continue. Among the grantees, Smitha Milli would receive further support https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/smitha-milli-participatory-approaches-machine-learning-workshop from Open Philanthropy, indicating continued confidence in the support.',
  /* notes */ NULL),

  ('Open Philanthropy','Open Phil AI Fellowship',2300000,'2020-05-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/open-phil-ai-fellowship-2020-class','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Alex Tamkin|Clare Lyle|Cody Coleman|Dami Choi|Dan Hendrycks|Ethan Perez|Frances Ding|Leqi Liu|Peter Henderson|Stanislav Fort','Catherine Olsson|Daniel Dewey','2020-05-12','day',NULL,NULL, /* date adjusted */
  /* donation_process */ 'According to the grant page: "These fellows were selected from more than 380 applicants for their academic excellence, technical knowledge, careful reasoning, and interest in making the long-term, large-scale impacts of AI a central focus of their research."',
  /* intended_use_of_funds_category */ 'Living expenses during project',
  /* intended_use_of_funds */ 'Grant to provide scholarship to ten machine learning researchers over five years',
  /* intended_funding_timeframe_in_months */ 60,
  /* donor_donee_reason */ 'According to the grant page: "The intent of the Open Phil AI Fellowship is both to support a small group of promising researchers and to foster a community with a culture of trust, debate, excitement, and intellectual excellence. We plan to host gatherings once or twice per year where fellows can get to know one another, learn about each other’s work, and connect with other researchers who share their interests." In a comment reply https://forum.effectivealtruism.org/posts/DXqxeg3zj6NefR9ZQ/open-philanthropy-our-progress-in-2019-and-plans-for-2020#BCvuhRCg9egAscpyu on the Effectiive Altruism Forum, grant investigator Catherine Olsson writes: "But the short answer is I think the key pieces to keep in mind are to view the fellowship as 1) a community, not just individual scholarships handed out, and as such also 2) a multi-year project, built slowly."',
  /* donor_amount_reason */ 'The amount is comparable to the total amount of the 2019 fellowship grants, though it is distributed among a slightly larger pool of people.',
  /* donor_timing_reason */ 'This is the third of annual sets of grants, decided through an annual application process, with the announcement made between April and June each year. The timing may have been chosen to sync with the academic year.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/open-phil-ai-fellowship-2021-class (2021) confirms that the program would continue.',
  /* notes */ NULL),

  ('Open Philanthropy','Open Phil AI Fellowship',1300000,'2021-04-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/open-phil-ai-fellowship-2021-class','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Collin Burns|Jared Quincy Davis|Jesse Mu|Meena Jagadeesan|Tan Zhi-Xuan','Daniel Dewey',NULL,NULL,NULL,NULL,
  /* donation_process */ 'According to the grant page: "These [five] fellows were selected from 397 applicants for their academic excellence, technical knowledge, careful reasoning, and interest in making the long-term, large-scale impacts of AI a central focus of their research."',
  /* intended_use_of_funds_category */ 'Living expenses during project',
  /* intended_use_of_funds */ 'Grant to provide scholarship to five machine learning researchers over five years.',
  /* intended_funding_timeframe_in_months */ 60,
  /* donor_donee_reason */ 'According to the grant page: "The intent of the Open Phil AI Fellowship is both to support a small group of promising researchers and to foster a community with a culture of trust, debate, excitement, and intellectual excellence. We plan to host gatherings once or twice per year where fellows can get to know one another, learn about each other’s work, and connect with other researchers who share their interests."',
  /* donor_amount_reason */ 'An explicit reason for the amount is not specified, and the total amount is lower than previous years, but the amount per researcher ($260,000) is a little higher than previous years. It''s likely that the amount per researcher is determined first and the total amount is the sum of these.',
  /* donor_timing_reason */ 'This is the fourth of annual sets of grants, decided through an annual application process, with the announcement made between April and June each year. The timing may have been chosen to sync with the academic year.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/grants/open-phil-ai-fellowship-2022-class/ confirms that the program would continue.',
  /* notes */ 'The initial grant page only listed four of the five fellows and an amount of $1,000,000. The fifth fellow, Tan Zhi-Xuan, was added later and the amount was increased to $1,300,000.'),

  ('Open Philanthropy','Open Phil AI Fellowship',1840000,'2022-04-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/open-phil-ai-fellowship-2022-class/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Adam Gleave|Cassidy Laidlaw|Cynthia Chen|Daniel Kunin|Erik Jenner|Johannes Treutlein|Lauro Langosco|Maksym Andriushchenko|Qian Huang|Usman Anwar|Zhijing Jin',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ 'The Open Phil AI Fellowship is awarded annually based on an application process. https://www.openphilanthropy.org/potential-risks-advanced-artificial-intelligence-the-open-phil-ai-fellowship/ has more details on the application process.',
  /* intended_use_of_funds_category */ 'Living expenses during project',
  /* intended_use_of_funds */ 'Grant to provide scholarship to eleven machine learning researchers over five years.',
  /* intended_funding_timeframe_in_months */ 60,
  /* donor_donee_reason */ 'According to the grant page: "These [eleven] fellows were selected for their academic excellence, technical knowledge, careful reasoning, and interest in making the long-term, large-scale impacts of AI a central focus of their research. [...] We believe that progress in artificial intelligence may eventually lead to changes in human civilization that are as large as the agricultural or industrial revolutions; while we think it’s most likely that this would lead to significant improvements in human well-being, we also see significant risks. Open Phil AI Fellows have a broad mandate to think through which kinds of research are likely to be most valuable, to share ideas and form a community with like-minded students and professors, and ultimately to act in the way that they think is most likely to improve outcomes from progress in AI. The intent of the Open Phil AI Fellowship is both to support a small group of promising researchers and to foster a community with a culture of trust, debate, excitement, and intellectual excellence."',
  /* donor_amount_reason */ 'Although the amount per researcher is lower than in previous years (at $1,840,000 over 11 years, it averages to around $170,000 per researcher, less than the $260,000 in the previous year), this reduced amount is partly explained by some of the grantees also receiving funding as Vitalik Buterin Postdoctoral Fellows (see https://futureoflife.org/team/fellowship-winners-2022/ for details); for these grantees, Open Phil and Future of Life Institute split the money equally. Also, regarding the amount, the grant page says: "This is an estimate because of uncertainty around future year tuition costs and currency exchange rates. This number may be updated as costs are finalized."',
  /* donor_timing_reason */ 'This is the fourth of annual sets of grants, decided through an annual application process, with the announcement made between April and June each year. The timing may have been chosen to sync with the academic year.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'Five of the eleven grantees (Cynthia Chen, Erik Jenner, Johannes Treutlein, Usman Anwar, and Zhijing Jin) also receiving funding as Vitalik Buterin Postdoctoral Fellows (see https://futureoflife.org/team/fellowship-winners-2022/ for details); for these grantees, Open Phil and Future of Life Institute split the money equally.');

/* Grants to Ought */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, influencer, donation_announcement_date, donation_announcement_date_precision,donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','Ought',525000,'2018-05-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ought-general-support','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Daniel Dewey','2018-05-30','day',NULL,NULL, /* date adjusted */
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'The grant page says at https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ought-general-support#Proposed_activities "Ought will conduct research on deliberation and amplification, aiming to organize the cognitive work of ML algorithms and humans so that the combined system remains aligned with human interests even as algorithms take on a much more significant role than they do today." It also links to https://ought.org/approach Also, https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ought-general-support#Budget says: "Ought intends to use it for hiring and supporting up to four additional employees between now and 2020. The hires will likely include a web developer, a research engineer, an operations manager, and another researcher."',
  /* intended_funding_timeframe_in_months */ 36,
  /* donor_donee_reason */ 'The case for the grant includes: (a) Open Phil considers research on deliberation and amplification important for AI safety, (b) Paul Christiano is excited by Ought''s approach, and Open Phil trusts his judgment, (c) Ought’s plan appears flexible and we think Andreas is ready to notice and respond to any problems by adjusting his plans, (d) Open Phil has indications that Ought is well-run and has a reasonable chance of success.',
  /* donor_amount_reason */ 'No explicit reason for the amount is given, but the grant is combined with another grant from Open Philanthropy Project technical advisor Paul Christiano',
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ 'https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ought-general-support#Key_questions_for_follow-up lists some questions for followup',
  /* donor_retrospective */ 'The followup grants https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ought-general-support-2019 and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ought-general-support-2020 suggest that Open Phil would continue to have a high opinion of Ought',
  /* notes */ NULL),

  ('Open Philanthropy','Ought',1000000,'2019-11-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ought-general-support-2019','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Daniel Dewey','2020-02-14','day',NULL,NULL, /* date adjusted */
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'The grant page says: "Ought conducts research on factored condition, which we consider relevant to AI alignment."',
  /* intended_funding_timeframe_in_months */ 24,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ought-general-support-2020 made on the recommendation of the Committee for Effective Altruism Support suggest that Open Phil would continue to have a high opinion of the work of Ought',
  /* notes */ NULL),

  ('Open Philanthropy','Ought',1593333,'2020-01-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ought-general-support-2020','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Committee for Effective Altruism Support','2020-02-14','day',NULL,NULL, /* date adjusted */
  /* donation_process */ 'The grant was recommended by the Committee for Effective Altruism Support following its process https://www.openphilanthropy.org/committee-effective-altruism-support',
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'The grant page says: "Ought conducts research on factored cognition, which we consider relevant to AI alignment and to reducing potential risks from advanced artificial intelligence."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'The grant page says "we see the basic pros and cons of this support similarly to what we’ve presented in past writeups on the matter"',
  /* donor_amount_reason */ 'The amount is decided by the Committee for Effective Altruism Support https://www.openphilanthropy.org/committee-effective-altruism-support but individual votes and reasoning are not public. Three other grants decided by CEAS at around the same time are: Machine Intelligence Research Institute ($7,703,750), Centre for Effective Altruism ($4,146,795), and 80,000 Hours ($3,457,284).',
  /* donor_timing_reason */ 'Reasons for timing are not discussed, but this is likely the time when the Committee for Effective Altruism Support does its 2020 allocation',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL);

/* Grants to The Wilson Center */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision,donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','The Wilson Center',400000,'2018-07-01','month','donation log','AI safety/governance','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/wilson-center-ai-policy-seminar-series','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Luke Muehlhauser','2018-08-01','day',NULL,NULL, /* date adjusted */
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support a series of in-depth AI policy seminars."',
  /* intended_funding_timeframe_in_months */ 24,
  /* donor_donee_reason */ 'The grant page says: "We believe the seminar series can help inform AI policy discussions and decision-making in Washington, D.C., and could help identify and empower influential experts in those discussions, a key component of our AI policy grantmaking strategy."',
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grants https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/wilson-center-ai-policy-seminar-series-february-2020 and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/wilson-center-ai-policy-seminar-series-june-2020 suggest that the donor was satisfied with the outcome of the grant.',
  /* notes */ NULL),

  ('Open Philanthropy','The Wilson Center',368440,'2020-02-01','month','donation log','AI safety/governance','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/wilson-center-ai-policy-seminar-series-february-2020','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Luke Muehlhauser',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to continue support for a series of in-depth AI policy seminars."',
  /* intended_funding_timeframe_in_months */ 24,
  /* donor_donee_reason */ 'The grat page says: "We continue to believe the seminar series can help inform AI policy discussions and decision-making in Washington, D.C., and could help identify and empower influential experts in those discussions, a key component of our AI policy grantmaking strategy."',
  /* donor_amount_reason */ 'The amount is similar to the previous grant of $400,000 over a similar time period (two years).',
  /* donor_timing_reason */ 'The grant is made almost two years after the original two-year grant, so its timing is likely determined by the original grant running out.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/wilson-center-ai-policy-seminar-series-june-2020 suggests ongoing satisfaction with the grant outcomes. A later grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/wilson-center-ai-policy-training-program in the same general area suggests Open Philanthropy''s continued satisfaction with the grantee.',
  /* notes */ NULL),

  ('Open Philanthropy','The Wilson Center',496540,'2020-06-01','month','donation log','AI safety/governance','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/wilson-center-ai-policy-seminar-series-june-2020','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Luke Muehlhauser',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to organize additional in-depth AI policy seminars as part of its seminar series."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'The grant page says "We continue to believe the seminar series can help inform AI policy discussions and decision-making in Washington, D.C., and could help identify and empower influential experts in those discussions, a key component of our AI policy grantmaking strategy."',
  /* donor_amount_reason */ 'No reason is given for the amount. The grant is a little more than the original $368,440 two-year grant so it is likely that the additional amount is expected to double the frequency of  AI policy seminars.',
  /* donor_timing_reason */ 'The grant is a top-up rather than a renewal; the previous two-year grant was made in February 2020. No specific reasons for timing are given.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'A later grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/wilson-center-ai-policy-training-program in the same general area suggests Open Philanthropy''s continued satisfaction with the grantee.',
  /* notes */ NULL),

  ('Open Philanthropy','The Wilson Center',291214,'2021-04-01','month','donation log','AI safety/governance','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/wilson-center-ai-policy-training-program','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Luke Muehlhauser',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to pilot an AI policy training program. The Wilson Center is a non-partisan policy forum for tackling global issues through independent research and open dialogue."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','The Wilson Center',2023322,'2022-01-01','month','donation log','AI safety/governance','https://www.openphilanthropy.org/grants/wilson-center-ai-policy-training-program-2022/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to "support [grantee''s] AI policy training program, which is aimed at staffers for members of Congress and other policymakers. The program’s ultimate goal is to increase policymakers’ access to technical AI expertise."',
  /* intended_funding_timeframe_in_months */ 24,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL);

/* Grants to Center for a New American Security */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision,donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','Center for a New American Security',24350,'2020-10-01','month','donation log','AI safety/governance','https://www.openphilanthropy.org/grants/center-for-a-new-american-security-ai-governance-projects/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Paul Scharre','Luke Muehlhauser',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support work exploring possible projects related to AI governance."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'No explicit reason is provided for the donation, but another donation https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/center-for-a-new-american-security-ai-and-security-projects is made at around the same time, to the same donee and with the same earmark (Paul Scharre) suggesting a broader endorsement.',
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ 'No explicit reason is provided for the timing of the donation, but another donation https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/center-for-a-new-american-security-ai-and-security-projects is made at around the same time, to the same donee and with the same earmark (Paul Scharre).',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Center for a New American Security',116744,'2020-10-01','month','donation log','AI safety/governance','https://www.openphilanthropy.org/grants/center-for-a-new-american-security-ai-and-security-projects/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Paul Scharre','Luke Muehlhauser',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support work by Paul Scharre on projects related to AI and security."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'No explicit reason is provided for the donation, but another donation https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/center-for-a-new-american-security-ai-governance-projects is made at around the same time, to the same donee and with the same earmark (Paul Scharre) suggesting a broader endorsement.',
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ 'No explicit reason is provided for the timing of the donation, but another donation https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/center-for-a-new-american-security-ai-governance-projects is made at around the same time, to the same donee and with the same earmark (Paul Scharre).',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Center for a New American Security',101187,'2021-09-01','month','donation log','AI safety/governance','https://www.openphilanthropy.org/grants/center-for-a-new-american-security-risks-from-militarized-ai/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to "support a working group that will focus on mitigating risks from possible military applications of artificial intelligence. This group will be composed of technical and policy experts from the US, Russia, China, and Europe, and will investigate possible confidence-building measures (actions designed to prevent miscalculation and conflict between states) for militarized AI."',
  /* intended_funding_timeframe_in_months */ 18,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Center for a New American Security',4816710,'2022-07-01','month','donation log','AI safety/governance','https://www.openphilanthropy.org/grants/center-for-a-new-american-security-work-on-ai-governance/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to "support work related to artificial intelligence policy and governance."',
  /* intended_funding_timeframe_in_months */ 36,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL);

/* Grants to Masschusetts Institute of Technology (MIT) */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision,donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','Massachusetts Institute of Technology',275344,'2020-11-01','month','donation log','AI safety/strategy','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/massachusetts-institute-of-technology-ai-trends-and-impacts-research','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Neil Thompson','Luke Muehlhauser',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'The grant page says "The research will consist of projects to learn how algorithmic improvement affects economic growth, gather data on the performance and compute usage of machine learning methods, and estimate cost models for deep learning projects."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/grants/massachusetts-institute-of-technology-ai-trends-and-impacts-research-2022/ suggests continued satisfaction with the grantee.',
  /* notes */ NULL),

  ('Open Philanthropy','Massachusetts Institute of Technology',1430000,'2021-02-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/mit-adversarial-robustness-research','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Aleksander Madry','Catherine Olsson|Daniel Dewey',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support research by Professor Aleksandr Madry on adversarial robustness as a means to improve AI safety."',
  /* intended_funding_timeframe_in_months */ 36,
  /* donor_donee_reason */ 'This is one of five grants made by the donor for "adversarial robustness research" in January and February 2021, all with the same grant investigators (Catherine Olsson and Daniel Dewey) except the Santa Cruz grant that had Olsson and Nick Beckstead. https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-santa-cruz-xie-adversarial-robustness https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/university-of-tuebingen-adversarial-robustness-hein https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-adversarial-robustness-wagner and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-adversarial-robustness-song are the four other grants. It looks like the donor became interested in funding this research topic at this time.',
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ 'This is one of five grants made by the donor for "adversarial robustness research" in January and February 2021, all with the same grant investigators (Catherine Olsson and Daniel Dewey) except the Santa Cruz grant that had Olsson and Nick Beckstead. https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-santa-cruz-xie-adversarial-robustness https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/university-of-tuebingen-adversarial-robustness-hein https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-adversarial-robustness-wagner and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-adversarial-robustness-song are the four other grants. It looks like the donor became interested in funding this research topic at this time.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Massachusetts Institute of Technology',13277348,'2022-03-01','month','donation log','AI safety/strategy','https://www.openphilanthropy.org/grants/massachusetts-institute-of-technology-ai-trends-and-impacts-research-2022/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Neil Thompson',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to "support research led by Neil Thompson on modeling the trends and impacts of AI and computing. Thompson will use this funding to hire new staff and expand his lab work."',
  /* intended_funding_timeframe_in_months */ 48,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL);

/* Grants to Stanford University */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision,donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','Stanford University',25000,'2017-03-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/stanford-university-percy-liang-planning-grant','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Percy Liang','Daniel Dewey','2017-09-26','day','https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/2zSvBZmn_J8',NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to enable Professor Liang to spend significant time engaging in our process to determine whether to provide his research group with a much larger grant." The larger grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/stanford-university-support-percy-liang would be made.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ 'The grant is a planning grant intended to help Percy Liang write up a proposal for a bigger grant.',
  /* donor_retrospective */ 'The bigger proposal whose writing was funded by this grant would lead to a bigger grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/stanford-university-support-percy-liang in May 2017.',
  /* notes */ NULL),

  ('Open Philanthropy','Stanford University',1337600,'2017-05-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/stanford-university-support-percy-liang','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Percy Liang','Daniel Dewey','2017-09-26','day','https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/2zSvBZmn_J8',NULL,
  /* donation_process */ 'The grant is the result of a proposal written by Percy Liang. The writing of the proposal was funded by a previous grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/stanford-university-percy-liang-planning-grant written March 2017. The proposal was reviewed by two of Open Phil''s technical advisors, who both felt largely positive about the proposed research directions.',
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'The grant is intended to fund about 20% of Percy Liang''s time as well as about three graduate students. Liang expects to focus on a subset of these topics: robustness against adversarial attacks on ML systems, verification of the implementation of ML systems, calibrated/uncertainty-aware ML, and natural language supervision.',
  /* intended_funding_timeframe_in_months */ 48,
  /* donor_donee_reason */ 'The grant page says: "Both [technical advisors who reviewed te garnt proposal] felt largely positive about the proposed research directions and recommended to Daniel that Open Philanthropy make this grant, despite some disagreements [...]."',
  /* donor_amount_reason */ 'The amount is likely determined by the grant proposal details; it covers about 20% of Percy Liang''s time as well as about three graduate students.',
  /* donor_timing_reason */ 'The timing is likely determined by the timing of the grant proposal being ready.',
  /* donor_next_donation_thoughts */ 'The grant page says: "At the end of the grant period, we will decide whether to renew our support based on our technical advisors’ evaluation of Professor Liang’s work so far, his proposed next steps, and our assessment of how well his research program has served as a pipeline for students entering the field. We are optimistic about the chances of renewing our support. We think the most likely reason we might choose not to renew would be if Professor Liang decides that AI alignment research isn’t a good fit for him or for his students."',
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/grants/stanford-university-ai-alignment-research-2021/ suggests satisfaction with the grant outcome.',
  /* notes */ NULL),

  ('Open Philanthropy','Stanford University',6771,'2018-04-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/stanford-nips-workshop-machine-learning','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Daniel Dewey','2018-04-18','day',NULL,NULL, /* date adjusted */
  /* donation_process */ 'Discretionary grant',
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to support the Neural Information Processing System (NIPS) workshop “Machine Learning and Computer Security.” at https://nips.cc/Conferences/2017/Schedule?showEvent=8775',
  /* intended_funding_timeframe_in_months */ 1,
  /* donor_donee_reason */ 'No specific reasons are included in the grant, but several of the workshop presenters for the previous year''s conference (2017) would have their research funded by Open Philanthropy, including Jacob Steinhardt, Percy Liang, and Dawn Song.',
  /* donor_amount_reason */ 'The amount was likely determined by the cost of running the workshop. The original amount of $2,539 was updated in June 2020 to $6,771.',
  /* donor_timing_reason */ 'The timing was likely determined by the timing of the conference.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'The original amount of $2,539 was updated in June 2020 to $6,771.'),

  ('Open Philanthropy','Stanford University',100000,'2018-07-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/stanford-university-machine-learning-security-research-dan-boneh-florian-tramer','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Dan Boneh|Florian Tremer','Daniel Dewey','2018-09-06','day',NULL,NULL, /* date adjusted */
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support machine learning security research led by Professor Dan Boneh and his PhD student, Florian Tramer."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'The grant page gives three reasons: (1) Florian Tremer is a very strong Ph.D. student, (2) excellent machine learning security work is important for AI safety, (3) increased funding in areas relevant to AI safety, like machine learning security, is expected to lead to more long-term benefits for AI safety.',
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'Grant is structured as an unrestricted "gift" to Stanford University Computer Science.'),

  ('Open Philanthropy','Stanford University',6500,'2020-01-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/stanford-university-ai-safety-seminar','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Dorsa Sadigh','Daniel Dewey',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'The grant "is intended to fund the travel costs for experts on AI safety to present at the [AI safety] seminar [led by Dorsa Sadigh]."',
  /* intended_funding_timeframe_in_months */ 1,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Stanford University',330792,'2021-08-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/stanford-adversarial-robustness-research-tsipras','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Dimitis Tsipras','Catherine Olsson',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support early-career research by Dimitris Tsipras on adversarial robustness as a means to improve AI safety."',
  /* intended_funding_timeframe_in_months */ 36,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ 'No explicit reasons for the amount are given, but the amount is similar to the amounts for other grants from Open Philanthropy to early-stage researchers in adversarial robustness research. This includes the two other grants https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/stanford-adversarial-robustness-research-santurkar and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/university-of-southern-california-adversarial-robustness-research made around the same time, as well as grants earlier in the year to researchers at Carnegie Mellon University, University of Tübingen, and UC Berkeley.',
  /* donor_timing_reason */ 'At around the same time as this grant, Open Philanthropy made two other grants https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/stanford-adversarial-robustness-research-santurkar and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/university-of-southern-california-adversarial-robustness-research to early-stage researchers in adversarial robustness research.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'Open Phil made another grant http://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/stanford-adversarial-robustness-research-santurkar at the same time, for the same amount and 3-year timeframe, with the same grant investigator, and with the same receiving university.'),

  ('Open Philanthropy','Stanford University',330792,'2021-08-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/stanford-university-adversarial-robustness-research-shibani-santurkar/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Shibani Santurkar','Catherine Olsson',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support early-career research by Shibani Santurkar on adversarial robustness as a means to improve AI safety."',
  /* intended_funding_timeframe_in_months */ 36,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ 'No explicit reasons for the amount are given, but the amount is similar to the amounts for other grants from Open Philanthropy to early-stage researchers in adversarial robustness research. This includes the two other grants https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/stanford-adversarial-robustness-research-tsipras and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/university-of-southern-california-adversarial-robustness-research made around the same time, as well as grants earlier in the year to researchers at Carnegie Mellon University, University of Tübingen, and UC Berkeley.',
  /* donor_timing_reason */ 'At around the same time as this grant, Open Philanthropy made two other grants https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/stanford-adversarial-robustness-research-tsipras and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/university-of-southern-california-adversarial-robustness-research to early-stage researchers in adversarial robustness research.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'Open Phil made another grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/stanford-adversarial-robustness-research-tsipras at the same time, for the same amount and 3-year timeframe, with the same grant investigator, and with the same receiving university.'),

  ('Open Philanthropy','Stanford University',78000,'2021-09-01','month','donation log','AI safety/strategy','https://www.openphilanthropy.org/grants/stanford-university-ai-index/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to "support the AI Index, which collects and reports data related to artificial intelligence, including data relevant to AI safety and AI ethics." The webpage https://aiindex.stanford.edu/ is linked.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Stanford University',1500000,'2021-11-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/stanford-university-ai-alignment-research-2021/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Percy Liang',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to "support research led by Professor Percy Liang on AI safety and alignment."',
  /* intended_funding_timeframe_in_months */ 36,
  /* donor_donee_reason */ 'The grant page says: "We hope this funding will accelerate progress on technical problems and help to build a pipeline for younger researchers to work on AI alignment."',
  /* donor_amount_reason */ 'No explicit reason is given for the amount. It is somewhat but not a lot higher per year ($1,500,000 over 3 years = $500,000) than the previous grant https://www.openphilanthropy.org/grants/stanford-university-support-for-percy-liang/ ($1,337,600 over 4 years = $334,400 per year).',
  /* donor_timing_reason */ 'No explicit reason is given for the timing. The grant is made right around the end of the timeframe of the previous grant https://www.openphilanthropy.org/grants/stanford-university-support-for-percy-liang/ (four-year grant made in 2017) also for Percy Liang''s research.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Stanford University',153820,'2022-07-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/stanford-university-ai-alignment-research-barrett-and-viteri/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Clark Barrett|Scott Viteri',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support research on AI alignment by Professor Clark Barrett and Stanford student Scott Viteri."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL);

/* Grants to Redwood Research */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision, donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','Redwood Research',9420000,'2021-11-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/redwood-research-general-support/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Nick Beckstead',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'Grant "for general support. Redwood Research is a new research institution that conducts research to better understand and make progress on AI alignment in order to improve the long-run future."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/grants/redwood-research-general-support-2/ of a comparable amount ($10.7 million) suggests continued satisfaction with the grantee.',
  /* notes */ 'This is a total across four grants.'),

  ('Open Philanthropy','Redwood Research',10700000,'2022-08-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/redwood-research-general-support-2/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'Grant "for general support. Redwood Research is a nonprofit research institution focused on aligning advanced AI with human interests."',
  /* intended_funding_timeframe_in_months */ 18,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/grants/redwood-research-general-support-2023/ suggests continued satisfaction with the grantee.',
  /* notes */ NULL),

  ('Open Philanthropy','Redwood Research',5300000,'2023-06-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/redwood-research-general-support-2023/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'Grant "for general support. Redwood Research is a nonprofit research institution focused on aligning advanced AI with human interests."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL);

/* Grants to Centre for the Governance of AI */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision, donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','Centre for the Governance of AI',450000,'2020-05-01','month','donation log','AI safety/governance','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/gov-ai-general-support','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Committee for Effective Altruism Support',NULL,NULL,NULL,NULL,
  /* donation_process */ 'The grant was recommended by the Committee for Effective Altruism Support following its process https://www.openphilanthropy.org/committee-effective-altruism-support',
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'The grant page says: "GovAI intends to use these funds to support the visit of two senior researchers and a postdoc researcher."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'The grant page says "we see the basic pros and cons of this support similarly to what we’ve presented in past writeups on the matter" but does not link to specific past writeups (Open Phil has not previously made grants directly to GovAI).',
  /* donor_amount_reason */ 'The amount is decided by the Committee for Effective Altruism Support https://www.openphilanthropy.org/committee-effective-altruism-support but individual votes and reasoning are not public.',
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The much larger followup grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/gov-ai-field-building (December 2021) suggests continued satisfaction with the grantee.',
  /* notes */ 'Grant made via the Berkeley Existential Risk Initiative.'),

  ('Open Philanthropy','Centre for the Governance of AI',2537600,'2021-12-01','month','donation log','AI safety/governance','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/gov-ai-field-building','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Luke Muehlhauser',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support activities related to building the field of AI governance research. GovAI intends to use this funding to conduct AI governance research and to develop a talent pipeline for those interested in entering the field."',
  /* intended_funding_timeframe_in_months */ 24,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grants https://www.openphilanthropy.org/grants/centre-for-the-governance-of-ai-research-assistant/ and https://www.openphilanthropy.org/grants/centre-for-the-governance-of-ai-general-support-2/ suggest continued satisfaction with the grantee.',
  /* notes */ 'Grant made via the Centre for Effective Altruism.'),

  ('Open Philanthropy','Centre for the Governance of AI',50532,'2022-09-01','month','donation log','AI safety/governance','https://www.openphilanthropy.org/grants/centre-for-the-governance-of-ai-compute-strategy-workshop/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support a workshop bringing together compute experts from several subfields, such as large-model infrastructure, ASIC design, and governance, to discuss compute governance ideas that could reduce existential risk from artificial intelligence."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Centre for the Governance of AI',19200,'2022-09-01','month','donation log','AI safety/governance','https://www.openphilanthropy.org/grants/centre-for-the-governance-of-ai-research-assistant/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support a new research assistant."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Centre for the Governance of AI',1000000,'2023-05-01','month','donation log','AI safety/governance','https://www.openphilanthropy.org/grants/centre-for-the-governance-of-ai-general-support-2/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'Grant "to the Centre for the Governance of AI (GovAI) for general support. GovAI conducts research on AI governance and works to develop a talent pipeline for those interested in entering the field."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL);

/* Grants to Alignment Research Center */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision, donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','Alignment Research Center',265000,'2022-03-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/alignment-research-center-general-support/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'Grant "for general support. ARC focuses on developing strategies for AI alignment that can be adopted by industry today and scaled to future machine learning systems."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'While no reason is specified in the grant page, it''s worth noting that the founder of the donee organization, Paul Christiano, has previously been a technical advisor to Open Philanthropy, and has been affiliated with multiple organizations (Machine Intelligence Research Institute, OpenAI, and Ought) that have previously received funding from Open Philanthropy for AI safety. These past connections may have influenced the grant.',
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ 'The grant is made shortly after the announcement by Alignment Research Center of its plans at https://www.alignment.org/blog/early-2022-hiring-round/ to hire beyond its current full-time staff of two. As grants are often committed after the internal decision process to make them, it is possible that the funding for this grant was sought for the purpose of this round of hiring, and was factored into the hiring announcement.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup two-year grant https://www.openphilanthropy.org/grants/alignment-research-center-general-support-november-2022/ suggests continued satisfaction with the grantee.',
  /* notes */ NULL),

  ('Open Philanthropy','Alignment Research Center',1250000,'2022-11-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/alignment-research-center-general-support-november-2022/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'Grant "for general support. The Alignment Research Center conducts research on how to align AI with human interests, with a focus on techniques that could be adopted in existing machine learning systems and effectively scale up to future systems."',
  /* intended_funding_timeframe_in_months */ 24,
  /* donor_donee_reason */ 'While no reason is specified in the grant page, it''s worth noting that the founder of the donee organization, Paul Christiano, has previously been a technical advisor to Open Philanthropy, and has been affiliated with multiple organizations (Machine Intelligence Research Institute, OpenAI, and Ought) that have previously received funding from Open Philanthropy for AI safety. These past connections may have influenced the grant.',
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ 'The grant is made eight months after the previous $265,000 grant https://www.openphilanthropy.org/grants/alignment-research-center-general-support/ and likely reflects the renewal of that now-used-up funding.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL);

/* Grants to FAR AI */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision, donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','FAR AI',425800,'2021-10-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/language-model-safety-fund-language-model-misalignment/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Ethan Perez',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to the Fund for Alignment Research, led by Ethan Perez, to support salaries and equipment for projects related to misalignment in language models. Perez plans to hire and supervise four engineers to work on these projects."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/grants/fund-for-alignment-research-language-model-misalignment-2022/ for a similar amount (and with the same research area and leader Ethan Perez), as well as several other followup grants in the coming years, suggest continued satisfaction with the grantee.',
  /* notes */ NULL),

  ('Open Philanthropy','FAR AI',463693,'2022-08-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/fund-for-alignment-research-language-model-misalignment-2022/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Ethan Perez',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support research projects, led by Ethan Perez, related to misalignment in language models."',
  /* intended_funding_timeframe_in_months */ 18,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'Several followup grants, such as https://www.openphilanthropy.org/grants/far-ai-general-support/ for general support, suggest continued satisfaction with the grantee. However, as of mid-2023, there are no followup grants exclusively for the research area of this grant (language model misalignment).',
  /* notes */ NULL),

  ('Open Philanthropy','FAR AI',50000,'2022-12-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/far-ai-interpretability-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Alex Tamkin',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support [FAR AI''s] research on machine learning interpretability, in collaboration with Open Philanthropy AI Fellow Alex Tamkin."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/grants/far-ai-ai-interpretability-research/ for the same research area and same leader (Alex Tamkin), as well as several other grants to the organization, suggest continued satisfaction with the grantee.',
  /* notes */ NULL),

  ('Open Philanthropy','FAR AI',49500,'2022-12-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/far-ai-inverse-scaling-prize/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support their Inverse Scaling Prize, which is a contest that awards prizes to contestants who find examples of tasks where language models perform worse as they scale."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ 'The announcement post https://www.alignmentforum.org/posts/eqxqgFxymP8hXDTt5/announcing-the-inverse-scaling-prize-usd250k-prize-pool (published six months prior to the grant) states that the total prize pool is $250,000. https://github.com/inverse-scaling/prize#prize-information says "2023/03/21 Update: The prize pool has been funded by Open Philanthropy" suggesting that the amount provided by Open Philanthropy closed the funding gap to the target amount of $250,000.',
  /* donor_timing_reason */ 'The grant is made six months after the announcement https://www.alignmentforum.org/posts/eqxqgFxymP8hXDTt5/announcing-the-inverse-scaling-prize-usd250k-prize-pool of the $250,000 prize pool for the prize. https://github.com/inverse-scaling/prize#prize-information says "2023/03/21 Update: The prize pool has been funded by Open Philanthropy"; this suggests that Open Philanthropy made the grant in light of the already-running prize in order to fill the funding gap.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'See the announcement post https://www.alignmentforum.org/posts/eqxqgFxymP8hXDTt5/announcing-the-inverse-scaling-prize-usd250k-prize-pool and the GitHub repository https://github.com/inverse-scaling/prize for more details.'),

  ('Open Philanthropy','FAR AI',625000,'2022-12-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/far-ai-general-support/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'Grant "for general support. FAR AI works to incubate and accelerate research agendas to ensure AI systems are more trustworthy and beneficial to society."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup general support grant https://www.openphilanthropy.org/grants/far-ai-general-support-2023/ as well as other followup grants to FAR AI suggest continued satisfaction with the grantee.',
  /* notes */ NULL),

  ('Open Philanthropy','FAR AI',280000,'2023-03-01','month','donation log','AI safety/movement growth','https://www.openphilanthropy.org/grants/far-ai-far-labs-office-space/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support FAR Labs, an office space in Berkeley for people working on AI safety and alignment."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','FAR AI',100000,'2023-03-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/far-ai-ai-interpretability-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support a research project, led by Open Philanthropy AI Fellow Alex Tamkin, aimed at developing a neural network architecture that could serve as a more interpretable alternative to the transformer architecture used in leading language models."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','FAR AI',460000,'2023-07-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/far-ai-general-support-2023/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ NULL,
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','FAR AI',166500,'2023-09-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/far-ai-alignment-workshop/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support a two-day alignment workshop in advance of NeurIPS 2023, a major machine learning and computational neuroscience conference."',
  /* intended_funding_timeframe_in_months */ 1,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL);

/* Grants to Rethink Priorities */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision,donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes, is_contractwork) values
  ('Open Philanthropy','Rethink Priorities',495685,'2021-07-01','month','donation log','AI safety/governance','https://www.openphilanthropy.org/grants/rethink-priorities-ai-governance-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Luke Muehlhauser',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support research projects on topics related to AI governance."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'The grant page says: "We believe that Rethink Priorities’ research outputs may help inform our AI policy grantmaking strategy."',
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL,
  /* is_contractwork */ 1),

  ('Open Philanthropy','Rethink Priorities',2728319,'2022-03-01','month','donation log','AI safety/governance','https://www.openphilanthropy.org/grants/rethink-priorities-ai-governance-research-2022/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to Rethink Priorities to expand its research on topics related to AI governance."',
  /* intended_funding_timeframe_in_months */ 24,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/grants/rethink-priorities-ai-governance-research-2023/ suggests continued satisfaction with the grantee.',
  /* notes */ NULL,
  /* is_contractwork */ 0),

  ('Open Philanthropy','Rethink Priorities',302390,'2023-04-01','month','donation log','AI safety/governance','https://www.openphilanthropy.org/grants/rethink-priorities-ai-governance-workshop/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support an in-person workshop bringing together professionals working on AI governance."',
  /* intended_funding_timeframe_in_months */ 1,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL,
  /* is_contractwork */ 0),

  ('Open Philanthropy','Rethink Priorities',154810,'2023-04-01','month','donation log','AI safety/governance','https://www.openphilanthropy.org/grants/rethink-priorities-ai-governance-research-2023/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support research on AI governance, with a focus on hardware security features."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'As of 2023-10-14, the text of the grant page states the amount to be $154,801, but the metadata on top and on the grants list page states the amount to be $154,810.',
  /* is_contractwork */ 0);

/* Grants to Mila */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision, donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes, amount_original_currency, original_currency, currency_conversion_basis) values
  ('Open Philanthropy','Mila',2400000,'2017-07-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/montreal-institute-for-learning-algorithms-ai-safety-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Yoshua Bengio|Joelle Pineau|Doina Precup',NULL,'2017-07-19','day','https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/qL9PbCP-U3w',NULL,
  /* donation_process */ 'The grant page says: "We spoke with Professor Bengio and several of his students during our recent outreach to machine learning researchers and formed a positive impression of him and his work. Our technical advisors spoke highly of Professor Bengio’s capabilities, reputation, and goals."',
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support technical research on potential risks from advanced artificial intelligence (AI). $1.6 million of this grant will support Professor Yoshua Bengio and his co-investigators at the Université de Montréal, and $800,000 will support Professors Joelle Pineau and Doina Precup at McGill University. We see Professor Bengio’s research group as one of the world’s preeminent deep learning labs and are excited to provide support for it to undertake AI safety research."',
  /* intended_funding_timeframe_in_months */ 48,
  /* donor_donee_reason */ 'The grant page says: "Among potential grantees in the field, we believe that Professor Bengio is one of the best positioned to help build the talent pipeline in AI safety research. Our understanding, based on conversations with our technical advisors and our general impressions from the field, is that many of the most talented machine learning researchers spend some time in Professor Bengio’s lab before joining other universities or industry groups. This is an important contributing factor to our expectations for the impact of this grant, both because it increases our confidence in the quality of the research that this grant will support and because of the potential benefits for pipeline building. In our conversations with Professor Bengio, we’ve found significant overlap between his perspective on AI safety and ours, and Professor Bengio was excited to be part of our overall funding activities in this area. We think that Professor Bengio is likely to serve as a valuable member of the AI safety research community, and that he will encourage his lab to be involved in that community as well. We believe that members of his lab could likely be valuable participants at future workshops on AI safety."',
  /* donor_amount_reason */ 'The grant page says: "Our impression is that MILA is already fairly well-funded, and that its ability to use additional marginal funding is somewhat limited. Professor Bengio told us that the amount of additional yearly funding that he would be able to use productively for AI safety research is $400,000; we have decided to grant this full amount for four years ($1.6 million total). We have also granted two of Professor Bengio’s co-investigators at MILA who are also interested in working on this agenda, Professors Pineau and Precup, $200,000 per year ($800,000 total), which they estimated as the amount of funding they would be able to use productively."',
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ 'The grant page says: "We expect to have a conversation with Professor Bengio six months after the start of the grant, and annually after that, to discuss his projects and results, with public notes if the conversation warrants it. In the first few months of the grant, we plan to visit Montreal for several days to meet Professor Bengio’s co-investigators and discuss the project with them. At the conclusion of this grant in 2020, we will decide whether to renew our support. If Professor Bengio’s research is going well (based on our technical advisors’ assessment and the impressions of others in the field), and if we have achieved a better mutual understanding with Professor Bengio about how his research is likely to be valuable, it is likely that we will decide to provide renewed funding. If Professor Bengio is using half or more of our funding to pursue research directions that we do not find particularly promising, it is likely that we would choose not to renew."',
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/grants/mila-research-project-on-artificial-intelligence/ suggests continued satisfaction with the grantee, though the amount of this followup grant is much smaller and the scope narrower than that of the original grant.',
  /* notes */ 'See also https://www.facebook.com/permalink.php?story_fbid=10110258359382500&id=13963931 for a Facebook share by David Krueger, a member of the grantee organization. The comments include some discussion about the grantee.',
  /* amount_original_currency */ NULL,
  /* original_currency */ NULL,
  /* currency_conversion_basis */ NULL),

  ('Open Philanthropy','Mila',237931,'2021-11-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/mila-research-project-on-artificial-intelligence/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Luke Muehlhauser',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support a research project investigating AI consciousness and moral patienthood. The research will be conducted in collaboration with the Université de Montréal and the Future of Humanity Institute. This funding will support postdoctoral researchers and students studying the topic, as well as publications and workshops."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL,
  /* amount_original_currency */ 295900,
  /* original_currency */ 'CAD',
  /* currency_conversion_basis */ 'donor calculation'),

  ('Open Philanthropy','Mila',50000,'2023-05-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/mila-workshop-on-human-level-ai/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Jacob Steinhardt',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support a workshop on human-level artificial intelligence, led by Professor Jacob Steinhardt, that will bring together experts on AI and AI alignment."',
  /* intended_funding_timeframe_in_months */ 1,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL,
  /* amount_original_currency */ NULL,
  /* original_currency */ NULL,
  /* currency_conversion_basis */ NULL);

/* Grants to Center for AI safety */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision, donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','Center for AI Safety',5160000,'2022-11-01','month','donation log','AI safety/technical research/movement growth','https://www.openphilanthropy.org/grants/center-for-ai-safety-general-support/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'Grant "for general support. The Center for AI Safety does technical research and field-building aimed at reducing catastrophic and existential risks from artificial intelligence."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup general support grant https://www.openphilanthropy.org/grants/center-for-ai-safety-general-support-2023/ in 2023 for a similar amount suggests continued satisfaction with the grantee.',
  /* notes */ NULL),

  ('Open Philanthropy','Center for AI Safety',1433000,'2023-02-01','month','donation log','AI safety/technical research/strategy','https://www.openphilanthropy.org/grants/center-for-ai-safety-philosophy-fellowship/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support the CAIS Philosophy Fellowship, which is a research fellowship that will support philosophers researching topics related to AI safety. This grant also supported a workshop on adversarial robustness, as well as prizes for safety-related competitions at the 2022 NeurIPS conference." Links: https://philosophy.safe.ai/ for CAIS Philosophy Fellowship, https://eccv22-arow.github.io/ for the workshop, and https://trojandetection.ai/ and https://neurips2022.mlsafety.org/ for the prizes.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Center for AI Safety',4025729,'2023-04-01','month','donation log','AI safety/technical research/movement growth','https://www.openphilanthropy.org/grants/center-for-ai-safety-general-support-2023/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'Grant "for general support. The Center for AI Safety works on research, field-building, and advocacy to reduce existential risks from artificial intelligence."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL);

/* Grants to Epoch */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision, donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','Epoch',1960000,'2022-06-01','month','donation log','AI safety/strategy','https://www.openphilanthropy.org/grants/epoch-general-support/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'Grant "for general support. Epoch is a research organization that works on investigating trends in machine learning and forecasting the development of transformative artificial intelligence."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Epoch',188558,'2023-02-01','month','donation log','AI safety/strategy','https://www.openphilanthropy.org/grants/epoch-ai-worldview-investigations/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support [Epoch''s] “worldview investigations” related to AI." The linked blog post section https://www.openphilanthropy.org/research/our-progress-in-2019-and-plans-for-2020/#worldview-investigations describes this in more detail, starting with: "Identify debatable views we hold that play a key role in our cause prioritization, such as the view that there’s a nontrivial likelihood of transformative artificial intelligence being developed by 2036."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Epoch',6922565,'2023-04-01','month','donation log','AI safety/strategy','https://www.openphilanthropy.org/grants/epoch-general-support-2023/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'Grant "for general support. Epoch researches trends in machine learning to better understand the pace of progress in artificial intelligence, and to help forecast the development of advanced AI and its subsequent economic impacts."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL);

/* Grants to Conjecture */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision, donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','Conjecture',457380,'2022-10-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/conjecture-seri-mats-program-in-london/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','SERI-MATS program',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support [Conjecture''s] collaboration with the Stanford Existential Risks Initiative (SERI) on SERI’s Machine Learning Alignment Theory Scholars (MATS) program. MATS is an educational seminar and independent research program that aims to provide talented scholars with talks, workshops, and research mentorship in the field of AI alignment. This grant will support a London-based extension for a MATS cohort that started in Berkeley. Conjecture will use this funding to provide office space in London and operations support."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'The grant is a followup to the grant https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-seri-mats-program/ for the original SERI-MATS cohort (the second cohort). Conjecture was likely selected for the grant due to its interest, willingness and ability to manage the logistics of the extension in London.',
  /* donor_amount_reason */ 'While no reason is provided for the amount, the amount is a little under half the amount granted at https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-seri-mats-program/ for the SERI-MATS cohort whose extension is being funded by this grant.',
  /* donor_timing_reason */ 'The grant is made six months after the grant https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-seri-mats-program/ for the SERI-MATS cohort whose extension is being funded by this grant. This makes sense since the extension happens after the program, whose duration (including application steps) is about 6 months.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/grants/conjecture-seri-mats-2023/ for a similar London-based extension of the third SERI-MATS cohort suggests continued satisfaction with the program being funded.',
  /* notes */ NULL),

  ('Open Philanthropy','Conjecture',245000,'2023-04-01','month','donation log','AI safety/technical research/talent pipeline','https://www.openphilanthropy.org/grants/conjecture-seri-mats-2023/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','SERI-MATS program',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support [Conjecture''s] collaboration with the Stanford Existential Risks Initiative (SERI) on SERI’s Machine Learning Alignment Theory Scholars (MATS) program. MATS is an educational seminar and independent research program that aims to provide talented scholars with talks, workshops, and research mentorship in the field of AI alignment. This grant will support a London-based extension of the MATS program’s third cohort, which we supported last year."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'This grant is a followup to the grant https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-machine-learning-alignment-theory-scholars/ for the original SERI-MATS cohort (the third cohort). Conjecture was likely selected for the grant due to its interest, willingness and ability to manage the logistics of the extension in London, and its success at running a similar extension for the second cohort; that extension was also funded by Open Philanthropy (see https://www.openphilanthropy.org/grants/conjecture-seri-mats-program-in-london/ for details).',
  /* donor_amount_reason */ 'While no reason is provided for the amount, the amount is a little over 10% of the amount for the SERI-MATS grant https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-machine-learning-alignment-theory-scholars/ and a little over half the amount of the previous extension grant. The previous extension grant had been about half the corresponding SERI-MATS cohort grant. The reason for the reduced amount of this grant is not clear.',
  /* donor_timing_reason */ 'The grant is made six months after the grant https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-machine-learning-alignment-theory-scholars/ for the SERI-MATS cohort whose extension is being funded by this grant. This makes sense since the extension happens after the program, whose duration (including application steps) is about 6 months.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL);

/* Grants to University of Utah */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision, donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','University of Utah',140000,'2023-04-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/university-of-utah-ai-alignment-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Daniel Brown',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support research led by Professor Daniel Brown on ways to verify the extent to which an AI system is aligned with human values."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','University of Utah',31773,'2023-08-01','month','donation log','AI safety/movement growth','https://www.openphilanthropy.org/grants/university-of-utah-course-on-human-ai-alignment/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Daniel Brown',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ 'The grant is part of Open Philanthropy Course Development Grants https://www.openphilanthropy.org/open-philanthropy-course-development-grants/ for which applications can be submitted online. The grant page https://www.openphilanthropy.org/grants/university-of-utah-course-on-human-ai-alignment/ says: "We sought applications for this funding to support the development of courses on a range of topics that are relevant to certain areas of Open Philanthropy’s grantmaking."',
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support Daniel Brown in developing a course on human-AI alignment."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ 'The online application form linked from https://www.openphilanthropy.org/open-philanthropy-course-development-grants/ requires applicants to include an estimated budget. The amount granted was likely based on the budget submitted by the applicant.',
  /* donor_timing_reason */ 'The timing was likely determined by the timing of the grant application, as well as the academic year cycle. Details are not publicly available.',
  /* donor_next_donation_thoughts */ 'The "Grantee expectations" section at https://www.openphilanthropy.org/open-philanthropy-course-development-grants/ does not specifically talk about followup grants. It says: "We would like grantees to continue teaching the developed course in the future (at least three times), but this is not a requirement of a grant. Grantees are required to provide us, after completion of the course, with a copy of the course syllabus, a copy of the final exam/final paper (if permitted of by the relevant university’s policies), enrollment statistics, student evaluations, and a brief summary (roughly half a page in length) describing their own experience teaching the course. We will strongly encourage grantees to make their syllabi available online, but we won’t require this." This suggests that the grantee is not expected to receive followup grants for teaching the same course, since the grant is for course *development*; further grants for different courses may be possible.',
  /* donor_retrospective */ NULL,
  /* notes */ 'On the Open Philanthropy website, the cause area is listed as global catastrophic risks rather than AI safety. We''re using AI safety on the donations list website, so this may result in inconsistencies in some totals between the Open Philanthropy website and the donations list website.');

/* Grants to AI Safety Support */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision, donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','AI Safety Support',42000,'2022-04-01','month','donation log','AI safety/strategy','https://www.openphilanthropy.org/grants/ai-safety-support-research-on-trends-in-machine-learning/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Jaime Sevilla',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to scale up a research group, led by Jaime Sevilla, which studies trends in machine learning."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'Further grant https://www.openphilanthropy.org/grants/ai-safety-support-seri-mats-program/ and https://www.openphilanthropy.org/grants/ai-safety-support-situational-awareness-research/ from Open Philanthropy, though with slightly different goals, suggest continued satisfaction with the grantee.',
  /* notes */ NULL),

  ('Open Philanthropy','AI Safety Support',1538000,'2022-11-01','month','donation log','AI safety/technical research/talent pipeline','https://www.openphilanthropy.org/grants/ai-safety-support-seri-mats-program/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','SERI-MATS program',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support [AI Safety Support''s] collaboration with Stanford Existential Risks Initiative (SERI) on SERI’s Machine Learning Alignment Theory Scholars (MATS) program. MATS is an educational seminar and independent research program that aims to provide talented scholars with talks, workshops, and research mentorship in the field of AI alignment, and connect them with in-person alignment research communities."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'See also the companion grant https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-machine-learning-alignment-theory-scholars/ to Berkeley Existential Risk Initiative and the grant https://www.openphilanthropy.org/grants/conjecture-seri-mats-2023/ to Conjecture for the London-based extension.'),

  ('Open Philanthropy','AI Safety Support',443716,'2023-04-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/ai-safety-support-situational-awareness-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Owain Evans',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Three grants "to support research led by Owain Evans to evaluate whether machine learning models have situational awareness. These grants were made to AI Safety Support, Effective Ventures Foundation USA, and the Berkeley Existential Risk Initiative, and will support salaries, office space, and compute for this research project."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'Both the Open Philanthropy website and the donations list website list the grantee as AI Safety Support, but this is actually a combination of three grants, one each to "AI Safety Support, Effective Ventures Foundation USA, and the Berkeley Existential Risk Initiative"; the single donee is for simplicity and due to system limitations.');

/* Grants to AI Safety Hub */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision, donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes, amount_original_currency, original_currency, currency_conversion_basis) values
  ('Open Philanthropy','AI Safety Hub',235000,'2022-09-01','month','donation log','AI safety/movement growth','https://www.openphilanthropy.org/grants/ai-safety-hub-startup-costs/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Julia Karbing',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'The grant page says: "Open Philanthropy recommended two grants totaling $235,000 to the AI Safety Hub to support their initial development costs, and to hire several contractors to work on projects related to AI safety. The AI Safety Hub, directed by Century Fellow Julia Karbing, is a new organization that will work on movement building in the AI safety field."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/grants/ai-safety-hub-safety-labs/ suggests satisfaction with the outcome of this grant.',
  /* notes */ 'This is a total across two grants.',
  /* amount_original_currency */ NULL,
  /* original_currency */ NULL,
  /* currency_conversion_basis */ NULL),

  ('Open Philanthropy','AI Safety Hub',63839,'2022-11-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/ai-safety-hub-safety-labs/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support [grantee''s] Safety Labs program, which will match students with mentors while the students research questions related to AI safety."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL,
  /* amount_original_currency */ 53700,
  /* original_currency */ 'GBP',
  /* currency_conversion_basis */ 'donor calculation');

/* Grants to Longview Philanthropy */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision, donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes, amount_original_currency, original_currency, currency_conversion_basis) values
  ('Open Philanthropy','Longview Philanthropy',770076,'2023-02-01','month','donation log','AI safety/governance','https://www.openphilanthropy.org/grants/longview-philanthropy-ai-policy-development-at-the-oecd/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to Longview Philanthropy to support their collaboration with the Organization for Economic Co-operation and Development (OECD) on a project to develop potential policies that could reduce existential risks from artificial intelligence."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL,
  /* amount_original_currency */ 720000,
  /* original_currency */ 'EUR',
  /* currency_conversion_basis */ 'donor calculation');

/* Grants to RAND Corporation */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision, donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','RAND Corporation',30751,'2020-01-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/rand-corporation-research-on-the-state-of-ai-assurance-methods','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Andrew Lohn','Luke Muehlhauser','2020-03-19','day',NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support exploratory research by Andrew Lohn on the state of AI assurance methods."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'A few months later, Open Phil would make a grant directly to Andrew Lohn for machine learning robustness research, suggesting that they were satisfied with the outcome from this grant.',
  /* notes */ NULL),

  ('Open Philanthropy','RAND Corporation',5500000,'2023-04-01','month','donation log','AI safety/governance','https://www.openphilanthropy.org/grants/rand-corporation-emerging-technology-fellowships-and-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Jason Matheny','Luke Muehlhauser',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to be spent at RAND President Jason Matheny’s discretion. Matheny has designated this funding to launch two new initiatives: a technology policy training program, and a research fund to help produce information that policymakers need to make wise decisions about emerging technology and security priorities."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'The grant page says: "We have been impressed with Matheny’s past work on technology and security — at IARPA, at the Center for Security and Technology, and in the White House — and we believe RAND is well-positioned to use such funding to great impact."',
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/grants/rand-corporation-emerging-technology-initiatives/ for similar purposes suggests continued satisfaction with the grantee.',
  /* notes */ NULL),

  ('Open Philanthropy','RAND Corporation',10500000,'2023-10-01','month','donation log','AI safety/governance','https://www.openphilanthropy.org/grants/rand-corporation-emerging-technology-initiatives/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Jason Matheny','Luke Muehlhauser',NULL,NULL,NULL,NULL,
  /* donation_process */ 'This is a followup grant to the grant https://www.openphilanthropy.org/grants/rand-corporation-emerging-technology-fellowships-and-research/ to the same grantee for similar purposes.',
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'The grant page lists the following initiatives to be funded by the grant: "(1) A technology policy training program. (2) Support for the Pardee RAND Graduate School. (3) A new research center focused on China studies. (4) A research fund that will help to produce information for policymakers about emerging technology and security priorities."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL);

/* Grants to University of Tübingen */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision, donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','University of Tübingen',590000,'2021-02-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/university-of-tubingen-robustness-research-wieland-brendel/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Wieland Brendel','Catherine Olsson|Nick Beckstead',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'The grant page says the grant is "to support early-career research by Wieland Brendel on robustness as a means to improve AI safety."',
  /* intended_funding_timeframe_in_months */ 36,
  /* donor_donee_reason */ 'Open Phil made five grants https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/university-of-tuebingen-adversarial-robustness-hein https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-santa-cruz-xie-adversarial-robustness https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-adversarial-robustness-wagner https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-adversarial-robustness-song https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/mit-adversarial-robustness-research for "adversarial robustness research" in January and February 2021, around the time of this grant. It looks like the donor became interested in funding this research topic at this time.',
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ 'Open Phil made five grants https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/university-of-tuebingen-adversarial-robustness-hein https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-santa-cruz-xie-adversarial-robustness https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-adversarial-robustness-wagner https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-adversarial-robustness-song https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/mit-adversarial-robustness-research for "adversarial robustness research" in January and February 2021, around the time of this grant. It looks like the donor became interested in funding this research topic at this time.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','University of Tübingen',300000,'2021-02-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/university-of-tubingen-adversarial-robustness-research-matthias-hein/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Matthias Hein','Catherine Olsson|Nick Beckstead',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support research by Professor Matthias Hein on adversarial robustness as a means to improve AI safety."',
  /* intended_funding_timeframe_in_months */ 36,
  /* donor_donee_reason */ 'This is one of five grants made by the donor for "adversarial robustness research" in January and February 2021, all with the same grant investigators (Catherine Olsson and Daniel Dewey) except the Santa Cruz grant that had Olsson and Nick Beckstead. https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-santa-cruz-xie-adversarial-robustness https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/mit-adversarial-robustness-research https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-adversarial-robustness-wagner and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-adversarial-robustness-song are the four other grants. It looks like the donor became interested in funding this research topic at this time.',
  /* donor_amount_reason */ 'No explicit reasons for the amount are given, but the amount is similar to the amounts for other grants from Open Philanthropy to early-stage researchers in adversarial robustness research. This includes three other grants https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-santa-cruz-xie-adversarial-robustness https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-adversarial-robustness-wagner and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-adversarial-robustness-song made at the same time as well as grants later in the year to early-stage researchers at Carnegie Mellon University, Stanford University, and University of Southern California.',
  /* donor_timing_reason */ 'This is one of five grants made by the donor for "adversarial robustness research" in January and February 2021, all with the same grant investigators (Catherine Olsson and Daniel Dewey) except the Santa Cruz grant that had Olsson and Nick Beckstead. https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-santa-cruz-xie-adversarial-robustness https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/mit-adversarial-robustness-research https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-adversarial-robustness-wagner and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-adversarial-robustness-song are the four other grants. It looks like the donor became interested in funding this research topic at this time.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','University of Tübingen',575000,'2023-02-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/university-of-tuebingen-adversarial-robustness-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Matthias Bethge',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support research led by Professor Matthias Bethge on adversarial robustness as a means to improve AI safety."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL);

/* Grants to University of Toronto */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision, donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','University of Toronto',520000,'2020-12-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/university-of-toronto-machine-learning-research','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Chris Maddison','Daniel Dewey|Catherine Olsson',NULL,NULL,NULL,NULL,
  /* donation_process */ 'The researcher (Chris Maddison) whose students'' work is to be funded with this grant had previously been an Open Phil AI Fellow while pursuing his DPhil in 2018. The past connection and subsequent academic progress of the researcher (now an assistant professor) may have been factors, but the grant page has no details on the decision process.',
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'The grant page says: "[the grant is] to support research on understanding, predicting, and controlling machine learning systems, led by Professor Chris Maddison, a former Open Phil AI Fellow. This funding is intended to enable three students and a postdoctoral researcher to work with Professor Maddison on the research."',
  /* intended_funding_timeframe_in_months */ 48,
  /* donor_donee_reason */ 'The researcher (Chris Maddison) whose students'' work is to be funded with htis grant had previously been an Open Phil AI Fellow while pursuing his DPhil in 2018. The past connection and subsequent academic progress of the researcher (now an assistant professor) may have been factors, but the grant page has no details on the decision process.',
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','University of Toronto',80000,'2023-01-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/university-of-toronto-alignment-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Toryn Klassen',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support Toryn Klassen’s research on topics related to AI alignment."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL);

/* Grants to University of California, Santa Cruz */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision, donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','University of California, Santa Cruz',265000,'2021-01-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/uc-santa-cruz-adversarial-robustness-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Cihang Xie','Catherine Olsson|Nick Beckstead',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'The grant page says the grant is "to support early-career research by Cihang Xie on adversarial robustness as a means to improve AI safety."',
  /* intended_funding_timeframe_in_months */ 36,
  /* donor_donee_reason */ 'This is one of five grants made by the donor for "adversarial robustness research" in January and February 2021, all with the same grant investigators (Catherine Olsson and Daniel Dewey) except the Santa Cruz grant that had Olsson and Nick Beckstead. https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/university-of-tuebingen-adversarial-robustness-hein https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/mit-adversarial-robustness-research https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-adversarial-robustness-wagner and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-adversarial-robustness-song are the four other grants. It looks like the donor became interested in funding this research topic at this time.',
  /* donor_amount_reason */ 'No explicit reasons for the amount are given, but the amount is similar to the amounts for other grants from Open Philanthropy to early-stage researchers in adversarial robustness research. This includes three other grants https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/university-of-tuebingen-adversarial-robustness-hein https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-adversarial-robustness-wagner and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-adversarial-robustness-song made at the same time as well as grants later in the year to early-stage researchers at Carnegie Mellon University, Stanford University, and University of Southern California.',
  /* donor_timing_reason */ 'This is one of five grants made by the donor for "adversarial robustness research" in January and February 2021, all with the same grant investigators (Catherine Olsson and Daniel Dewey) except the Santa Cruz grant that had Olsson and Nick Beckstead. https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/university-of-tuebingen-adversarial-robustness-hein https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/mit-adversarial-robustness-research https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-adversarial-robustness-wagner and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-adversarial-robustness-song are the four other grants. It looks like the donor became interested in funding this research topic at this time.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/grants/university-of-california-santa-cruz-adversarial-robustness-research-2023/ to support the same research leader and research agenda suggests satisfaction with the grant outcome.',
  /* notes */ NULL),

  ('Open Philanthropy','University of California, Santa Cruz',114000,'2023-01-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/university-of-california-santa-cruz-adversarial-robustness-research-2023/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Cihang Xie',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support research, led by Professor Cihang Xie, on adversarial robustness in AI systems. This funding will support salaries and other costs for two graduate students in Professor Xie’s lab." The webpage https://cihangxie.github.io/ is linked.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ 'This grant is made two years after the previous grant, that was intended to be a three-year grant. So, the new grant is being made a year before the running out of the old grant. The reason for the timing is not explicitly specified.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL);

/* Grants to Georgetown University */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision, donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','Georgetown Universty',246564,'2021-12-01','month','donation log','AI safety','https://www.openphilanthropy.org/grants/georgetown-university-policy-fellowship-2021/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support a fellowship related to AI and cybersecurity policy."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/grants/georgetown-university-policy-fellowship-2022/ the next year for the same purpose and for a similar amount suggest satisfaction with the outcome of the grant.',
  /* notes */ NULL),

  ('Open Philanthropy','Georgetown University',239061,'2022-12-01','month','donation log','AI safety','https://www.openphilanthropy.org/grants/georgetown-university-policy-fellowship-2022/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support a fellowship related to AI and cybersecurity policy."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ 'The amount of the grant is very similar to the amount of the previous grant https://www.openphilanthropy.org/grants/georgetown-university-policy-fellowship-2021/ to the grantee for the same fellowship the previous year.',
  /* donor_timing_reason */ 'The grant is made exactly one year after the previous grant https://www.openphilanthropy.org/grants/georgetown-university-policy-fellowship-2021/ to the grantee for the same fellowship, suggesting that this is an annual renewal grant as the fellowship is being run for a second year.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL);

/* Grants to Brian Christian */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision,donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes, is_contractwork, amount_original_currency, original_currency, currency_conversion_basis) values
  ('Open Philanthropy','Brian Christian',66000,'2021-03-01','month','donation log','AI safety/movement growth','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/brian-christian-alignment-book-promotion','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Nick Beckstead',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Contractor agreement "with Brian Christian to support the promotion of his book The Alignment Problem: Machine Learning and Human Values."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'The grant page says: "Our potential risks from advanced artificial intelligence team hopes that the book will generate interest in AI alignment among academics and others."',
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL,
  /* is_contractwork */ 1,
  /* amount_original_currency */ NULL,
  /* original_currency */ NULL,
  /* currency_conversion_basis */ NULL),

  ('Open Philanthropy','Brian Christian',37903,'2023-02-01','month','donation log','AI safety/strategy','https://www.openphilanthropy.org/grants/brian-christian-psychology-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support a DPhil in psychology at the University of Oxford. His research will focus on human preferences, with the goal of informing efforts to align AI systems with human values."',
  /* intended_funding_timeframe_in_months */  NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL,
  /* is_contractwork */ 0,
  /* amount_original_currency */ 29700,
  /* original_currency */ 'GBP',
  /* currency_conversion_basis */ 'donor calculation');

/* Grants to Daniel Dewey */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision, donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','Daniel Dewey',175000,'2021-05-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/daniel-dewey-ai-alignment-project','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Nick Beckstead',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to support "work on an AI alignment project and related field-building efforts. Daniel plans to use this funding to produce writing and reports summarizing existing research and investigating potentially valuable projects relevant to AI alignment, with the goal of helping junior researchers and others understand how they can contribute to the field."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/grants/daniel-dewey-ai-alignment-projects-2022/ suggests continued satisfaction with the grant outcome.',
  /* notes */ NULL),

  ('Open Philanthropy','Daniel Dewey',175000,'2022-08-01','month','donation log','AI safety/strategy','https://www.openphilanthropy.org/grants/daniel-dewey-ai-alignment-projects-2022/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to "support [Dewey''s] work on AI alignment. Daniel will continue work on a website explaining how artificial intelligence poses a global risk, and continue work on proposals for experiments related to AI safety." The webpage https://www.danieldewey.net/risk/index.html is linked.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL);

/* Grants to Carnegie Mellon University */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision, donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','Carnegie Mellon University',330000,'2021-05-01','month','donation log','AI safety','https://www.openphilanthropy.org/grants/carnegie-mellon-university-adversarial-robustness-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Zico Kolter','Catherine Olsson',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support Professor Zico Kolter on adversarial robustness as a means to improve AI safety."',
  /* intended_funding_timeframe_in_months */ 36,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ 'No explicit reasons for the amount are given, but the amount is similar to the amounts for other grants from Open Philanthropy to early-stage researchers in adversarial robustness research. This includes grants earlier and later in the year to early-stage researchers at UC Berkeley, University of Tübingen, Stanfard University, and University of Southern California.',
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Carnegie Mellon University',343235,'2022-07-01','month','donation log','AI safety','https://www.openphilanthropy.org/grants/carnegie-mellon-university-research-on-adversarial-examples/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Aditi Raghunathan',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to "support research led by Professor Aditi Raghunathan on adversarial examples (inputs optimized to cause machine learning models to make mistakes)." The webpages https://www.cs.cmu.edu/~aditirag/ and https://en.wikipedia.org/wiki/Adversarial_machine_learning#Adversarial_examples are linked.',
  /* intended_funding_timeframe_in_months */ 36,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'Aditi Raghunathan, whose work the grant funds, previously received money from Open Philanthropy as part of the Open Phil AI Fellowship https://www.openphilanthropy.org/grants/open-phil-ai-fellowship-2018-class/ and https://www.openphilanthropy.org/grants/uc-berkeley-adversarial-robustness-research-aditi-raghunathan/ while at UC Berkeley.');

/* Grants to Northeastern University */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision, donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','Northeastern University',562128,'2022-11-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/northeastern-university-large-language-model-interpretability-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','David Bau',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ 'This is a followup grant to the grant to David Bau made as part of a collection of grants https://www.openphilanthropy.org/grants/funding-for-ai-alignment-projects-working-with-deep-learning-systems/ providing funding for projects working with deep learning systems. That previous grant had been made through grant applications sought at https://www.openphilanthropy.org/request-for-proposals-for-projects-in-ai-alignment-that-work-with-deep-learning-systems/ (a request for proposals).',
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support Professor David Bau’s research on interpreting large language models." The webpage https://baulab.info/ is linked.',
  /* intended_funding_timeframe_in_months */ 24,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/grants/northeastern-university-mechanistic-interpretability-research/ to Northeastern University for David Bau''s lab suggests continued satisfaction with the grantee.',
  /* notes */ NULL),

  ('Open Philanthropy','Northeastern University',116072,'2023-09-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/northeastern-university-mechanistic-interpretability-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','David Bau|Sam Marks',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ 'This is a followup grant to the grant https://www.openphilanthropy.org/grants/northeastern-university-large-language-model-interpretability-research/ to David Bau''s lab.',
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support a postdoctoral position for Sam Marks in Professor David Bau’s lab, where Sam will conduct research on mechanistic interpretability." The webpages https://baulab.info/ and https://www.neelnanda.io/mechanistic-interpretability/quickstart are linked.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL);

/* Grants to OpenMined */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision, donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','OpenMined',28320,'2022-04-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/openmined-research-on-privacy-enhancing-technologies-and-ai-safety/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to "support research on the intersection between privacy-enhancing technologies and technical infrastructure for AI safety." The webpage https://en.wikipedia.org/wiki/Privacy-enhancing_technologies is linked.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The followup grant https://www.openphilanthropy.org/grants/openmined-software-for-ai-audits/ in September 2023 for a much larger amount suggests continued satisfaction with the grantee.',
  /* notes */ NULL),

  ('Open Philanthropy','OpenMined',6000000,'2023-09-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/openmined-software-for-ai-audits/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support work on developing software that facilitates access to advanced AI systems for external researchers and auditors while preserving privacy, security, and intellectual property."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL);


/* Other grants (new format) */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision, donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes) values
  ('Open Philanthropy','Press Shop',17000,'2020-01-01','month','donation log','AI safety/movement growth','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/press-shop-human-compatible','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Stuart Russell','Daniel Dewey',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to the publicity firm Press Shop to support expenses related to publicizing Professor Stuart Russell’s book Human Compatible: Artificial Intelligence and the Problem of Control."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'The grant page links this grant to past support for the Center for Human-Compatible AI (CHAI) where Russell is director, so the reason for the grant is likely similar to reasons for that past support. Grant pages: https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-center-human-compatible-ai and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-center-human-compatible-ai-2019',
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ 'The grant is made shortly after the release of the book (book release date: October 8, 2019) so the timing is likely related to the release date.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Johns Hopkins University',55000,'2020-03-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/johns-hopkins-kaplan-menard','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Jared Kaplan|Brice Ménard','Luke Muehlhauser',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support the initial research of Professors Jared Kaplan and Brice Ménard on principles underlying neural network training and performance."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Study and Training Related to AI Policy Careers',594420,'2020-03-01','month','donation log','AI safety/governance/talent pipeline','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/study-and-training-related-to-ai-policy-careers','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Emefa Agawu|Karson Elmgren|Matthew Gentzel|Becca Kagan|Benjamin Mueller','Luke Muehlhauser',NULL,NULL,NULL,NULL,
  /* donation_process */ 'This is a scholarship program run by Open Philanthropy. Applications were sought at https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/funding-AI-policy-careers with the last date for applications being 2019-10-15.',
  /* intended_use_of_funds_category */ 'Living expenses during project',
  /* intended_use_of_funds */ 'Grant is "flexible support to enable individuals to pursue and explore careers in artificial intelligence policy." Recipients include Emefa Agawu, Karson Elmgren, Matthew Gentzel, Becca Kagan, and Benjamin Mueller. The ways that specific recipients intend to use the funds is not described, but https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/funding-AI-policy-careers#examples gives general guidance on the kinds of uses Open Philanthropy was expecting to see when it opened applications.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/funding-AI-policy-careers#goal says: "The goal of this program is to provide flexible support that empowers exceptional people who are interested in positively affecting the long-run effects of transformative AI via careers in AI policy, which we see as an important and neglected issue." https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/funding-AI-policy-careers#appendix provides links to Open Philanthropy''s other writing on the importance of the issue.',
  /* donor_amount_reason */ 'https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/funding-AI-policy-careers#summary says: "There is neither a maximum nor a minimum number of applications we intend to fund; rather, we intend to fund the applications that seem highly promising to us."',
  /* donor_timing_reason */ 'Timing is likely determined by the time taken to review all applications after the close of applications on 2019-10-15.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'As of early 2022, there do not appear to have been further rounds of grantmaking from Open Philanthropy for this purpose.',
  /* notes */ 'Open Philanthropy runs a related fellowship program called the Open Phil AI Fellowship, that has an annual cadence of announcing new grants, though individual grants are often multi-year. The Open Phil AI Fellowship grantees are mostly people working on technical AI safety, whereas this grant is focused on AI policy work. Moreover, the Open Phil AI Fellowship targets graduate-level research, whereas this grant targets study and training.'),

  ('Open Philanthropy','World Economic Forum',50000,'2020-04-01','month','donation log','AI safety/governance','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/world-economic-forum-global-ai-council-workshop','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Daniel Dewey',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support a workshop hosted by the Global AI Council and co-developed with the Center for Human-Compatible AI at UC Berkeley. The workshop will facilitate the development of AI policy recommendations that could lead to future economic prosperity, and is part of a series of workshops examining solutions to maximize economic productivity and human wellbeing."',
  /* intended_funding_timeframe_in_months */ 1,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','International Conference on Learning Representations',3500,'2020-05-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ICLR-machine-learning-paper-awards','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Daniel Dewey',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to the International Conference on Learning Representations to provide awards for the best papers submitted as part of the “Towards Trustworthy Machine Learning” virtual workshop.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Andrew Lohn',15000,'2020-06-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/andrew-lohn-paper-machine-learning-model-robustness','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Andrew Lohn','Luke Muehlhauser',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to write a paper on machine learning model robustness for safety-critical AI systems."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'Nothing is specified, but the grantee''s work had previously been funded by Open Phil via the RAND Corporation for AI assurance methods.',
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Center for Strategic and International Studies',118307,'2020-09-01','month','donation log','AI safety/strategy','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/center-for-strategic-and-international-studies-ai-accident-risk-and-technology-competition','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Luke Muehlhauser',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to explore possible projects related to AI accident risk in the context of technology competition."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'No specific reasons are provided, but two other grants https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/center-for-international-security-and-cooperation-ai-accident-risk-and-technology-competition and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/rice-hadley-gates-manuel-ai-risk made at about the same time for the same intended use suggests interest from the donor in this particular use case at this time.',
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ 'No specific reasons are provided, but two other grants https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/center-for-international-security-and-cooperation-ai-accident-risk-and-technology-competition and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/rice-hadley-gates-manuel-ai-risk made at about the same time for the same intended use suggests interest from the donor in this particular use case at this time.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The increase in grant amount in May 2021, from $75,245 to $118,307, suggests that Open Phil was satisfied with initial progress on the grant.',
  /* notes */ 'The grant amount was updated in May 2021. The original amount was $75,245.'),

  ('Open Philanthropy','Center for International Security and Cooperation',67000,'2020-09-01','month','donation log','AI safety/strategy','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/center-for-international-security-and-cooperation-ai-accident-risk-and-technology-competition','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Luke Muehlhauser',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to explore possible projects related to AI accident risk in the context of technology competition."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'No specific reasons are provided, but two other grants https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/center-for-strategic-and-international-studies-ai-accident-risk-and-technology-competition and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/rice-hadley-gates-manuel-ai-risk made at about the same time for the same intended use suggests interest from the donor in this particular use case at this time.',
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ 'No specific reasons are provided, but two other grants https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/center-for-strategic-and-international-studies-ai-accident-risk-and-technology-competition and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/rice-hadley-gates-manuel-ai-risk made at about the same time for the same intended use suggests interest from the donor in this particular use case at this time.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Smitha Milli',370,'2020-10-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/smitha-milli-participatory-approaches-machine-learning-workshop','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Smitha Milli','Daniel Dewey',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support Participatory Approaches to Machine Learning, a virtual workshop held during the 2020 International Conference on Machine Learning."',
  /* intended_funding_timeframe_in_months */ 1,
  /* donor_donee_reason */ 'The donee had previously been a recipient of the Open Phil AI Fellowship, so it is likely that that relationship helped make the way for this grant.',
  /* donor_amount_reason */ 'No specific reasons are given for the amount; this is an unusually small grant size by the donor''s standards. The amount is likely determined by the limited funding needs of the grantee.',
  /* donor_timing_reason */ 'The 2020 International Conference on Machine Learning was held in July 2020, so this grant seems to have been made after the thing it was supporting was already finished. No details on timing are provided.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Berryville Institute of Machine Learning',150000,'2021-01-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/berryville-institute-of-machine-learning','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Gary McGraw','Catherine Olsson|Daniel Dewey',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'The grant page says: "[the grant is] to support research led by Gary McGraw on machine learning security. The research will focus on building a taxonomy of known attacks on machine learning, exploring a hypothesis of representation and machine learning risk, and performing an architectural risk analysis of machine learning systems."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'The grant page says: "Our potential risks from advanced artificial intelligence team hopes that the research will help advance the field of machine learning security."',
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','University of Cambridge',250000,'2021-04-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/university-of-cambridge-david-krueger','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','David Krueger','Daniel Dewey',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support Professor David Krueger’s machine learning research."',
  /* intended_funding_timeframe_in_months */ 48,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'Grant made via Cambridge in America.'),

  ('Open Philanthropy','University of Southern California',320000,'2021-08-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/university-of-southern-california-adversarial-robustness-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Robin Jia','Catherine Olsson|Nick Beckstead',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support early-career research by Robin Jia on adversarial robustness and out-of-distribution generalization as a means to improve AI safety."',
  /* intended_funding_timeframe_in_months */ 36,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ 'No explicit reasons for the amount are given, but the amount is similar to the amounts for other grants from Open Philanthropy to early-stage researchers in adversarial robustness research. This includes the two other grants https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/stanford-adversarial-robustness-research-tsipras and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/stanford-adversarial-robustness-research-santurkar made around the same time, as well as grants earlier in the year to researchers at Carnegie Mellon University, University of Tübingen, and UC Berkeley.',
  /* donor_timing_reason */ 'At around the same time as this grant, Open Philanthropy made two other grants https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/stanford-adversarial-robustness-research-tsipras and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/stanford-adversarial-robustness-research-santurkar to early-stage researchers in adversarial robustness research.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','University of Washington',730000,'2021-10-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/university-of-washington-adversarial-robustness-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Ludwig Schmidt',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to "support early-career research by Ludwig Schmidt on adversarial robustness as a means to improve AI safety."',
  /* intended_funding_timeframe_in_months */ 36,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','National Academies of Sciences, Engineering, and Medicine',309441,'2022-02-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/national-academies-of-sciences-engineering-and-medicine-safety-critical-machine-learning/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to "support research on machine learning in safety-critical environments."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Michael Page',52500,'2022-02-01','month','donation log','AI safety','https://www.openphilanthropy.org/grants/michael-page-career-transition-grant/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to work on several short-term projects while [Michael Page] explores different career options."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'The grant page says: "Page recently finished his tenure as a Research Fellow at the Center for Security and Emerging Technology, and we believe that his expertise on forecasting and AI policy makes him an exceptionally strong candidate for an impactful career."',
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Hofvarpnir Studios',1443540,'2022-03-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/hofvarpnir-studios-compute-cluster-for-ai-safety-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Jacob Steinhardt|Center for Human-Compatible Artificial Intelligence',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to "create and maintain a compute cluster for Jacob Steinhardt’s lab that will also be used by researchers at the Center for Human-Compatible Artificial Intelligence." The webpage https://jsteinhardt.stat.berkeley.edu/ is linked.',
  /* intended_funding_timeframe_in_months */ 36,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Carnegie Endowment for International Peace',597717,'2022-03-01','month','donation log','AI safety/governance','https://www.openphilanthropy.org/grants/carnegie-endowment-for-international-peace-ai-governance-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Matt Sheehan',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to "support a research project on international AI governance led by Matt Sheehan."',
  /* intended_funding_timeframe_in_months */ 24,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Center for Long-Term Cybersecurity',20000,'2022-04-01','month','donation log','AI safety/governance','https://www.openphilanthropy.org/grants/center-for-long-term-cybersecurity-ai-standards-2022/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to "support work by CLTC’s AI Security Initiative on the development and implementation of AI standards."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'This grant is made via the University of California, Berkeley. A related, much larger ($210,000) grant https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-ai-standards-2022/ is made by Open Philanthropy to the Berkeley Existential Risk Initiative for supporting work.'),

  ('Open Philanthropy','Centre for Effective Altruism',250000,'2022-08-01','month','donation log','AI safety/movement growth','https://www.openphilanthropy.org/grants/centre-for-effective-altruism-harvard-ai-safety-office/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to rent and refurbish a temporary office space for one year for the Harvard AI Safety Team." The webpage https://haist.ai/ is linked.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','AI Alignment Awards',70000,'2022-09-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/ai-alignment-awards-shutdown-problem-contest/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "over 1.5 years to AI Alignment Awards to support a contest asking participants to share ideas on how AI systems can be designed or trained to avoid the shutdown problem." The webpage https://www.alignmentawards.com/shutdown is linked.',
  /* intended_funding_timeframe_in_months */ 18,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Foundation Model Tracker',15000,'2022-10-01','month','donation log','AI safety/strategy','https://www.openphilanthropy.org/grants/thomas-liao-foundation-model-tracker/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Thomas Liao',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to Thomas Liao to support his work on maintaining Foundation Model Tracker, a website that tracks the release of large AI models." The webpage https://foundationmodeltracker.com/ is linked, but does not work as of 2023-11-19. However, the webpage https://foundationmodeltracker.notion.site/foundationmodeltracker/Model-Tracker-v0-9-794ba77f74ec469186efdbdb87e9b8e6 that https://foundationmodeltracker.com/ used to redirect to still works.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Mordechai Rorvig',110000,'2022-11-01','month','donation log','AI safety/movement growth','https://www.openphilanthropy.org/grants/mordechai-rorvig-independent-ai-journalism/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ 'The grantee''s webpage gives context on why the grantee sought the grant: "In November 2022, I was awarded a grant from Open Philanthropy, a grantmaking organization, to provide one year’s worth of support for my independent journalism work in computer science and AI. This grant neither affects my editorial independence nor indicates an endorsement of my writing by the Open Philanthropy organization. I sought out grants after leaving Quanta in August 2022, and becoming increasingly informed about what I believe is a severe state of underfunding in science journalism, particularly for areas as important as computer science and AI." The Twitter thread https://twitter.com/mordecwhy/status/1559254697940336640 is linked from the webpage.',
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support his independent journalism on topics related to computer science, AI, and AI safety." The webpage https://mordechairorvig.com/ is linked.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Jacob Steinhardt',100000,'2022-11-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/jacob-steinhardt-ai-alignment-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to provide operational support to Steinhardt’s lab at the University of California Berkeley, which specializes in research on how to align machine learning systems." The webpage https://jsteinhardt.stat.berkeley.edu/ is linked.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Apart Research',89000,'2022-12-01','month','donation log','AI safety','https://www.openphilanthropy.org/grants/apart-research-ai-alignment-hackathons/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'The grant page says: "two grants totaling $130,050 to Apart Research to support their work hosting four “hackathons” where participants will work on small projects related to AI alignment."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'This is a total across two grants.'),

  ('Open Philanthropy','Jérémy Scheurer',110000,'2022-12-01','month','donation log','AI safety/technical research/talent pipeline','https://www.openphilanthropy.org/grants/jeremy-scheurer-independent-ai-alignment-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support [grantee''s] independent research on AI alignment." The Google Scholar citations page https://scholar.google.com/citations?user=_6nYXQYAAAAJ is linked from the grantee''s name.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'The grant page says: "This is part of our strategy to grow the field of AI researchers who are focused on reducing potential risks from advanced artificial intelligence."',
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Simon McGregor',7000,'2022-12-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/simon-mcgregor-ai-risk-workshop/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support [grantee''s] work to organize a workshop on AI risk."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Purdue University',170000,'2022-12-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/purdue-university-language-model-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Xiangyu Zhang',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support research led by Professor Xiangyu Zhang on improving the robustness of language models against adversarial attacks." The webpage https://www.cs.purdue.edu/homes/xyzhang/ is linked.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','University of British Columbia',100375,'2023-01-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/university-of-british-columbia-ai-alignment-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Jeff Clune',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "over two years to the University of British Columbia to support research led by Professor Jeff Clune on AI alignment." The webpage https://www.cs.ubc.ca/people/jeff-clune is linked.',
  /* intended_funding_timeframe_in_months */ 24,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Adam Jermyn',19231,'2023-01-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/adam-jermyn-independent-ai-alignment-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to Adam Jermyn to support his independent technical research on AI alignment."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Neel Nanda',70000,'2023-01-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/neel-nanda-interpretability-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to Neel Nanda to support his independent research on interpretability. His work is aimed at improving human understanding of neural networks and machine learning models."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Responsible AI Collaborative',100000,'2023-02-01','month','donation log','AI safety/strategy','https://www.openphilanthropy.org/grants/responsible-ai-collaborative-ai-incident-database/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support its work maintaining the AI Incident Database, which is a database of incidents where AI systems have caused real-world harm." https://incidentdatabase.ai/ is the linked webpage for the AI Incident Database.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Alignment Research Engineer Accelerator',18800,'2023-02-01','month','donation log','AI safety/movement growth','https://www.openphilanthropy.org/grants/alignment-research-engineer-accelerator-ai-safety-technical-program/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'Grant "to support the Alignment Research Engineer Accelerator (ARENA), which is a program to help individuals interested in AI safety improve their technical skills in machine learning."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Cornell University',342645,'2023-02-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/cornell-university-ai-safety-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Lionel Levine',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support Professor Lionel Levine’s research related to AI alignment and safety."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','California State University, San José',39000,'2023-03-01','month','donation log','AI safety/governance/forecasting','https://www.openphilanthropy.org/grants/san-jose-state-university-ai-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Yan Zhang',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support research by Professor Yan Zhang on AI forecasting and AI governance." The webpage https://www.sjsu.edu/people/yan.zhang/ is linked.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Forecasting Research Institute',150000,'2023-03-01','month','donation log','AI safety/strategy/forecasting','https://www.openphilanthropy.org/grants/forecasting-research-institute-ai-forecasting-project/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support a project that will bring together forecasters who disagree about the magnitude of AI existential risk to discuss and make predictions about AI, with the goal of identifying key views and arguments driving their forecasts and disagreements. The participants will include some “superforecasters” (people with a strong track record of making accurate predictions) and some AI subject-matter experts, among others."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','University of Illinois',80000,'2023-03-01','month','donation log','AI safety/tecnical research','https://www.openphilanthropy.org/grants/university-of-illinois-ai-alignment-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Ben Levinstein',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support Professor Ben Levinstein’s research on AI alignment." Levinstein''s website https://www.levinstein.org/ is linked.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','University of Maryland',312959,'2023-04-01','month','donation log','AI safety','https://www.openphilanthropy.org/grants/university-of-maryland-policy-fellowship-2023/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to the University of Maryland to support a fellowship related to technology and national security."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','National Science Foundation',5000000,'2023-04-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/national-science-foundation-safe-learning-enabled-systems/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Regranting',
  /* intended_use_of_funds */ 'Grant "to support [National Science Foundation''s] Safe Learning-Enabled Systems program, which will regrant the funds to foundational research projects aimed at finding ways to guarantee the safety of machine learning systems." https://new.nsf.gov/funding/opportunities/safe-learning-enabled-systems is the linked webpage for the Safe Learning-Enabled Systems program.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Leap Labs',230000,'2023-04-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/leap-labs-interpretability-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ 'The grant page says: "This grant was made primarily based on the recommendation of an external technical advisor."',
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to Leap Labs to support research on AI interpretability, particularly model agnostic interpretability." The links are as follows: https://www.alignmentforum.org/s/Tp3ryR4AxY56ctGh2/p/CzZ6Fch4JSpwCpu6C for AI interpretability and https://www.lesswrong.com/posts/uXGLciramzNfb8Hvz/why-i-m-working-on-model-agnostic-interpretability for model agnostic interpretability.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'The grant page says: "This grant was made primarily based on the recommendation of an external technical advisor."',
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','University of Chicago',250000,'2023-05-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/university-of-chicago-research-on-complementary-ai/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Chenhao Tan',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support research, led by Professor Chenhao Tan, on how to train AI systems to complement human efforts." Chenhao Tan''s website https://chenhaot.com/ is linnked from the grant page.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Apollo Research',1535480,'2023-06-01','month','donation log','AI safety/technical research/governance','https://www.openphilanthropy.org/grants/apollo-research-startup-funding/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'Grant "for startup costs. Apollo Research is a new organization that will conduct research on how to evaluate whether AI models are aligned and safe, with a focus on interpretability and detecting whether models are deceptive. Apollo also plans to do research on AI governance."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ 'The timing is likely determined by the timing of the start of the funded organization (Apollo Research).',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Legal Priorities Project',75000,'2023-08-01','month','donation log','AI safety/governance','https://www.openphilanthropy.org/grants/legal-priorities-project-law-ai-summer-research-fellowship/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support [grantee''s] Summer Research Fellowship in Law & AI. Participants will work with researchers at LPP on projects at the intersection of law and risks from advanced AI."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Modulo Research',408255,'2023-08-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/modulo-research-ai-safety-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Gabriel Recchia',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support research — led by Gabriel Recchia — into large language model sandwiching experiments, dataset development, and capability evaluations." The webpage https://uk.linkedin.com/in/gabriel-recchia-38575b10 and the LessWrong blog post section https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models#Potential_near_future_projects___sandwiching_ are linked.',
  /* intended_funding_timeframe_in_months */ 24,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'The grant https://www.openphilanthropy.org/grants/surge-ai-data-production-for-ai-safety-research/ to Surge AI made around the same time also supports work by the same person (Gabriel Recchia) on a "research project on sandwiching experiments and capability evaluations of large language models."'),

  ('Open Philanthropy','AI Safety Communications Centre',288000,'2023-08-01','month','donation log','AI safety','https://www.openphilanthropy.org/grants/effective-ventures-foundation-ai-safety-communications-centre/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Organizational general support',
  /* intended_use_of_funds */ 'The grant page says: "This project provides the AI safety community with communications support, and connects journalists to AI safety experts and resources." https://aiscc.org/ is the linked grantee website.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'Grant via the Effective Ventures Foundation.'),

  ('Open Philanthropy','University of Pennsylvania',110000,'2023-09-01','month','donation log','AI safety/governance','https://www.openphilanthropy.org/grants/university-of-pennsylvania-ai-governance-roundtables/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Peter Conti-Brown',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support a series of roundtables led by Professor Peter Conti-Brown. At these events, experts will discuss how insights from financial regulation might inform emerging discussions on AI governance." The webpage https://lgst.wharton.upenn.edu/profile/petercb/ is linked.',
  /* intended_funding_timeframe_in_months */ 24,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL),

  ('Open Philanthropy','Eleuther AI',2642273,'2023-11-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/eleuther-ai-interpretability-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Nora Belrose',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support the work of Nora Belrose. Nora will conduct research on AI interpretability and hire other researchers to assist her in this work."',
  /* intended_funding_timeframe_in_months */ 48,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL);

/* Other grants (new format) in other currencies */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision, donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes, amount_original_currency, original_currency, currency_conversion_basis) values
  ('Open Philanthropy','Université de Montréal',210552,'2021-09-01','month','donation log','AI safety','https://www.openphilanthropy.org/grants/universite-de-montreal-research-project-on-artificial-intelligence/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Mila|Future of Humanity Institute',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to "support a research project investigating AI consciousness and moral patienthood. The research will be conducted in collaboration with Mila and the Future of Humanity Institute. This funding will support post-docs and students studying the topic, as well as publications and workshops."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL,
  /* amount_original_currency */ 266200,
  /* original_currency */ 'EUR',
  /* currency_conversion_basis */ 'donor calculation'),

  ('Open Philanthropy','Stiftung Neue Verantwortung',444000,'2022-03-01','month','donation log','AI safety/strategy','https://www.openphilanthropy.org/grants/stiftung-neue-verantwortung-ai-policy-analysis/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to "support data-driven reports on AI-related talent flows and the global microchip supply chain."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL,
  /* amount_original_currency */ 390528,
  /* original_currency */ 'EUR',
  /* currenncy_conversion_basis */ 'donor calculation'),

  ('Open Philanthropy','Egor Krasheninnikov',6526,'2022-03-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/egor-krasheninnikov-research-collaboration-with-david-krueger/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','David Krueger',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to "support [grantee''s] research on machine learning in collaboration with Professor David Krueger." The webpage https://www.davidscottkrueger.com/ is linked.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'This is a followup to the April 2021 support https://www.openphilanthropy.org/grants/university-of-cambridge-machine-learning-research/ to University of Cambridge in support of David Krueger''s work. At around the same time, a similar grant https://www.openphilanthropy.org/grants/usman-anwar-research-collaboration-with-david-krueger/ is made to Egor Krasheninnikov, also for work with David Krueger.',
  /* amount_original_currency */ 5000,
  /* original_currency */ 'GBP',
  /* currency_conversion_basis */ 'donor calculation'),

  ('Open Philanthropy','Usman Anwar',6526,'2022-03-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/usman-anwar-research-collaboration-with-david-krueger/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','David Krueger',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant to "support his research on machine learning in collaboration with Professor David Krueger." The webpage https://www.davidscottkrueger.com/ is linked.',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'Usman Anwar would also be the recipient of the Open Phil AI Fellowship https://www.openphilanthropy.org/grants/open-phil-ai-fellowship-2022-class/ in 2022.',
  /* notes */ 'This is a followup to the April 2021 support https://www.openphilanthropy.org/grants/university-of-cambridge-machine-learning-research/ to University of Cambridge in support of David Krueger''s work. At around the same time, a similar grant https://www.openphilanthropy.org/grants/egor-krasheninnikov-research-collaboration-with-david-krueger/ is made to Egor Krasheninnikov, also for work with David Krueger.',
  /* amount_original_currency */ 5000,
  /* original_currency */ 'GBP',
  /* currency_conversion_basis */ 'donor calculation'),

  ('Open Philanthropy','OxAI Safety Hub',11622,'2022-10-01','month','donation log','AI safety/movement growth','https://www.openphilanthropy.org/grants/catherine-brewer-oxai-safety-hub/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Catherine Brewer',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to Catherine Brewer to support the OxAI Safety Hub, which is a new Oxford-based group working on building the AI safety community."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL,
  /* amount_original_currency */ 10540,
  /* original_currency */ 'GBP',
  /* currency_conversion_basis */ 'donor calculation');

/* Other grants (new format) that are contract work */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision,donation_announcement_url, predictions, donation_process, intended_use_of_funds_category, intended_use_of_funds, intended_funding_timeframe_in_months, donor_donee_reason, donor_amount_reason, donor_timing_reason, donor_next_donation_thoughts, donor_retrospective, notes, is_contractwork) values
  ('Open Philanthropy','Daniel Kang|Jacob Steinhardt|Yi Sun|Alex Zhai',2351,'2018-11-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/study-robustness-machine-learning-models','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Daniel Kang|Jacob Steinhardt|Yi Sun|Alex Zhai','Daniel Dewey',NULL,NULL,NULL,NULL,
  /* donation_process */ 'The grant page says: "This project was supported through a contractor agreement. While we typically do not publish pages for contractor agreements, we occasionally opt to do so."',
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to reimburse technology costs for their efforts to study the robustness of machine learning models, especially robustness to unforeseen adversaries."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'The grant page says "We believe this will accelerate progress in adversarial, worst-case robustness in machine learning."',
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL,
  /* is_contractwork */ 1),

  ('Open Philanthropy','WestExec',540000,'2020-02-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/westexec-report-on-assurance-in-machine-learning-systems','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Luke Muehlhauser','2020-03-20','day',NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Contractor agreement "to support the production and distribution of a report on advancing policy, process, and funding for the Department of Defense’s work on test, evaluation, verification, and validation for deep learning systems."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ 'The increases in grant amounts suggest that the donor was satisfied with initial progress.',
  /* notes */ 'The grant amount was updated in October and November 2020 and again in May 2021. The original grant amount had been $310,000.',
  /* is_contractwork */ 1),

  ('Open Philanthropy','Rice, Hadley, Gates & Manuel LLC',25000,'2020-09-01','month','donation log','AI safety/strategy','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/rice-hadley-gates-manuel-ai-risk','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/',NULL,'Luke Muehlhauser',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Contractor agreement "to explore possible projects related to AI accident risk in the context of technology competition."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ 'No specific reasons are provided, but two other grants https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/center-for-strategic-and-international-studies-ai-accident-risk-and-technology-competition and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/center-for-international-security-and-cooperation-ai-accident-risk-and-technology-competition made at about the same time for the same intended use suggests interest from the donor in this particular use case at this time.',
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ 'No specific reasons are provided, but two other grants https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/center-for-strategic-and-international-studies-ai-accident-risk-and-technology-competition and https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/center-for-international-security-and-cooperation-ai-accident-risk-and-technology-competition made at about the same time for the same intended use suggests interest from the donor in this particular use case at this time.',
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL,
  /* is_contractwork */ 1),

  ('Open Philanthropy','Hypermind',121124,'2021-03-01','month','donation log','AI safety/strategy/forecasting','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/hypermind-ai-forecasting-tournament','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Metaculus','Luke Muehlhauser',NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Contractor agreement "to collaborate with Metaculus on an AI development forecasting tournament. Forecasts will cover the themes of hardware and supercomputing, performance and benchmarks, research trends, and economic and financial impact."',
  /* intended_funding_timeframe_in_months */ NULL,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ NULL,
  /* is_contractwork */ 1),

  ('Open Philanthropy','Surge AI',123750,'2023-09-01','month','donation log','AI safety/technical research','https://www.openphilanthropy.org/grants/surge-ai-data-production-for-ai-safety-research/','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Gabriel Recchia',NULL,NULL,NULL,NULL,NULL,
  /* donation_process */ NULL,
  /* intended_use_of_funds_category */ 'Direct project expenses',
  /* intended_use_of_funds */ 'Grant "to support Gabriel Recchia in producing data points for a research project on sandwiching experiments and capability evaluations of large language models." The webpage https://uk.linkedin.com/in/gabriel-recchia-38575b10 and the LessWrong post section https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models#Potential_near_future_projects___sandwiching_ are linked.',
  /* intended_funding_timeframe_in_months */ 24,
  /* donor_donee_reason */ NULL,
  /* donor_amount_reason */ NULL,
  /* donor_timing_reason */ NULL,
  /* donor_next_donation_thoughts */ NULL,
  /* donor_retrospective */ NULL,
  /* notes */ 'The grant https://www.openphilanthropy.org/grants/modulo-research-ai-safety-research/ to Modulo Research made around the same time also supports work by the same person (Gabriel Recchia) on a "research project on sandwiching experiments and capability evaluations of large language models."',
  /* is_contractwork */ 1);

/* Other grants (old format) */
insert into donations(donor, donee, amount, donation_date, donation_date_precision, donation_date_basis, cause_area, url, donor_cause_area_url, notes, donation_earmark, influencer, donation_announcement_date, donation_announcement_date_precision,donation_announcement_url, predictions) values
  ('Open Philanthropy','Future of Life Institute',1186000,'2015-08-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/future-life-institute-artificial-intelligence-risk-reduction','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Grant accompanied a grant by Elon Musk to FLI for the same purpose. See also the March 2015 blog post https://www.openphilanthropy.org/blog/open-philanthropy-project-update-global-catastrophic-risks that describes strategy and developments prior to the grant. An update on the grant was posted in 2017-04 at https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/update-fli-grant discussing impressions of Howie Lempel and Daniel Dewey of the grant and of the effect on and role of Open Phil', NULL, NULL,'2015-08-26','day','https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/q5PuecwdXGk',NULL),
  ('Open Philanthropy','Electronic Frontier Foundation',199000,'2016-11-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/electronic-frontier-foundation-ai-social','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Grant funded work by Peter Eckersley, whom the Open Philanthropy Project believed in. Followup conversation with Peter Eckersley and Jeremy Gillula of grantee organization at https://www.openphilanthropy.org/sites/default/files/Peter_Eckersley_Jeremy_Gillula_05-26-16_%28public%29.pdf on 2016-05-26', 'Peter Eckersley', NULL, '2016-12-15','day','https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/qVU8m1QRQPI',NULL),
  ('Open Philanthropy','George Mason University',277435,'2016-06-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/george-mason-university-research-future-artificial-intelligence-scenarios','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Earmarked for Robin Hanson research. Grant page references https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence for background. Original amount $264,525. Increased to $277,435 through the addition of $12,910 in July 2017 to cover an increase in George Mason University’s instructional release costs (teaching buyouts).', 'Robin Hanson', NULL, '2016-07-07', 'day', 'https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/ZDGCJGCb2C4',NULL),
  ('Open Philanthropy','Future of Humanity Institute',1994000,'2017-03-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/future-humanity-institute-general-support','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Grant for general support. A related grant specifically for biosecurity work was granted in 2016-09, made earlier for logistical reasons', NULL, NULL, '2017-03-06', 'day', 'https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/Ehx9XD2XYPg',NULL),
  ('Open Philanthropy','UCLA School of Law',1536222,'2017-05-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ucla-artificial-intelligence-governance','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Grant to support work on governance related to AI risk led by Edward Parson and Richard Re','Edward Parson,Richard Re','Helen Toner','2017-07-27','day','https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/hCmf-IGe40Y',NULL),
  ('Open Philanthropy','Distill',25000,'2017-03-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/distill-prize-clarity-machine-learning-general-support','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Grant covers 25000 out of a total of 125000 USD initial endowment for the Distill prize https://distill.pub/prize/ administered by the Open Philanthropy Project. Other contributors to the endowment include Chris Olah, Greg Brockman, Jeff Dean, and DeepMind. The Open Philanthropy Project grant page says: "Without our funding, we estimate that there is a 60% chance that the prize would be administered at the same level of quality, a 30% chance that it would be administered at lower quality, and a 10% chance that it would not move forward at all. We believe that our assistance in administering the prize will also be of significant help to Distill."',NULL,'Daniel Dewey','2017-08-11','day','https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/iyErXoVcmSE',NULL),
  ('Open Philanthropy','Yale University',299320,'2017-07-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/yale-university-global-politics-of-ai-dafoe','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Grant to support research into the global politics of artificial intelligence, led by Assistant Professor of Political Science, Allan Dafoe, who will conduct part of the research at the Future of Humanity Institute in Oxford, United Kingdom over the next year. Funds from the two gifts will support the hiring of two full-time research assistants, travel, conferences, and other expenses related to the research efforts, as well as salary, relocation, and health insurance expenses related to Professor Dafoe’s work in Oxford.','Allan Dafoe','Nick Beckstead','2017-09-28','day','https://groups.google.com/a/openphilanthropy.org/forum/#!topic/newly.published/5UjOZN6KlWQ',NULL),
  ('Open Philanthropy','University of Oxford',429770,'2018-07-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/oxford-university-global-politics-of-ai-dafoe','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Grant to support research on the global politics of advanced artificial intelligence. The work will be led by Professor Allan Dafoe at the Future of Humanity Institute in Oxford, United Kingdom. The Open Philanthropy Project recommended additional funds to support this work in 2017, while Professor Dafoe was at Yale. Continuation of grant https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/yale-university-global-politics-of-ai-dafoe','Allan Dafoe','Nick Beckstead','2018-07-20','day',NULL,NULL),
  ('Open Philanthropy','AI Scholarships',159000,'2018-02-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ai-scholarships-2018','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Discretionary grant; total across grants to two artificial intelligence researcher, both over two years. The funding is intended to be used for the students’ tuition, fees, living expenses, and travel during their respective degree programs, and is part of an overall effort to grow the field of technical AI safety by supporting value-aligned and qualified early-career researchers. Recipients are Dmitrii Krasheninnikov, master’s degree, University of Amsterdam and Michael Cohen, master’s degree, Australian National University','Dmitrii Krasheninnikov|Michael Cohen','Daniel Dewey','2018-07-26','day',NULL,NULL),
  ('Open Philanthropy','GoalsRL',7500,'2018-08-01','month','donation log','AI safety','https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/goals-rl-workshop-on-goal-specifications-for-reinforcement-learning','https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/','Discretionary grant to offset travel, registration, and other expenses associated with attending the GoalsRL 2018 workshop on goal specifications for reinforcement learning. The workshop was organized by Ashley Edwards, a recent computer science PhD candidate interested in reward learning.','Ashley Edwards','Daniel Dewey','2018-10-05','day',NULL,NULL);
